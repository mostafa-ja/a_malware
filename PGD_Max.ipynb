{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYoZiICQ6Bo01xnObwG64A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/a_malware/blob/main/PGD_Max.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install androguard\n",
        "!pip install captum\n",
        "\n",
        "import androguard\n",
        "import captum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdWcbQsbDh0u",
        "outputId": "e2580376-eff7-48d4-b42b-864b64faacb7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting androguard\n",
            "  Downloading androguard-3.3.5-py3-none-any.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.4/922.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto>=0.24.0 (from androguard)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from androguard) (8.1.7)\n",
            "Collecting colorama (from androguard)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from androguard) (0.18.3)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from androguard) (7.34.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from androguard) (4.9.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from androguard) (3.7.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from androguard) (3.2)\n",
            "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from androguard) (1.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from androguard) (2.16.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->androguard)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (3.0.39)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (4.8.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot>=1.4.1->androguard) (3.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->androguard) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.0.0->androguard) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->androguard) (0.2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->androguard) (1.16.0)\n",
            "Installing collected packages: asn1crypto, jedi, colorama, androguard\n",
            "Successfully installed androguard-3.3.5 asn1crypto-1.5.1 colorama-0.4.6 jedi-0.19.1\n",
            "Collecting captum\n",
            "  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eRx10YoAiFg",
        "outputId": "5a381f54-1125-4c9a-c4dc-2e1a6cd1478c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Cloning into 'malware_detection'...\n",
            "remote: Enumerating objects: 16028, done.\u001b[K\n",
            "remote: Counting objects: 100% (2180/2180), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 16028 (delta 2017), reused 2165 (delta 2003), pack-reused 13848\u001b[K\n",
            "Receiving objects: 100% (16028/16028), 249.84 MiB | 23.21 MiB/s, done.\n",
            "Resolving deltas: 100% (12947/12947), done.\n",
            "Updating files: 100% (141/141), done.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os.path as path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import gdown\n",
        "from functools import partial\n",
        "import random\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "!git clone https://github.com/mostafa-ja/malware_detection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our sharable address : https://drive.google.com/file/d/1-MPYbRTFn_zH3rybfAYihHuIjq2XSiI5/view?usp=sharing\n",
        "download_link = 'https://drive.google.com/uc?id=1Rro-6k4D8y5t7wX8L8dRd4SPCKAKGZXl'\n",
        "output_filepath = '/content/'\n",
        "gdown.download(download_link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "nBAlbafmmMfq",
        "outputId": "06ea0703-6c26-48a9-e315-0f219734d4b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Rro-6k4D8y5t7wX8L8dRd4SPCKAKGZXl\n",
            "To: /content/archive.zip\n",
            "100%|██████████| 71.3M/71.3M [00:00<00:00, 138MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/archive.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_link = 'https://drive.google.com/uc?id=1--gjFTkAJUW74RXWE4anJ5wssV7K_VAo'\n",
        "output_filepath = '/content/'\n",
        "gdown.download(download_link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "LWrVkcPVjjzg",
        "outputId": "d4e02821-4095-457f-e9d7-a9d1cc8b15b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--gjFTkAJUW74RXWE4anJ5wssV7K_VAo\n",
            "To: /content/train_dataset.pt\n",
            "100%|██████████| 97.4M/97.4M [00:01<00:00, 90.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/train_dataset.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_link = 'https://drive.google.com/uc?id=1-2NnBk3YKauGTGK8DfQHBxrLrYyTxNsp'\n",
        "output_filepath = '/content/'\n",
        "gdown.download(download_link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "P3KZMz42m4c1",
        "outputId": "e6a64cae-8135-40fc-f208-6a9da41f5970"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-2NnBk3YKauGTGK8DfQHBxrLrYyTxNsp\n",
            "To: /content/validation_dataset.pt\n",
            "100%|██████████| 32.5M/32.5M [00:00<00:00, 152MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/validation_dataset.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_link = 'https://drive.google.com/uc?id=1-4PhQGYHpMXrCcxcgMadgPPA4S-9yluX'\n",
        "output_filepath = '/content/'\n",
        "gdown.download(download_link, output_filepath)"
      ],
      "metadata": {
        "id": "9vo-g1RHm46G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip"
      ],
      "metadata": {
        "id": "sZsmPn-7pXNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/content/naive_data /content/malware_detection/datasets/\n",
        "!cp /content/train_dataset.pt /content/malware_detection/datasets\n",
        "!cp /content/validation_dataset.pt /content/malware_detection/datasets\n",
        "!cp /content/test_dataset.pt /content/malware_detection/datasets"
      ],
      "metadata": {
        "id": "ml8RC85unN-m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = os.listdir('/content/malware_detection/datasets/naive_data')\n",
        "len(file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buq6_Gzz_JbQ",
        "outputId": "9047d7b4-43bf-4d3d-8756-0148559b8f9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5995"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = [(torch.tensor(data, dtype=torch.float32), label) for data, label in samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.samples[index]"
      ],
      "metadata": {
        "id": "a4E-Kgs2AoJG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MalwareDetectionDNN(nn.Module):\n",
        "    def __init__(self, input_size, n_classes, device='cpu', name='DNN', dense_hidden_units=[200, 200], dropout=0.6, smooth=False):\n",
        "        \"\"\"\n",
        "        Construct malware detector\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        @param input_size: Integer, the dimentionality number of input vector\n",
        "        @param n_classes: Integer, the number of classes, n=2\n",
        "        @param device: String, 'cpu' or 'cuda'\n",
        "        @param name: String, model name\n",
        "        \"\"\"\n",
        "        super(MalwareDetectionDNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.n_classes = n_classes\n",
        "        self.name = name\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.smooth = smooth\n",
        "        self.dense_hidden_units = dense_hidden_units\n",
        "\n",
        "        self.dense_layers = []\n",
        "        if len(self.dense_hidden_units) >= 1:\n",
        "            self.dense_layers.append(nn.Linear(self.input_size, self.dense_hidden_units[0]))\n",
        "        else:\n",
        "            raise ValueError(\"Expect at least one hidden layer.\")\n",
        "        for i in range(len(self.dense_hidden_units[0:-1])):\n",
        "            self.dense_layers.append(nn.Linear(self.dense_hidden_units[i],  # start from idx=1\n",
        "                                               self.dense_hidden_units[i + 1]))\n",
        "        self.dense_layers.append(nn.Linear(self.dense_hidden_units[-1], self.n_classes))\n",
        "\n",
        "        # registration\n",
        "        for idx_i, dense_layer in enumerate(self.dense_layers):\n",
        "            self.add_module('nn_model_layer_{}'.format(idx_i), dense_layer)\n",
        "\n",
        "        if self.smooth:\n",
        "            self.activation_func = F.selu  # partial(F.elu, alpha=self.alpha_)\n",
        "        else:\n",
        "            self.activation_func = F.relu\n",
        "\n",
        "        self.model_save_path = path.join('/content/malware_detection/save/drebin/md_dnn'+ '_' + self.name,'model.pth')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Go through the neural network\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        @param x: 2D tensor, feature representation\n",
        "        \"\"\"\n",
        "        for dense_layer in self.dense_layers[:-1]:\n",
        "            x = self.activation_func(dense_layer(x))\n",
        "\n",
        "        latent_representation = F.dropout(x, self.dropout, training=self.training)\n",
        "        logits = self.dense_layers[-1](latent_representation)\n",
        "        return logits\n",
        "\n",
        "    def customize_loss(self, logits, gt_labels, representation = None, mini_batch_idx=None):\n",
        "        return F.cross_entropy(logits, gt_labels)\n",
        "\n",
        "    # for predict pertubed malicious and create a y_true same size for theam\n",
        "    def inference_batch_wise(self, x):\n",
        "        \"\"\"\n",
        "        support malware samples solely\n",
        "        \"\"\"\n",
        "        assert isinstance(x, torch.Tensor)\n",
        "        logit = self.forward(x)\n",
        "        return torch.softmax(logit, dim=-1).detach().cpu().numpy(), np.ones((logit.size()[0],))\n",
        "\n",
        "    def inference(self, test_data_producer):\n",
        "        confidences = []\n",
        "        gt_labels = []\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_data_producer: # for large dataset we have to consider test dataset with batches\n",
        "                x, y = x.double().to(device), y.long().to(device)\n",
        "                logits = self.forward(x)\n",
        "                confidences.append(F.softmax(logits, dim=-1))\n",
        "                gt_labels.append(y)\n",
        "        confidences = torch.vstack(confidences) #[[1,2,3],[4,5,6]] > below each other\n",
        "        gt_labels = torch.cat(gt_labels, dim=0) #[[1,2,3],[4,5,6]] > [1,2,3,4,5,6]\n",
        "        return confidences, gt_labels\n",
        "\n",
        "\n",
        "    def predict(self, test_data_producer, indicator_masking=True):\n",
        "        \"\"\"\n",
        "        predict labels and conduct evaluation\n",
        "\n",
        "        Parameters\n",
        "        --------\n",
        "        @param test_data_producer, torch.DataLoader\n",
        "        \"\"\"\n",
        "        # evaluation\n",
        "        confidence, y_true = self.inference(test_data_producer)\n",
        "        y_pred = confidence.argmax(1).cpu().numpy()\n",
        "        y_true = y_true.cpu().numpy()\n",
        "        from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        b_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "        print(\"The accuracy on the test dataset is {:.5f}%\".format(accuracy * 100))\n",
        "        print(\"The balanced accuracy on the test dataset is {:.5f}%\".format(b_accuracy * 100))\n",
        "\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        fpr = fp / float(tn + fp)\n",
        "        fnr = fn / float(tp + fn)\n",
        "        f1 = f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "        print(\"Other evaluation metrics we may need:\")\n",
        "        print(\"False Negative Rate (FNR) is {:.5f}%, False Positive Rate (FPR) is {:.5f}%, F1 score is {:.5f}%\".format(fnr * 100, fpr * 100, f1 * 100))\n",
        "\n"
      ],
      "metadata": {
        "id": "IBNqn_OEAuV-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/malware_detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRRx7oWaDnGY",
        "outputId": "e4ee4787-0395-405c-ee43-c696144a2e50"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/malware_detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projected gradient descent (ascent)."
      ],
      "metadata": {
        "id": "-45R7q66ygAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from core.droidfeature import InverseDroidFeature\n",
        "\n",
        "inverse_feature = InverseDroidFeature()\n",
        "\n",
        "# manipulation_x : removable features\n",
        "manipulation_x = inverse_feature.get_manipulation()\n",
        "manipulation_x = torch.LongTensor(manipulation_x).to(device)\n",
        "\n",
        "# index of vocab which are interdependent\n",
        "omega = inverse_feature.get_interdependent_apis()\n",
        "omega = torch.sum(F.one_hot(torch.tensor(omega), num_classes=len(inverse_feature.vocab)),dim=0).to(device)\n",
        "\n",
        "# a list of vocab length : if the type of feature is api , index of that feature will be true\n",
        "api_flag = inverse_feature.get_api_flag()"
      ],
      "metadata": {
        "id": "sY7SRwnGYixM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_x0(x, rounding_threshold=0.5, is_sample=False):\n",
        "    \"\"\"\n",
        "    Helper function to randomly initialize the the inner maximizer algos\n",
        "    randomize such that the functionality is preserved.\n",
        "    :param x: training sample\n",
        "    :param is_sample: flag to sample randomly from feasible area or return just x\n",
        "    :return: randomly sampled feasible version of x\n",
        "    \"\"\"\n",
        "    if is_sample:\n",
        "        rand_x = round_x(torch.rand(x.size()), alpha=rounding_threshold)\n",
        "        if x.is_cuda:\n",
        "            rand_x = rand_x.cuda()\n",
        "        return (rand_x.byte() | x.byte()).double()\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "\n",
        "def round_x(x, alpha=0.5):\n",
        "    \"\"\"\n",
        "    rounds x by thresholding it according to alpha which can be a scalar or vector\n",
        "    :param x:\n",
        "    :param alpha: threshold parameter\n",
        "    :return: a double tensor of 0s and 1s.\n",
        "    \"\"\"\n",
        "    return (x >= alpha).double()"
      ],
      "metadata": {
        "id": "Ye0d01eYuWZv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PGD():\n",
        "    \"\"\"\n",
        "    Projected gradient descent (ascent).\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    @param norm, 'l2' or 'linf'\n",
        "    @param use_random, Boolean,  whether use random start point\n",
        "    @param rounding_threshold, float, a threshold for rounding real scalars\n",
        "    @param is_attacker, Boolean, play the role of attacker (note: the defender conducts adversarial training)\n",
        "    @param oblivion, Boolean, whether know the adversary indicator or not\n",
        "    @param kappa, attack confidence\n",
        "    @param manipulation_x, manipulations\n",
        "    @param omega, the indices of interdependent apis corresponding to each api\n",
        "    @param device, 'cpu' or 'cuda'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, norm, use_random=False, rounding_threshold=0.5,\n",
        "                 is_attacker=True, manipulation_x=None, omega=None,api_flag=None, device=None):\n",
        "        super(PGD, self).__init__()\n",
        "        assert norm == 'l1' or norm == 'l2' or norm == 'linf', \"Expect 'l1', 'l2' or 'linf'.\"\n",
        "        self.norm = norm\n",
        "        self.use_random = use_random\n",
        "        self.round_threshold = rounding_threshold\n",
        "        self.is_attacker = is_attacker\n",
        "        self.lambda_ = 1.\n",
        "        self.omeag = omega\n",
        "        self.manipulation_x = manipulation_x\n",
        "        self.api_flag = api_flag\n",
        "        self.device = device\n",
        "    def _perturb(self, model, x, label=None,\n",
        "                 steps=10,\n",
        "                 step_length=1.,\n",
        "                 lambda_=1.,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        perturb node feature vectors\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        @param model, a victim model\n",
        "        @param x: torch.FloatTensor, node feature vectors (each represents the occurrences of apis in a graph) with shape [batch_size, vocab_dim]\n",
        "        @param label: torch.LongTensor, ground truth labels\n",
        "        @param steps: Integer, maximum number of iterations\n",
        "        @param step_length: float, the step length in each iteration\n",
        "        @param lambda_, float, penalty factor\n",
        "        \"\"\"\n",
        "        adv_x = x\n",
        "        self.lambda_ = lambda_\n",
        "        model.eval()\n",
        "        loss_natural = 0.\n",
        "        for t in range(steps):\n",
        "            if t == 0 and self.use_random:\n",
        "                adv_x = get_x0(adv_x, rounding_threshold=self.round_threshold, is_sample=True)\n",
        "            var_adv_x = torch.autograd.Variable(adv_x, requires_grad=True)\n",
        "            loss, done = self.get_loss(model, var_adv_x, label, self.lambda_)\n",
        "            if t == 0:\n",
        "                loss_natural = loss\n",
        "            grad = torch.autograd.grad(torch.mean(loss), var_adv_x)[0].detach().data\n",
        "            perturbation = self.get_perturbation(grad, x, adv_x)\n",
        "            adv_x = torch.clamp(adv_x + perturbation * step_length, min=0., max=1.)\n",
        "        # round\n",
        "        if self.norm == 'linf' and (not hasattr(model, 'is_detector_enabled')):\n",
        "            round_threshold = torch.rand(x.size()).to(self.device)\n",
        "        else:\n",
        "            round_threshold = self.round_threshold\n",
        "        adv_x = round_x(adv_x, round_threshold)\n",
        "        loss_adv, _1 = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "        replace_flag = (loss_adv < loss_natural).unsqueeze(1).expand_as(adv_x)\n",
        "        adv_x[replace_flag] = x[replace_flag]\n",
        "        return adv_x\n",
        "\n",
        "    def perturb(self, model, x, label=None,\n",
        "                steps=10,\n",
        "                step_length=1.,\n",
        "                min_lambda_=1e-5,\n",
        "                max_lambda_=1e5,\n",
        "                base=10.,\n",
        "                verbose=False):\n",
        "        \"\"\"\n",
        "        enhance attack\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            self.lambda_ = min_lambda_\n",
        "        else:\n",
        "            self.lambda_ = max_lambda_\n",
        "\n",
        "        adv_x = x.detach().clone().to(torch.double)\n",
        "        while self.lambda_ <= max_lambda_:\n",
        "            with torch.no_grad():\n",
        "                _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            pert_x = self._perturb(model, adv_x[~done], label[~done],\n",
        "                                   steps,\n",
        "                                   step_length,\n",
        "                                   lambda_=self.lambda_\n",
        "                                   )\n",
        "            adv_x[~done] = pert_x\n",
        "            self.lambda_ *= base\n",
        "        with torch.no_grad():\n",
        "            _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            print(f\"pgd {self.norm}: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "        return adv_x\n",
        "\n",
        "    def get_perturbation(self, gradients, features, adv_features):\n",
        "        # look for allowable position, because only '1--> -' and '0 --> +' are permitted\n",
        "        # api insertion\n",
        "        pos_insertion = (adv_features <= 0.5) * 1 * (adv_features >= 0.)\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "        # grad4insertion = (gradients > 0) * gradients\n",
        "        # api removal\n",
        "        pos_removal = (adv_features > 0.5) * 1\n",
        "        grad4removal = (gradients < 0) * (pos_removal & self.manipulation_x) * gradients\n",
        "        if self.is_attacker:\n",
        "            # cope with the interdependent apis\n",
        "            checking_nonexist_api = (pos_removal ^ self.omega) & self.omega\n",
        "            grad4removal[:, self.api_flag] += torch.sum(gradients * checking_nonexist_api, dim=-1, keepdim=True)\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # norm\n",
        "        if self.norm == 'linf':\n",
        "            perturbation = torch.sign(gradients)\n",
        "        elif self.norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = torch.minimum(\n",
        "                torch.tensor(1., dtype=features.dtype, device=features.device),\n",
        "                gradients / l2norm\n",
        "            )\n",
        "            perturbation = torch.where(torch.isnan(perturbation), 0., perturbation)\n",
        "            perturbation = torch.where(torch.isinf(perturbation), 1., perturbation)\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # add the extra perturbation owing to the interdependent apis\n",
        "        if self.norm == 'linf' and self.is_attacker:\n",
        "            perturbation += torch.any(perturbation[:, self.api_flag] < 0, dim=-1,\n",
        "                                      keepdim=True) * checking_nonexist_api\n",
        "        if self.norm == 'l2' and self.is_attacker:\n",
        "            min_val = torch.amin(perturbation, dim=-1, keepdim=True).clamp_(max=0.)\n",
        "            perturbation += (torch.any(perturbation[:, self.api_flag] < 0, dim=-1,\n",
        "                                       keepdim=True) * torch.abs(min_val) * checking_nonexist_api)\n",
        "        return perturbation\n",
        "\n",
        "    def get_loss(self, model, adv_x, label, lambda_=None):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(adv_x)\n",
        "        else:\n",
        "            logits_f = model.forward(adv_x)\n",
        "\n",
        "        ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            assert lambda_ is not None\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            if self.is_attacker:\n",
        "                loss_no_reduction = ce + lambda_ * (torch.clamp(tau - prob_g,\n",
        "                                                                max=self.kappa)\n",
        "                                                    )\n",
        "            else:\n",
        "                loss_no_reduction = ce + lambda_ * (tau - prob_g)\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            loss_no_reduction = ce\n",
        "            done = y_pred != label\n",
        "        return loss_no_reduction, done"
      ],
      "metadata": {
        "id": "Wd8Q6mNduaya"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PGDl1():\n",
        "    \"\"\"\n",
        "    Projected gradient descent (ascent) with gradients 'normalized' using l1 norm.\n",
        "    By comparing BCA, the api removal is leveraged\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    @param is_attacker, Boolean, play the role of attacker (note: the defender conducts adversarial training)\n",
        "    @param oblivion, Boolean, whether know the adversary indicator or not\n",
        "    @param kappa, attack confidence\n",
        "    @param manipulation_x, manipulations\n",
        "    @param omega, the indices of interdependent apis corresponding to each api\n",
        "    @param device, 'cpu' or 'cuda'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_attacker=True, oblivion=False, kappa=1., manipulation_x=None, omega=None, api_flag=None, device=None):\n",
        "        super(PGDl1, self).__init__()\n",
        "        self.is_attacker = is_attacker\n",
        "        self.lambda_ = 1.\n",
        "        self.omeag = omega\n",
        "        self.manipulation_x = manipulation_x\n",
        "        self.api_flag = api_flag\n",
        "        self.device = device\n",
        "\n",
        "    def _perturb(self, model, x,  label=None,\n",
        "                 steps=10,\n",
        "                 lambda_=1.):\n",
        "        \"\"\"\n",
        "        perturb node feature vectors\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        @param model, a victim model\n",
        "        @param x: torch.FloatTensor, node feature vectors (each represents the occurrences of apis in a graph) with shape [batch_size, number_of_graphs, vocab_dim]\n",
        "        @param label: torch.LongTensor, ground truth labels\n",
        "        @param steps: Integer, maximum number of perturbations\n",
        "        @param lambda_, float, penalty factor\n",
        "        \"\"\"\n",
        "        adv_x = x\n",
        "        worst_x = x.detach().clone()\n",
        "        self.lambda_ = lambda_\n",
        "        self.padding_mask = torch.sum(adv_x, dim=-1, keepdim=True) > 1  # we set a graph contains two apis at least\n",
        "        model.eval()\n",
        "        for t in range(steps):\n",
        "            var_adv_x = torch.autograd.Variable(adv_x, requires_grad=True)\n",
        "            loss, done = self.get_loss(model, var_adv_x, label, self.lambda_)\n",
        "            worst_x[done] = adv_x[done]\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            grad = torch.autograd.grad(torch.mean(loss), var_adv_x)[0]\n",
        "            perturbation, direction = self.get_perturbation(grad, x, adv_x)\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            perturbation[done] = 0.\n",
        "            adv_x = torch.clamp(adv_x + perturbation * direction, min=0., max=1.)\n",
        "        done = self.get_scores(model, adv_x, label)\n",
        "        worst_x[done] = adv_x[done]\n",
        "        return worst_x\n",
        "\n",
        "    def perturb(self, model, x, label=None,\n",
        "                steps=10,\n",
        "                min_lambda_=1e-5,\n",
        "                max_lambda_=1e5,\n",
        "                base=10.,\n",
        "                verbose=True):\n",
        "        \"\"\"\n",
        "        enhance attack\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            self.lambda_ = min_lambda_\n",
        "        else:\n",
        "            self.lambda_ = max_lambda_\n",
        "        adv_x = x.detach().clone().to(torch.double)\n",
        "        while self.lambda_ <= max_lambda_:\n",
        "            _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            pert_x = self._perturb(model, adv_x[~done], label[~done],\n",
        "                                   steps,\n",
        "                                   lambda_=self.lambda_\n",
        "                                   )\n",
        "            adv_x[~done] = pert_x\n",
        "            self.lambda_ *= base\n",
        "            #if not self.check_lambda(model):\n",
        "                #break\n",
        "        with torch.no_grad():\n",
        "            _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            if verbose:\n",
        "                print(f\"pgd l1: attack effectiveness {done.sum().item() / x.size()[0]}.\")\n",
        "        return adv_x\n",
        "\n",
        "    def get_perturbation(self, gradients, features, adv_features):\n",
        "        # 1. mask paddings\n",
        "        gradients = gradients * self.padding_mask\n",
        "\n",
        "        # 2. look for allowable position, because only '1--> -' and '0 --> +' are permitted\n",
        "        #    2.1 api insertion\n",
        "        pos_insertion = (adv_features <= 0.5) * 1\n",
        "        grad4insertion = (gradients > 0) * pos_insertion * gradients\n",
        "        #    2.2 api removal\n",
        "        pos_removal = (adv_features > 0.5) * 1\n",
        "        grad4removal = (gradients <= 0) * (pos_removal & self.manipulation_x) * gradients\n",
        "        if self.is_attacker:\n",
        "            #     2.2.1 cope with the interdependent apis\n",
        "            checking_nonexist_api = (pos_removal ^ self.omega) & self.omega\n",
        "            grad4removal[:, self.api_flag] += torch.sum(gradients * checking_nonexist_api, dim=-1, keepdim=True)\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # 3. remove duplications\n",
        "        un_mod = torch.abs(features - adv_features) <= 1e-6\n",
        "        gradients = gradients * un_mod\n",
        "\n",
        "        # 4. look for important position\n",
        "        absolute_grad = torch.abs(gradients).reshape(features.shape[0], -1)\n",
        "        _, position = torch.max(absolute_grad, dim=-1)\n",
        "        perturbations = F.one_hot(position, num_classes=absolute_grad.shape[-1]).double()\n",
        "        perturbations = perturbations.reshape(features.shape)\n",
        "        directions = torch.sign(gradients) * (perturbations > 1e-6)\n",
        "\n",
        "        # 5. tailor the interdependent apis\n",
        "        if self.is_attacker:\n",
        "            perturbations += (torch.any(directions[:, self.api_flag] < 0, dim=-1, keepdim=True)) * checking_nonexist_api\n",
        "            directions += perturbations * self.omega\n",
        "        return perturbations, directions\n",
        "\n",
        "\n",
        "    def get_loss(self, model, adv_x, label, lambda_=None):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(adv_x)\n",
        "        else:\n",
        "            logits_f = model.forward(adv_x)\n",
        "\n",
        "        ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            assert lambda_ is not None\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            if self.is_attacker:\n",
        "                loss_no_reduction = ce + lambda_ * (torch.clamp(tau - prob_g,\n",
        "                                                                max=self.kappa)\n",
        "                                                    )\n",
        "            else:\n",
        "                loss_no_reduction = ce + lambda_ * (tau - prob_g)\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            loss_no_reduction = ce\n",
        "            done = y_pred != label\n",
        "        return loss_no_reduction, done\n",
        "\n",
        "    def get_scores(self, model, pertb_x, label, lmda=1.):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(pertb_x)\n",
        "        else:\n",
        "            logits_f = model.forward(pertb_x)\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            done = y_pred != label\n",
        "        return done"
      ],
      "metadata": {
        "id": "q-FgY4O2hy2g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P2qjRebzAU2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQzXOfTmAUtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.load('/content/malware_detection/datasets/train_dataset.pt')\n",
        "train_Loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "validation_dataset = torch.load('/content/malware_detection/datasets/validation_dataset.pt')\n",
        "validation_Loader = data.DataLoader(validation_dataset, batch_size=10000, shuffle=False)\n",
        "test_dataset = torch.load('/content/malware_detection/datasets/test_dataset.pt')\n",
        "test_Loader = data.DataLoader(test_dataset, batch_size=10000, shuffle=False)"
      ],
      "metadata": {
        "id": "MRAX7j_qywXf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/malware_detection/dataset/drebin/data.vocab', 'rb') as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "\n",
        "print(len(loaded_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngwMMct9nohS",
        "outputId": "d6a1e3de-4344-4234-e773-e8453bce4d15"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples,labels = next(iter(test_Loader))\n",
        "x,label = samples[0:5].double(),labels[0:5].long()"
      ],
      "metadata": {
        "id": "Aunmabl3Xb6d"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "from functools import partial\n",
        "\n",
        "# A normal function\n",
        "def f(a, b, c, x):\n",
        "    return 1000*a + 100*b + 10*c + x\n",
        "\n",
        "# A partial function that calls f with\n",
        "# a as 3, b as 1 and c as 4.\n",
        "g = partial(f, 3, 1, 4)\n",
        "\n",
        "# Calling g()\n",
        "print(g(5))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XAgKob_UFdxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MalwareDetectionDNN(6693,2)\n",
        "model = model.double().to(device)\n",
        "\n",
        "pgdlinf = PGD(norm='linf', use_random=False,is_attacker=False,manipulation_x=manipulation_x, omega=omega,api_flag=api_flag,device=device)\n",
        "pgdlinf.perturb = partial(pgdlinf.perturb, steps=10, step_length=1.)\n",
        "\n",
        "pgdl2 = PGD(norm='l2', use_random=False, is_attacker=False,manipulation_x=manipulation_x, omega=omega,api_flag=api_flag, device=device)\n",
        "pgdl2.perturb = partial(pgdl2.perturb, steps=10, step_length=1.)\n",
        "\n",
        "pgdl1 = PGDl1(is_attacker=False,manipulation_x=manipulation_x, omega=omega,api_flag=api_flag,device=device)\n",
        "pgdl1.perturb = partial(pgdl1.perturb, steps=10)\n"
      ],
      "metadata": {
        "id": "tHO333FtBO55"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pgdlinf.perturb(model,x,label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIiiATuQC1LD",
        "outputId": "01effbc0-1d30-4178-f377-acc767eb026d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd linf: attack effectiveness 100.000%.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.,  ..., 0., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
              "        [0., 0., 1.,  ..., 1., 1., 1.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Max():\n",
        "    \"\"\"\n",
        "    max attack: select results from several attacks iteratively\n",
        "\n",
        "    Parameters\n",
        "    --------\n",
        "    @param attack_list: List, a list of instantiated attack object\n",
        "    @param varepsilon: Float, a scaler for justifying the convergence\n",
        "    \"\"\"\n",
        "    def __init__(self, attack_list, varepsilon=1e-20,is_attacker=True, oblivion=False, kappa=1., manipulation_x=None, omega=None, device=None):\n",
        "        super(Max, self).__init__()\n",
        "        assert len(attack_list) > 0, 'Expect one attack at least.'\n",
        "        self.attack_list = attack_list\n",
        "        self.varepsilon = varepsilon\n",
        "        self.device = device\n",
        "\n",
        "    def perturb(self, model, x, label=None, steps_max=5, min_lambda_=1e-5, max_lambda_=1e5, verbose=False):\n",
        "        \"\"\"\n",
        "        perturb node features\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        @param model, a victim model\n",
        "        @param x: torch.FloatTensor, feature vectors with shape [batch_size, vocab_dim]\n",
        "        @param label: torch.LongTensor, ground truth labels\n",
        "        @param steps_max: Integer, maximum number of iterations\n",
        "        @param min_lambda_: float, balance the importance of adversary detector, if it exists\n",
        "        @param max_lambda_: float, balance the importance of adversary detector, if it exists\n",
        "        @param verbose: Boolean, print verbose log\n",
        "        \"\"\"\n",
        "        if x is None or x.shape[0] <= 0:\n",
        "            return []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            loss, done = self.get_scores(model, x, label) #shape:[samples],[samples]\n",
        "        pre_loss = loss\n",
        "        n, red_n = x.size()[0], x.size()[1:] #samples,features\n",
        "        red_ind = list(range(2, len(x.size()) + 1)) #2\n",
        "        adv_x = x.detach().clone()\n",
        "        stop_flag = torch.zeros(n, dtype=torch.bool, device=self.device) #[samples]\n",
        "        for t in range(steps_max):\n",
        "            num_sample_red = n - torch.sum(stop_flag)\n",
        "            if num_sample_red <= 0:\n",
        "                break\n",
        "\n",
        "            red_label = label[~stop_flag]\n",
        "            pertbx = []\n",
        "            for attack in self.attack_list:\n",
        "                assert 'perturb' in type(attack).__dict__.keys()\n",
        "                if t > 0 and 'use_random' in attack.__dict__.keys():\n",
        "                    attack.use_random = False\n",
        "                if 'Orthogonal' in type(attack).__name__:\n",
        "                    pertbx.append(attack.perturb(model=model, x=adv_x[~stop_flag], label=red_label))\n",
        "                else:\n",
        "                    pertbx.append(attack.perturb(model=model, x=adv_x[~stop_flag], label=red_label,\n",
        "                                                 min_lambda_=1e-5,\n",
        "                                                 max_lambda_=1e5,\n",
        "                                                 ))\n",
        "            # here pertbx.shape = a list of (number of attacks  ,(num_sample_red,features))\n",
        "            pertbx = torch.vstack(pertbx)\n",
        "            # here pertbx.shape = a tensor (num_sample_red*number of attacks samples, features)\n",
        "            with torch.no_grad():\n",
        "                red_label_ext = torch.cat([red_label] * len(self.attack_list)) #(labels*number of attacks )\n",
        "                loss, done = self.get_scores(model, pertbx, red_label_ext) #(labels*number of attacks )\n",
        "                loss = loss.reshape(len(self.attack_list), num_sample_red).permute(1, 0) #(num_sample_red,number of attacks) IMPORTANT: NOT(number of attacks, samples)\n",
        "                done = done.reshape(len(self.attack_list), num_sample_red).permute(1, 0) #(num_sample_red,number of attacks)\n",
        "                success_flag = torch.any(done, dim=-1) #(num_sample_red)\n",
        "                # for a sample, if there is at least one successful attack, we will select the one with maximum loss;\n",
        "                # while if no attacks evade the victim successful, all perturbed examples are reminded for selection\n",
        "                done[~torch.any(done, dim=-1)] = 1 #loss.shape=done.shape=(samples,number of attacks)\n",
        "                loss = (loss * done.to(torch.float)) + torch.min(loss) * (~done).to(torch.float) #(num_sample_red,number of attacks)\n",
        "                pertbx = pertbx.reshape(len(self.attack_list), num_sample_red, *red_n).permute([1, 0, *red_ind])#(num_sample_red,attacks,features)\n",
        "                _, indices = loss.max(dim=-1) # ans:(samples), max loss among attacks which worked, and max loss among all attacks for sample , none of them worked\n",
        "                adv_x[~stop_flag] = pertbx[torch.arange(num_sample_red), indices]\n",
        "                a_loss = loss[torch.arange(num_sample_red), indices]\n",
        "                pre_stop_flag = stop_flag.clone()\n",
        "                stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < self.varepsilon) | success_flag\n",
        "                pre_loss[~pre_stop_flag] = a_loss\n",
        "        if verbose:\n",
        "            with torch.no_grad():\n",
        "                _, done = self.get_scores(model, adv_x, label)\n",
        "                print(f\"max: attack effectiveness {done.sum().item() / x.size()[0] * 100}%.\")\n",
        "        return adv_x\n",
        "\n",
        "    def get_scores(self, model, pertb_x, label):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(pertb_x)\n",
        "        else:\n",
        "            logits_f = model.forward(pertb_x)\n",
        "        ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            loss_no_reduction = -prob_g\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            loss_no_reduction = ce\n",
        "            done = y_pred != label\n",
        "        return loss_no_reduction, done"
      ],
      "metadata": {
        "id": "P00GaglimTRy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack = Max(attack_list=[pgdlinf, pgdl2, pgdl1],varepsilon=1e-9,is_attacker=False,device=device)\n",
        "attack.perturb(model,x,label,steps_max=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqZT2OCgoMI4",
        "outputId": "685436db-4eb5-468d-95f2-1dcb5650d63c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd linf: attack effectiveness 100.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.0.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.,  ..., 0., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
              "        [0., 0., 1.,  ..., 1., 1., 1.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PGDAdvTraining(object):\n",
        "    \"\"\"adversarial training incorporating pgd attack\n",
        "\n",
        "    Parameters\n",
        "    -------\n",
        "    @param model, Object,  a model to be protected, e.g., MalwareDetector\n",
        "    @attack_model: Object, adversary's model for generating adversarial malware on the feature space\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, attack=None, attack_param=None):\n",
        "        self.model = model\n",
        "        self.attack = attack\n",
        "        self.attack_param = attack_param\n",
        "\n",
        "        self.name = self.model.name\n",
        "        self.model_save_path =  path.join('/content/malware_detection/save/md_at_pgd' + '_' + 'train','model.pth')\n",
        "\n",
        "    def fit(self, train_data_producer, validation_data_producer=None, adv_epochs=45,beta=0.001,lr=0.005,weight_decay=0, verbose=True):\n",
        "        \"\"\"\n",
        "        Applying adversarial train to enhance the malware detector.\n",
        "\n",
        "        Parameters\n",
        "        -------\n",
        "        @param train_data_producer: Object, an dataloader object for producing a batch of training data\n",
        "        @param validation_data_producer: Object, an dataloader object for producing validation dataset\n",
        "        @param epochs: Integer, epochs for adversarial training\n",
        "        @param adv_epochs: Integer, epochs for adversarial training\n",
        "        @param beta: Float, penalty factor for adversarial loss\n",
        "        @param lr: Float, learning rate of Adam optimizer\n",
        "        @param weight_decay: Float, penalty factor, default value 5e-4\n",
        "        @param verbose: Boolean, whether to show verbose info\n",
        "        \"\"\"\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        total_time = 0.\n",
        "        nbatches = len(train_data_producer)\n",
        "        best_acc_val = 0.\n",
        "        acc_val_adv_be = 0.\n",
        "        best_epoch = 0\n",
        "        for i in range(adv_epochs):\n",
        "            losses, accuracies = [], []\n",
        "            for idx_batch, (x_batch, y_batch) in enumerate(train_data_producer):\n",
        "                x_batch, y_batch = x_batch.double().to(device), y_batch.long().to(device)\n",
        "                batch_size = x_batch.shape[0]\n",
        "                # make data\n",
        "                mal_x_batch, ben_x_batch, mal_y_batch, ben_y_batch = x_batch[y_batch == 1], x_batch[y_batch == 0], y_batch[y_batch == 1], y_batch[y_batch == 0]\n",
        "                start_time = time.time()\n",
        "                self.model.eval()\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "                total_time += time.time() - start_time\n",
        "                x_batch = torch.cat([ben_x_batch, pertb_mal_x], dim=0)\n",
        "                y_batch = torch.cat([ben_y_batch, mal_y_batch])\n",
        "                start_time = time.time()\n",
        "                self.model.train()\n",
        "                optimizer.zero_grad()\n",
        "                logits = self.model.forward(x_batch)\n",
        "                loss_train = self.model.customize_loss(logits,y_batch)\n",
        "\n",
        "                loss_train.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_time += time.time() - start_time\n",
        "                mins, secs = int(total_time / 60), int(total_time % 60)\n",
        "                acc_train = (logits.argmax(1) == y_batch).sum().item()\n",
        "                acc_train /= x_batch.size()[0]\n",
        "                accuracies.append(acc_train)\n",
        "                losses.append(loss_train.item())\n",
        "                if verbose:\n",
        "                    print(f'Mini batch: {i * nbatches + idx_batch + 1}/{adv_epochs * nbatches} | training time in {mins:.0f} minutes, {secs} seconds.')\n",
        "                    print(f'Training loss (batch level): {losses[-1]:.4f} | Train accuracy: {acc_train * 100:.2f}%.')\n",
        "            if verbose:\n",
        "                print(f'Training loss (epoch level): {np.mean(losses):.4f} | Train accuracy: {np.mean(accuracies) * 100:.2f}')\n",
        "\n",
        "            # select model\n",
        "            self.model.eval()\n",
        "            # long-time to train (save the model temporally in case of interruption)\n",
        "            self.save_to_disk(i + 1, optimizer, self.model_save_path + '.tmp')\n",
        "\n",
        "            res_val = []\n",
        "            avg_acc_val = []\n",
        "            for x_val, y_val in validation_data_producer:\n",
        "                x_val, y_val = x_val.double().to(device), y_val.long().to(device)\n",
        "                logits = self.model.forward(x_val)\n",
        "                acc_val = (logits.argmax(1) == y_val).sum().item()\n",
        "                acc_val /= x_val.size()[0]\n",
        "                avg_acc_val.append(acc_val)\n",
        "\n",
        "                mal_x_batch, mal_y_batch = x_val[y_val == 1], y_val[y_val == 1]\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "                y_cent_batch, x_density_batch = self.model.inference_batch_wise(pertb_mal_x)\n",
        "                y_pred = np.argmax(y_cent_batch, axis=-1)\n",
        "                res_val.append(y_pred == 1.)\n",
        "            assert len(res_val) > 0\n",
        "            res_val = np.concatenate(res_val)\n",
        "            acc_val_adv = np.sum(res_val).astype(float) / res_val.shape[0]\n",
        "            acc_val = (np.mean(avg_acc_val) + acc_val_adv) / 2.\n",
        "            # Owing to we look for a new threshold after each epoch, this hinders the convergence of training.\n",
        "            # We save the model's parameters at last several epochs as a well-trained model may be obtained.\n",
        "            if acc_val >= best_acc_val:\n",
        "                best_acc_val = acc_val\n",
        "                acc_val_adv_be = acc_val_adv\n",
        "                best_epoch = i + 1\n",
        "                self.save_to_disk(best_epoch, optimizer, self.model_save_path)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\tVal accuracy {acc_val * 100:.4}% with accuracy {acc_val_adv * 100:.4}% under attack.\")\n",
        "                print(f\"\\tModel select at epoch {best_epoch} with validation accuracy {best_acc_val * 100:.4}% and accuracy {acc_val_adv_be * 100:.4}% under attack.\")\n",
        "                if hasattr(self.model, 'tau'):\n",
        "                    print(f'The threshold is {self.model.tau}.')\n",
        "\n",
        "    #we define here not in model definition, beacause we need attack.pertube()\n",
        "    #also we must not use \"with no grad\" because we need grad for pertubation\n",
        "    def adv_predict(self, test_data_producer):\n",
        "      res_test = []\n",
        "      for x, y in test_data_producer:\n",
        "          x, y = x.double().to(device), y.long().to(device)\n",
        "\n",
        "          mal_x_batch, mal_y_batch = x[y == 1], y[y == 1]\n",
        "          pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "          y_cent_batch, x_density_batch = self.model.inference_batch_wise(pertb_mal_x)\n",
        "          y_pred = np.argmax(y_cent_batch, axis=-1)\n",
        "          res_test.append(y_pred == 1.)\n",
        "      assert len(res_test) > 0\n",
        "      res_test = np.concatenate(res_test)\n",
        "      acc_val_adv = np.sum(res_test).astype(float) / res_test.shape[0]\n",
        "      print(f\"\\tadversarial accuracy  {acc_val_adv * 100:.4}% under attack.\")\n",
        "\n",
        "\n",
        "    def load(self):\n",
        "        assert path.exists(self.model_save_path), 'train model first'\n",
        "        ckpt = torch.load(self.model_save_path)\n",
        "        self.model.load_state_dict(ckpt['model'])\n",
        "\n",
        "    def save_to_disk(self, epoch, optimizer, save_path=None):\n",
        "        if not path.exists(save_path):\n",
        "            os.makedirs(path.dirname(save_path), exist_ok = True)\n",
        "        torch.save({'model': self.model.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'optimizer_state_dict': optimizer.state_dict()\n",
        "                    },\n",
        "                   save_path)"
      ],
      "metadata": {
        "id": "4eqeD6NWvqhF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxAdvTraining(object):\n",
        "    \"\"\"adversarial training incorporating mixture of attacks\n",
        "\n",
        "    Parameters\n",
        "    -------\n",
        "    @param model, Object,  a model to be protected, e.g., MalwareDetector\n",
        "    @attack_model: Object, adversary's model for generating adversarial malware on the feature space\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, attack=None, attack_param=None):\n",
        "        self.model = model\n",
        "        if attack is not None:\n",
        "            #assert isinstance(attack, (Max, StepwiseMax))\n",
        "            if 'is_attacker' in attack.__dict__.keys():\n",
        "                assert not attack.is_attacker\n",
        "        self.attack = attack\n",
        "        self.attack_param = attack_param\n",
        "\n",
        "        self.name = self.model.name\n",
        "        self.model_save_path =  path.join('/content/malware_detection/save/md_at_ma' + '_' + 'train','model.pth')\n",
        "\n",
        "    def fit(self, train_data_producer, validation_data_producer=None, adv_epochs=50,\n",
        "            beta=0.01,\n",
        "            lr=0.001,\n",
        "            under_sampling_ratio=1.,\n",
        "            weight_decay=0, verbose=True):\n",
        "        \"\"\"\n",
        "        Applying adversarial train to enhance the malware detector.\n",
        "\n",
        "        Parameters\n",
        "        -------\n",
        "        @param train_data_producer: Object, an dataloader object for producing a batch of training data\n",
        "        @param validation_data_producer: Object, an dataloader object for producing validation dataset\n",
        "        @param epochs: Integer, epochs for adversarial training\n",
        "        @param adv_epochs: Integer, epochs for adversarial training\n",
        "        @param beta: Float, penalty factor for adversarial loss\n",
        "        @param lr: Float, learning rate of Adam optimizer\n",
        "        @param under_sampling_ratio: [0,1], under-sampling a portion of malware examples for adversarial training\n",
        "        @param weight_decay: Float, penalty factor, default value 5e-4\n",
        "        @param verbose: Boolean, whether to show verbose info\n",
        "        \"\"\"\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        total_time = 0.\n",
        "        nbatches = len(train_data_producer)\n",
        "        print(\"Max adversarial training is starting ...\")\n",
        "        best_acc_val = 0.\n",
        "        acc_val_adv_be = 0.\n",
        "        best_epoch = 0\n",
        "        for i in range(adv_epochs):\n",
        "            random.seed(0)\n",
        "            losses, accuracies = [], []\n",
        "            for idx_batch, (x_batch, y_batch) in enumerate(train_data_producer):\n",
        "                x_batch, y_batch = x_batch.double().to(device), y_batch.long().to(device)\n",
        "                batch_size = x_batch.shape[0]\n",
        "                # make data\n",
        "                mal_x_batch, ben_x_batch, mal_y_batch, ben_y_batch = x_batch[y_batch == 1], x_batch[y_batch == 0], y_batch[y_batch == 1], y_batch[y_batch == 0]\n",
        "                if 0. < under_sampling_ratio < 1.:\n",
        "                    n_mal = mal_x_batch.shape[0]\n",
        "                    n_mal_sampling = int(under_sampling_ratio * n_mal) if int(under_sampling_ratio * n_mal) > 1 else 1\n",
        "                    idx_sampling = random.sample(range(n_mal), n_mal_sampling)\n",
        "                    mal_x_batch, mal_y_batch = mal_x_batch[idx_sampling], mal_y_batch[idx_sampling]\n",
        "\n",
        "                if null_flag:\n",
        "                    continue\n",
        "                start_time = time.time()\n",
        "                self.model.eval()\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "                pertb_mal_x = round_x(pertb_mal_x, 0.5)\n",
        "                total_time += time.time() - start_time\n",
        "                x_batch = torch.cat([x_batch, pertb_mal_x], dim=0)\n",
        "                y_batch = torch.cat([y_batch, mal_y_batch])\n",
        "                start_time = time.time()\n",
        "                self.model.train()\n",
        "                optimizer.zero_grad()\n",
        "                logits = self.model.forward(x_batch)\n",
        "                loss_train = self.model.customize_loss(logits[:batch_size],y_batch[:batch_size])\n",
        "                loss_train += beta * self.model.customize_loss(logits[batch_size:],y_batch[batch_size:])\n",
        "\n",
        "                loss_train.backward()\n",
        "                optimizer.step()\n",
        "                total_time += time.time() - start_time\n",
        "                mins, secs = int(total_time / 60), int(total_time % 60)\n",
        "                acc_train = (logits.argmax(1) == y_batch).sum().item()\n",
        "                acc_train /= x_batch.size()[0]\n",
        "                accuracies.append(acc_train)\n",
        "                losses.append(loss_train.item())\n",
        "                if verbose:\n",
        "                    logger.info(\n",
        "                        f'Mini batch: {i * nbatches + idx_batch + 1}/{adv_epochs * nbatches} | training time in {mins:.0f} minutes, {secs} seconds.')\n",
        "                    logger.info(\n",
        "                        f'Training loss (batch level): {losses[-1]:.4f} | Train accuracy: {acc_train * 100:.2f}%.')\n",
        "            if verbose:\n",
        "                logger.info(\n",
        "                    f'Training loss (epoch level): {np.mean(losses):.4f} | Train accuracy: {np.mean(accuracies) * 100:.2f}')\n",
        "\n",
        "            # select model\n",
        "            self.model.eval()\n",
        "            # long-time to train (save the model temporally in case of interruption)\n",
        "            self.save_to_disk(i + 1, optimizer, self.model_save_path + '.tmp')\n",
        "            self.attack.is_attacker = True\n",
        "            res_val = []\n",
        "            avg_acc_val = []\n",
        "            for x_val, y_val in validation_data_producer:\n",
        "                x_val, y_val = utils.to_tensor(x_val.double(), y_val.long(), self.model.device)\n",
        "                logits = self.model.forward(x_val)\n",
        "                acc_val = (logits.argmax(1) == y_val).sum().item()\n",
        "                acc_val /= x_val.size()[0]\n",
        "                avg_acc_val.append(acc_val)\n",
        "\n",
        "                mal_x_batch, mal_y_batch, null_flag = utils.get_mal_data(x_val, y_val)\n",
        "                if null_flag:\n",
        "                    continue\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,\n",
        "                                                  **self.attack_param\n",
        "                                                  )\n",
        "                pertb_mal_x = utils.round_x(pertb_mal_x, 0.5)\n",
        "                y_cent_batch, x_density_batch = self.model.inference_batch_wise(pertb_mal_x)\n",
        "                y_pred = np.argmax(y_cent_batch, axis=-1)\n",
        "                res_val.append(y_pred == 1.)\n",
        "            assert len(res_val) > 0\n",
        "            res_val = np.concatenate(res_val)\n",
        "            acc_val_adv = np.sum(res_val).astype(np.float) / res_val.shape[0]\n",
        "            acc_val = (np.mean(avg_acc_val) + acc_val_adv) / 2.\n",
        "            # Owing to we look for a new threshold after each epoch, this hinders the convergence of training.\n",
        "            # We save the model's parameters at last several epochs as a well-trained model may be obtained.\n",
        "            if acc_val >= best_acc_val:\n",
        "                best_acc_val = acc_val\n",
        "                acc_val_adv_be = acc_val_adv\n",
        "                best_epoch = i + 1\n",
        "                self.save_to_disk(best_epoch, optimizer, self.model_save_path)\n",
        "\n",
        "            if verbose:\n",
        "                logger.info(\n",
        "                    f\"\\tVal accuracy {acc_val * 100:.4}% with accuracy {acc_val_adv * 100:.4}% under attack.\")\n",
        "                logger.info(\n",
        "                    f\"\\tModel select at epoch {best_epoch} with validation accuracy {best_acc_val * 100:.4}% and accuracy {acc_val_adv_be * 100:.4}% under attack.\")\n",
        "                if hasattr(self.model, 'tau'):\n",
        "                    logger.info(\n",
        "                        f'The threshold is {self.model.tau}.'\n",
        "                    )\n",
        "            self.attack.is_attacker = False\n",
        "\n",
        "    def load(self):\n",
        "        assert path.exists(self.model_save_path), 'train model first'\n",
        "        ckpt = torch.load(self.model_save_path)\n",
        "        # self.model.tau = ckpt['tau']\n",
        "        # self.model.md_nn_model.load_state_dict(ckpt['md_model'])\n",
        "        # self.model.load_state_dict(ckpt['amd_model'])\n",
        "        self.model.load_state_dict(ckpt['model'])\n",
        "\n",
        "    def save_to_disk(self, epoch, optimizer, save_path=None):\n",
        "        if not path.exists(save_path):\n",
        "            utils.mkdir(path.dirname(save_path))\n",
        "        # torch.save({'tau': self.model.tau,\n",
        "        #             'md_model': self.model.md_nn_model.state_dict(),\n",
        "        #             'amd_model': self.model.state_dict(),\n",
        "        #             'epoch': epoch,\n",
        "        #             'optimizer_state_dict': optimizer.state_dict()\n",
        "        #             },\n",
        "        #            save_path)\n",
        "        torch.save({'model': self.model.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'optimizer_state_dict': optimizer.state_dict()\n",
        "                    },\n",
        "                   save_path)"
      ],
      "metadata": {
        "id": "ep4KRO56n0nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack = Max(attack_list=[pgdlinf, pgdl2, pgdl1],varepsilon=1e-9,is_attacker=False,device=device)\n",
        "attack_param = {\n",
        "            'steps_max': 1,  # steps for max attack\n",
        "            'verbose': True\n",
        "              }\n",
        "max_adv_training_model = MaxAdvTraining(model, attack, attack_param)\n",
        "\n",
        "max_adv_training_model.fit(train_dataset_producer,\n",
        "                                   val_dataset_producer,\n",
        "                                   adv_epochs=args.epochs,\n",
        "                                   beta=args.beta,\n",
        "                                   lr=args.lr,\n",
        "                                   under_sampling_ratio=args.under_sampling,\n",
        "                                   weight_decay=args.weight_decay\n",
        "                                   )"
      ],
      "metadata": {
        "id": "RptuASbsmQly"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erNp0A5_mW98",
        "outputId": "de53c007-0baf-463c-9d63-c9f74b4a77e0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Max at 0x790930df1780>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6KZfsBWmmr1",
        "outputId": "ee66efc3-7865-4af4-b8ed-5a08ffd2caeb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 6693])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_list = [pgdlinf,pgdl2,pgdl1]\n",
        "varepsilon=1e-20\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    loss, done = get_scores(model, x, label)\n",
        "    print(loss.shape)\n",
        "    print(done.shape)\n",
        "pre_loss = loss\n",
        "n, red_n = x.size()[0], x.size()[1:]\n",
        "print(n)\n",
        "print(red_n)\n",
        "red_ind = list(range(2, len(x.size()) + 1))\n",
        "adv_x = x.detach().clone()\n",
        "stop_flag = torch.zeros(n, dtype=torch.bool)\n",
        "for t in range(5):\n",
        "    num_sample_red = n - torch.sum(stop_flag)\n",
        "    if num_sample_red <= 0:\n",
        "        break\n",
        "\n",
        "    red_label = label[~stop_flag]\n",
        "    pertbx = []\n",
        "    for attack in attack_list:\n",
        "        assert 'perturb' in type(attack).__dict__.keys()\n",
        "        if t > 0 and 'use_random' in attack.__dict__.keys():\n",
        "            attack.use_random = False\n",
        "        if 'Orthogonal' in type(attack).__name__:\n",
        "            pertbx.append(attack.perturb(model=model, x=adv_x[~stop_flag], label=red_label))\n",
        "        else:\n",
        "            pertbx.append(attack.perturb(model=model, x=adv_x[~stop_flag], label=red_label,min_lambda_=1e-5,max_lambda_=1e5))\n",
        "\n",
        "    pertbx = torch.vstack(pertbx)\n",
        "    with torch.no_grad():\n",
        "        red_label_ext = torch.cat([red_label] * len(attack_list))\n",
        "        loss, done = get_scores(model, pertbx, red_label_ext)\n",
        "        loss = loss.reshape(len(attack_list), num_sample_red).permute(1, 0)\n",
        "        done = done.reshape(len(attack_list), num_sample_red).permute(1, 0)\n",
        "        print(done)\n",
        "        success_flag = torch.any(done, dim=-1)\n",
        "        print('success_flag : ',success_flag)\n",
        "        done[~torch.any(done, dim=-1)] = 1\n",
        "        print('done : ',done)\n",
        "        print('loss :',loss.shape)\n",
        "        print(torch.min(loss) * (~done).to(torch.float))\n",
        "        loss = (loss * done.to(torch.float)) + torch.min(loss) * (~done).to(torch.float)\n",
        "        print('loss : ',loss)\n",
        "        pertbx = pertbx.reshape(len(attack_list), num_sample_red, *red_n).permute([1, 0, *red_ind])\n",
        "        print(pertbx.shape)\n",
        "        _, indices = loss.max(dim=-1)\n",
        "        print(indices.shape)\n",
        "        adv_x[~stop_flag] = pertbx[torch.arange(num_sample_red), indices]\n",
        "        a_loss = loss[torch.arange(num_sample_red), indices]\n",
        "        pre_stop_flag = stop_flag.clone()\n",
        "        stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < varepsilon) | success_flag\n",
        "        pre_loss[~pre_stop_flag] = a_loss\n",
        "\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NZZ-fH7i2eO",
        "outputId": "514715ef-726f-4af7-87aa-9ef27a2bc6b0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5])\n",
            "torch.Size([5])\n",
            "5\n",
            "torch.Size([6693])\n",
            "pgd linf: attack effectiveness 100.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "tensor([[ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False]])\n",
            "success_flag :  tensor([True, True, True, True, True])\n",
            "done :  tensor([[ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False]])\n",
            "loss : torch.Size([5, 3])\n",
            "tensor([[0.0000, 0.6474, 0.6474],\n",
            "        [0.0000, 0.6474, 0.6474],\n",
            "        [0.0000, 0.6474, 0.6474],\n",
            "        [0.0000, 0.6474, 0.6474],\n",
            "        [0.0000, 0.6474, 0.6474]])\n",
            "loss :  tensor([[2.4045, 0.6474, 0.6474],\n",
            "        [2.2561, 0.6474, 0.6474],\n",
            "        [2.4190, 0.6474, 0.6474],\n",
            "        [2.6429, 0.6474, 0.6474],\n",
            "        [2.5728, 0.6474, 0.6474]], dtype=torch.float64)\n",
            "torch.Size([5, 3, 6693])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(success_flag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GFeA5yHtx9T",
        "outputId": "aca837f6-be74-446a-9257-99554e48b7a1"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True, True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scores(model, pertb_x, label):\n",
        "    if hasattr(model, 'is_detector_enabled'):\n",
        "        logits_f, prob_g = model.forward(pertb_x)\n",
        "    else:\n",
        "        logits_f = model.forward(pertb_x)\n",
        "    ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "    y_pred = logits_f.argmax(1)\n",
        "    if hasattr(model, 'is_detector_enabled') and (not oblivion):\n",
        "        tau = model.get_tau_sample_wise(y_pred)\n",
        "        loss_no_reduction = -prob_g\n",
        "        done = (y_pred != label) & (prob_g <= tau)\n",
        "    else:\n",
        "        loss_no_reduction = ce\n",
        "        done = y_pred != label\n",
        "    return loss_no_reduction, done"
      ],
      "metadata": {
        "id": "HsN_g4cAa1vg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    with torch.no_grad():\n",
        "        red_label_ext = torch.cat([red_label] * len(self.attack_list))\n",
        "        loss, done = self.get_scores(model, pertbx, red_label_ext)\n",
        "        loss = loss.reshape(len(self.attack_list), num_sample_red).permute(1, 0)\n",
        "        done = done.reshape(len(self.attack_list), num_sample_red).permute(1, 0)\n",
        "        success_flag = torch.any(done, dim=-1)\n",
        "        # for a sample, if there is at least one successful attack, we will select the one with maximum loss;\n",
        "        # while if no attacks evade the victim successful, all perturbed examples are reminded for selection\n",
        "        done[~torch.any(done, dim=-1)] = 1\n",
        "        loss = (loss * done.to(torch.float)) + torch.min(loss) * (~done).to(torch.float)\n",
        "        pertbx = pertbx.reshape(len(self.attack_list), num_sample_red, *red_n).permute([1, 0, *red_ind])\n",
        "        _, indices = loss.max(dim=-1)\n",
        "        adv_x[~stop_flag] = pertbx[torch.arange(num_sample_red), indices]\n",
        "        a_loss = loss[torch.arange(num_sample_red), indices]\n",
        "        pre_stop_flag = stop_flag.clone()\n",
        "        stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < self.varepsilon) | success_flag\n",
        "        pre_loss[~pre_stop_flag] = a_loss"
      ],
      "metadata": {
        "id": "XL8VfeMPjpJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pgdlinf.__dict__.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tuSIr3rLLJD",
        "outputId": "d62856bc-dcf5-4b61-fb40-cba4e6ac800d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['norm', 'use_random', 'round_threshold', 'is_attacker', 'lambda_', 'omeag', 'manipulation_x', 'api_flag', 'device', 'perturb'])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(pgdlinf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGTSGoAhdqr8",
        "outputId": "33b04a99-e6cf-4034-9b06-79bb8864edb4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.PGD"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(pgdlinf).__name__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NizOQFdNdc3H",
        "outputId": "cd602d65-38f3-497f-d5aa-43b19b14d39a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PGD'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(pgdl1).__name__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rdu47DVMdiK5",
        "outputId": "3d3c79f4-942a-4e3a-87f3-492e6de884e3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PGDl1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "done = torch.tensor(([[1,0,0],[1,1,0],[1,1,1],[0,0,0]]))\n",
        "done.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU_NJiNQgxaA",
        "outputId": "66e90305-8316-4238-e5b6-4390c0924cf5"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.any(done, dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqTJkWjwgxKc",
        "outputId": "4c9d95df-1054-45c7-f974-f58a0cc7da2a"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True,  True,  True, False])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "done[torch.any(done, dim=-1)]=1\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaFyT1Gzhwhf",
        "outputId": "0419c4c9-8d3c-4e89-a998-c4c7e0f37b14"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1],\n",
              "        [1, 1, 1],\n",
              "        [1, 1, 1],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.min(done) * done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0U9zY6PiO3A",
        "outputId": "acd2238f-1772-4baf-b6bb-f7a6b29b0e0e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQYOiHP8LLFj",
        "outputId": "73eed89c-b835-484f-a97b-f4c08fb1d36a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6693])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(2, len(x.size()) + 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2zW5PsRLLAB",
        "outputId": "e6c93e7b-2b8c-4c46-d870-0a6ee9a7d4f4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0DxLH5uEZEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MalwareDetectionDNN(3216,2)\n",
        "#IMPORTANT: model must be double means that its weights must be double for getting float results\n",
        "model = model.double().to(device)\n",
        "#IMPORTANT : we can use pretrain model to check how much pgd attack is effective against normal pretrained model\n",
        "model.load_state_dict(torch.load('/content/malware_detection/MalwareDetectionDNN.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5e2HDkPD-om",
        "outputId": "c3e802d0-99c3-4400-9f5d-f073e9a9001a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MalwareDetectionDNN(\n",
              "  (nn_model_layer_0): Linear(in_features=3216, out_features=200, bias=True)\n",
              "  (nn_model_layer_1): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (nn_model_layer_2): Linear(in_features=200, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS2dGm5kDCRu",
        "outputId": "92f58d6a-f869-4e67-fc52-531e7dcf8205"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6693])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attack.shape)\n",
        "print(torch.sum(x,dim=1))\n",
        "print(torch.sum(attack,dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6jYZnksC4DF",
        "outputId": "de6b5dd0-43d4-4657-c6b3-1392573d947f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6693])\n",
            "tensor([35., 57.])\n",
            "tensor([35., 57.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1PN5wN0ZCmh4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}