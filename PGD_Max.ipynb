{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/V0VxFfdLQtFtt6kqjuoq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/a_malware/blob/main/PGD_Max.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install androguard\n",
        "!pip install captum\n",
        "\n",
        "import androguard\n",
        "import captum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdWcbQsbDh0u",
        "outputId": "c200c4fa-1dd0-41f4-ebcc-735a7e529971"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting androguard\n",
            "  Downloading androguard-3.3.5-py3-none-any.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.4/922.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto>=0.24.0 (from androguard)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from androguard) (8.1.7)\n",
            "Collecting colorama (from androguard)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from androguard) (0.18.3)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from androguard) (7.34.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from androguard) (4.9.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from androguard) (3.7.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from androguard) (3.2)\n",
            "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from androguard) (1.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from androguard) (2.16.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->androguard)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (3.0.39)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (4.8.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot>=1.4.1->androguard) (3.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->androguard) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.0.0->androguard) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->androguard) (0.2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->androguard) (1.16.0)\n",
            "Installing collected packages: asn1crypto, jedi, colorama, androguard\n",
            "Successfully installed androguard-3.3.5 asn1crypto-1.5.1 colorama-0.4.6 jedi-0.19.1\n",
            "Collecting captum\n",
            "  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eRx10YoAiFg",
        "outputId": "3b4e3c53-e2c1-4d7b-fa2b-ec00f68e3ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Cloning into 'malware_detection'...\n",
            "remote: Enumerating objects: 16028, done.\u001b[K\n",
            "remote: Counting objects: 100% (2180/2180), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 16028 (delta 2017), reused 2165 (delta 2003), pack-reused 13848\u001b[K\n",
            "Receiving objects: 100% (16028/16028), 249.84 MiB | 14.57 MiB/s, done.\n",
            "Resolving deltas: 100% (12947/12947), done.\n",
            "Updating files: 100% (141/141), done.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os.path as path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import gdown\n",
        "from functools import partial\n",
        "import random\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "!git clone https://github.com/mostafa-ja/malware_detection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our sharable address : https://drive.google.com/file/d/1-MPYbRTFn_zH3rybfAYihHuIjq2XSiI5/view?usp=sharing\n",
        "download_link = 'https://drive.google.com/uc?id=1Rro-6k4D8y5t7wX8L8dRd4SPCKAKGZXl'\n",
        "output_filepath = '/content/'\n",
        "gdown.download(download_link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "nBAlbafmmMfq",
        "outputId": "341b888e-2cf9-4612-ea56-43586c8fc00c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Rro-6k4D8y5t7wX8L8dRd4SPCKAKGZXl\n",
            "To: /content/archive.zip\n",
            "100%|██████████| 71.3M/71.3M [00:00<00:00, 170MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/archive.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_link = 'https://drive.google.com/uc?id=1--gjFTkAJUW74RXWE4anJ5wssV7K_VAo'\n",
        "output_filepath = '/content/'\n",
        "gdown.download(download_link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "LWrVkcPVjjzg",
        "outputId": "f1a67590-39df-49ef-8e9d-b0f975cbfe69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--gjFTkAJUW74RXWE4anJ5wssV7K_VAo\n",
            "To: /content/train_dataset.pt\n",
            "100%|██████████| 97.4M/97.4M [00:00<00:00, 114MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/train_dataset.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_link = 'https://drive.google.com/uc?id=1-2NnBk3YKauGTGK8DfQHBxrLrYyTxNsp'\n",
        "output_filepath = '/content/'\n",
        "gdown.download(download_link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "P3KZMz42m4c1",
        "outputId": "56e09c75-603c-41a8-872e-6bc4a8a45574"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-2NnBk3YKauGTGK8DfQHBxrLrYyTxNsp\n",
            "To: /content/validation_dataset.pt\n",
            "100%|██████████| 32.5M/32.5M [00:00<00:00, 118MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/validation_dataset.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_link = 'https://drive.google.com/uc?id=1-4PhQGYHpMXrCcxcgMadgPPA4S-9yluX'\n",
        "output_filepath = '/content/'\n",
        "gdown.download(download_link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "9vo-g1RHm46G",
        "outputId": "c99b2942-f1c0-46a9-c286-de7f7fe864f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-4PhQGYHpMXrCcxcgMadgPPA4S-9yluX\n",
            "To: /content/test_dataset.pt\n",
            "100%|██████████| 32.5M/32.5M [00:00<00:00, 157MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/test_dataset.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip"
      ],
      "metadata": {
        "id": "sZsmPn-7pXNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/content/naive_data /content/malware_detection/datasets/\n",
        "!cp /content/train_dataset.pt /content/malware_detection/datasets\n",
        "!cp /content/validation_dataset.pt /content/malware_detection/datasets\n",
        "!cp /content/test_dataset.pt /content/malware_detection/datasets"
      ],
      "metadata": {
        "id": "ml8RC85unN-m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = os.listdir('/content/malware_detection/datasets/naive_data')\n",
        "len(file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buq6_Gzz_JbQ",
        "outputId": "c0072341-78a5-46f2-c7c2-41209aef2c68"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5995"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = [(torch.tensor(data, dtype=torch.float32), label) for data, label in samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.samples[index]"
      ],
      "metadata": {
        "id": "a4E-Kgs2AoJG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MalwareDetectionDNN(nn.Module):\n",
        "    def __init__(self, input_size, n_classes, device='cpu', name='DNN', dense_hidden_units=[200, 200], dropout=0.6, smooth=False):\n",
        "        \"\"\"\n",
        "        Construct malware detector\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        @param input_size: Integer, the dimentionality number of input vector\n",
        "        @param n_classes: Integer, the number of classes, n=2\n",
        "        @param device: String, 'cpu' or 'cuda'\n",
        "        @param name: String, model name\n",
        "        \"\"\"\n",
        "        super(MalwareDetectionDNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.n_classes = n_classes\n",
        "        self.name = name\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.smooth = smooth\n",
        "        self.dense_hidden_units = dense_hidden_units\n",
        "\n",
        "        self.dense_layers = []\n",
        "        if len(self.dense_hidden_units) >= 1:\n",
        "            self.dense_layers.append(nn.Linear(self.input_size, self.dense_hidden_units[0]))\n",
        "        else:\n",
        "            raise ValueError(\"Expect at least one hidden layer.\")\n",
        "        for i in range(len(self.dense_hidden_units[0:-1])):\n",
        "            self.dense_layers.append(nn.Linear(self.dense_hidden_units[i],  # start from idx=1\n",
        "                                               self.dense_hidden_units[i + 1]))\n",
        "        self.dense_layers.append(nn.Linear(self.dense_hidden_units[-1], self.n_classes))\n",
        "\n",
        "        # registration\n",
        "        for idx_i, dense_layer in enumerate(self.dense_layers):\n",
        "            self.add_module('nn_model_layer_{}'.format(idx_i), dense_layer)\n",
        "\n",
        "        if self.smooth:\n",
        "            self.activation_func = F.selu  # partial(F.elu, alpha=self.alpha_)\n",
        "        else:\n",
        "            self.activation_func = F.relu\n",
        "\n",
        "        self.model_save_path = path.join('/content/malware_detection/save/drebin/md_dnn'+ '_' + self.name,'model.pth')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Go through the neural network\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        @param x: 2D tensor, feature representation\n",
        "        \"\"\"\n",
        "        for dense_layer in self.dense_layers[:-1]:\n",
        "            x = self.activation_func(dense_layer(x))\n",
        "\n",
        "        latent_representation = F.dropout(x, self.dropout, training=self.training)\n",
        "        logits = self.dense_layers[-1](latent_representation)\n",
        "        return logits\n",
        "\n",
        "    def customize_loss(self, logits, gt_labels, representation = None, mini_batch_idx=None):\n",
        "        return F.cross_entropy(logits, gt_labels)\n",
        "\n",
        "    # for predict pertubed malicious and create a y_true same size for theam\n",
        "    def inference_batch_wise(self, x):\n",
        "        \"\"\"\n",
        "        support malware samples solely\n",
        "        \"\"\"\n",
        "        assert isinstance(x, torch.Tensor)\n",
        "        logit = self.forward(x)\n",
        "        return torch.softmax(logit, dim=-1).detach().cpu().numpy(), np.ones((logit.size()[0],))\n",
        "\n",
        "    def inference(self, test_data_producer):\n",
        "        confidences = []\n",
        "        gt_labels = []\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_data_producer: # for large dataset we have to consider test dataset with batches\n",
        "                x, y = x.double().to(device), y.long().to(device)\n",
        "                logits = self.forward(x)\n",
        "                confidences.append(F.softmax(logits, dim=-1))\n",
        "                gt_labels.append(y)\n",
        "        confidences = torch.vstack(confidences) #[[1,2,3],[4,5,6]] > below each other\n",
        "        gt_labels = torch.cat(gt_labels, dim=0) #[[1,2,3],[4,5,6]] > [1,2,3,4,5,6]\n",
        "        return confidences, gt_labels\n",
        "\n",
        "\n",
        "    def predict(self, test_data_producer, indicator_masking=True):\n",
        "        \"\"\"\n",
        "        predict labels and conduct evaluation\n",
        "\n",
        "        Parameters\n",
        "        --------\n",
        "        @param test_data_producer, torch.DataLoader\n",
        "        \"\"\"\n",
        "        # evaluation\n",
        "        confidence, y_true = self.inference(test_data_producer)\n",
        "        y_pred = confidence.argmax(1).cpu().numpy()\n",
        "        y_true = y_true.cpu().numpy()\n",
        "        from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        b_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "        print(\"The accuracy on the test dataset is {:.5f}%\".format(accuracy * 100))\n",
        "        print(\"The balanced accuracy on the test dataset is {:.5f}%\".format(b_accuracy * 100))\n",
        "\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        fpr = fp / float(tn + fp)\n",
        "        fnr = fn / float(tp + fn)\n",
        "        f1 = f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "        print(\"Other evaluation metrics we may need:\")\n",
        "        print(\"False Negative Rate (FNR) is {:.5f}%, False Positive Rate (FPR) is {:.5f}%, F1 score is {:.5f}%\".format(fnr * 100, fpr * 100, f1 * 100))\n",
        "\n"
      ],
      "metadata": {
        "id": "IBNqn_OEAuV-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/malware_detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRRx7oWaDnGY",
        "outputId": "e7e10654-d326-439d-a53e-fe49df811fc1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/malware_detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projected gradient descent (ascent)."
      ],
      "metadata": {
        "id": "-45R7q66ygAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from core.droidfeature import InverseDroidFeature\n",
        "\n",
        "inverse_feature = InverseDroidFeature()\n",
        "\n",
        "# manipulation_x : removable features\n",
        "manipulation_x = inverse_feature.get_manipulation()\n",
        "manipulation_x = torch.LongTensor(manipulation_x).to(device)\n",
        "\n",
        "# index of vocab which are interdependent\n",
        "omega = inverse_feature.get_interdependent_apis()\n",
        "omega = torch.sum(F.one_hot(torch.tensor(omega), num_classes=len(inverse_feature.vocab)),dim=0).to(device)\n",
        "\n",
        "# a list of vocab length : if the type of feature is api , index of that feature will be true\n",
        "api_flag = inverse_feature.get_api_flag()"
      ],
      "metadata": {
        "id": "sY7SRwnGYixM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_x0(x, rounding_threshold=0.5, is_sample=False):\n",
        "    \"\"\"\n",
        "    Helper function to randomly initialize the the inner maximizer algos\n",
        "    randomize such that the functionality is preserved.\n",
        "    :param x: training sample\n",
        "    :param is_sample: flag to sample randomly from feasible area or return just x\n",
        "    :return: randomly sampled feasible version of x\n",
        "    \"\"\"\n",
        "    if is_sample:\n",
        "        rand_x = round_x(torch.rand(x.size()), alpha=rounding_threshold)\n",
        "        if x.is_cuda:\n",
        "            rand_x = rand_x.cuda()\n",
        "        return (rand_x.byte() | x.byte()).double()\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "\n",
        "def round_x(x, alpha=0.5):\n",
        "    \"\"\"\n",
        "    rounds x by thresholding it according to alpha which can be a scalar or vector\n",
        "    :param x:\n",
        "    :param alpha: threshold parameter\n",
        "    :return: a double tensor of 0s and 1s.\n",
        "    \"\"\"\n",
        "    return (x >= alpha).double()"
      ],
      "metadata": {
        "id": "Ye0d01eYuWZv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PGD():\n",
        "    \"\"\"\n",
        "    Projected gradient descent (ascent).\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    @param norm, 'l2' or 'linf'\n",
        "    @param use_random, Boolean,  whether use random start point\n",
        "    @param rounding_threshold, float, a threshold for rounding real scalars\n",
        "    @param is_attacker, Boolean, play the role of attacker (note: the defender conducts adversarial training)\n",
        "    @param oblivion, Boolean, whether know the adversary indicator or not\n",
        "    @param kappa, attack confidence\n",
        "    @param manipulation_x, manipulations\n",
        "    @param omega, the indices of interdependent apis corresponding to each api\n",
        "    @param device, 'cpu' or 'cuda'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, norm, use_random=False, rounding_threshold=0.5,\n",
        "                 is_attacker=True, manipulation_x=None, omega=None,api_flag=None, device=None):\n",
        "        super(PGD, self).__init__()\n",
        "        assert norm == 'l1' or norm == 'l2' or norm == 'linf', \"Expect 'l1', 'l2' or 'linf'.\"\n",
        "        self.norm = norm\n",
        "        self.use_random = use_random\n",
        "        self.round_threshold = rounding_threshold\n",
        "        self.is_attacker = is_attacker\n",
        "        self.lambda_ = 1.\n",
        "        self.omeag = omega\n",
        "        self.manipulation_x = manipulation_x\n",
        "        self.api_flag = api_flag\n",
        "        self.device = device\n",
        "    def _perturb(self, model, x, label=None,\n",
        "                 steps=10,\n",
        "                 step_length=1.,\n",
        "                 lambda_=1.,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        perturb node feature vectors\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        @param model, a victim model\n",
        "        @param x: torch.FloatTensor, node feature vectors (each represents the occurrences of apis in a graph) with shape [batch_size, vocab_dim]\n",
        "        @param label: torch.LongTensor, ground truth labels\n",
        "        @param steps: Integer, maximum number of iterations\n",
        "        @param step_length: float, the step length in each iteration\n",
        "        @param lambda_, float, penalty factor\n",
        "        \"\"\"\n",
        "        adv_x = x\n",
        "        self.lambda_ = lambda_\n",
        "        model.eval()\n",
        "        loss_natural = 0.\n",
        "        for t in range(steps):\n",
        "            if t == 0 and self.use_random:\n",
        "                adv_x = get_x0(adv_x, rounding_threshold=self.round_threshold, is_sample=True)\n",
        "            var_adv_x = torch.autograd.Variable(adv_x, requires_grad=True)\n",
        "            loss, done = self.get_loss(model, var_adv_x, label, self.lambda_)\n",
        "            if t == 0:\n",
        "                loss_natural = loss\n",
        "            grad = torch.autograd.grad(torch.mean(loss), var_adv_x)[0].detach().data\n",
        "            perturbation = self.get_perturbation(grad, x, adv_x)\n",
        "            adv_x = torch.clamp(adv_x + perturbation * step_length, min=0., max=1.)\n",
        "        # round\n",
        "        if self.norm == 'linf' and (not hasattr(model, 'is_detector_enabled')):\n",
        "            round_threshold = torch.rand(x.size()).to(self.device)\n",
        "        else:\n",
        "            round_threshold = self.round_threshold\n",
        "        adv_x = round_x(adv_x, round_threshold)\n",
        "        loss_adv, _1 = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "        replace_flag = (loss_adv < loss_natural).unsqueeze(1).expand_as(adv_x)\n",
        "        adv_x[replace_flag] = x[replace_flag]\n",
        "        return adv_x\n",
        "\n",
        "    def perturb(self, model, x, label=None,\n",
        "                steps=10,\n",
        "                step_length=1.,\n",
        "                min_lambda_=1e-5,\n",
        "                max_lambda_=1e5,\n",
        "                base=10.,\n",
        "                verbose=False):\n",
        "        \"\"\"\n",
        "        enhance attack\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            self.lambda_ = min_lambda_\n",
        "        else:\n",
        "            self.lambda_ = max_lambda_\n",
        "\n",
        "        adv_x = x.detach().clone().to(torch.double)\n",
        "        while self.lambda_ <= max_lambda_:\n",
        "            with torch.no_grad():\n",
        "                _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            pert_x = self._perturb(model, adv_x[~done], label[~done],\n",
        "                                   steps,\n",
        "                                   step_length,\n",
        "                                   lambda_=self.lambda_\n",
        "                                   )\n",
        "            adv_x[~done] = pert_x\n",
        "            self.lambda_ *= base\n",
        "        with torch.no_grad():\n",
        "            _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            print(f\"pgd {self.norm}: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "        return adv_x\n",
        "\n",
        "    def get_perturbation(self, gradients, features, adv_features):\n",
        "        # look for allowable position, because only '1--> -' and '0 --> +' are permitted\n",
        "        # api insertion\n",
        "        pos_insertion = (adv_features <= 0.5) * 1 * (adv_features >= 0.)\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "        # grad4insertion = (gradients > 0) * gradients\n",
        "        # api removal\n",
        "        pos_removal = (adv_features > 0.5) * 1\n",
        "        grad4removal = (gradients < 0) * (pos_removal & self.manipulation_x) * gradients\n",
        "        if self.is_attacker:\n",
        "            # cope with the interdependent apis\n",
        "            checking_nonexist_api = (pos_removal ^ self.omega) & self.omega\n",
        "            grad4removal[:, self.api_flag] += torch.sum(gradients * checking_nonexist_api, dim=-1, keepdim=True)\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # norm\n",
        "        if self.norm == 'linf':\n",
        "            perturbation = torch.sign(gradients)\n",
        "        elif self.norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = torch.minimum(\n",
        "                torch.tensor(1., dtype=features.dtype, device=features.device),\n",
        "                gradients / l2norm\n",
        "            )\n",
        "            perturbation = torch.where(torch.isnan(perturbation), 0., perturbation)\n",
        "            perturbation = torch.where(torch.isinf(perturbation), 1., perturbation)\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # add the extra perturbation owing to the interdependent apis\n",
        "        if self.norm == 'linf' and self.is_attacker:\n",
        "            perturbation += torch.any(perturbation[:, self.api_flag] < 0, dim=-1,\n",
        "                                      keepdim=True) * checking_nonexist_api\n",
        "        if self.norm == 'l2' and self.is_attacker:\n",
        "            min_val = torch.amin(perturbation, dim=-1, keepdim=True).clamp_(max=0.)\n",
        "            perturbation += (torch.any(perturbation[:, self.api_flag] < 0, dim=-1,\n",
        "                                       keepdim=True) * torch.abs(min_val) * checking_nonexist_api)\n",
        "        return perturbation\n",
        "\n",
        "    def get_loss(self, model, adv_x, label, lambda_=None):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(adv_x)\n",
        "        else:\n",
        "            logits_f = model.forward(adv_x)\n",
        "\n",
        "        ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            assert lambda_ is not None\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            if self.is_attacker:\n",
        "                loss_no_reduction = ce + lambda_ * (torch.clamp(tau - prob_g,\n",
        "                                                                max=self.kappa)\n",
        "                                                    )\n",
        "            else:\n",
        "                loss_no_reduction = ce + lambda_ * (tau - prob_g)\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            loss_no_reduction = ce\n",
        "            done = y_pred != label\n",
        "        return loss_no_reduction, done"
      ],
      "metadata": {
        "id": "Wd8Q6mNduaya"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PGDl1():\n",
        "    \"\"\"\n",
        "    Projected gradient descent (ascent) with gradients 'normalized' using l1 norm.\n",
        "    By comparing BCA, the api removal is leveraged\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    @param is_attacker, Boolean, play the role of attacker (note: the defender conducts adversarial training)\n",
        "    @param oblivion, Boolean, whether know the adversary indicator or not\n",
        "    @param kappa, attack confidence\n",
        "    @param manipulation_x, manipulations\n",
        "    @param omega, the indices of interdependent apis corresponding to each api\n",
        "    @param device, 'cpu' or 'cuda'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_attacker=True, oblivion=False, kappa=1., manipulation_x=None, omega=None, api_flag=None, device=None):\n",
        "        super(PGDl1, self).__init__()\n",
        "        self.is_attacker = is_attacker\n",
        "        self.lambda_ = 1.\n",
        "        self.omeag = omega\n",
        "        self.manipulation_x = manipulation_x\n",
        "        self.api_flag = api_flag\n",
        "        self.device = device\n",
        "\n",
        "    def _perturb(self, model, x,  label=None,\n",
        "                 steps=10,\n",
        "                 lambda_=1.):\n",
        "        \"\"\"\n",
        "        perturb node feature vectors\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        @param model, a victim model\n",
        "        @param x: torch.FloatTensor, node feature vectors (each represents the occurrences of apis in a graph) with shape [batch_size, number_of_graphs, vocab_dim]\n",
        "        @param label: torch.LongTensor, ground truth labels\n",
        "        @param steps: Integer, maximum number of perturbations\n",
        "        @param lambda_, float, penalty factor\n",
        "        \"\"\"\n",
        "        adv_x = x\n",
        "        worst_x = x.detach().clone()\n",
        "        self.lambda_ = lambda_\n",
        "        self.padding_mask = torch.sum(adv_x, dim=-1, keepdim=True) > 1  # we set a graph contains two apis at least\n",
        "        model.eval()\n",
        "        for t in range(steps):\n",
        "            var_adv_x = torch.autograd.Variable(adv_x, requires_grad=True)\n",
        "            loss, done = self.get_loss(model, var_adv_x, label, self.lambda_)\n",
        "            worst_x[done] = adv_x[done]\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            grad = torch.autograd.grad(torch.mean(loss), var_adv_x)[0]\n",
        "            perturbation, direction = self.get_perturbation(grad, x, adv_x)\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            perturbation[done] = 0.\n",
        "            adv_x = torch.clamp(adv_x + perturbation * direction, min=0., max=1.)\n",
        "        done = self.get_scores(model, adv_x, label)\n",
        "        worst_x[done] = adv_x[done]\n",
        "        return worst_x\n",
        "\n",
        "    def perturb(self, model, x, label=None,\n",
        "                steps=10,\n",
        "                min_lambda_=1e-5,\n",
        "                max_lambda_=1e5,\n",
        "                base=10.,\n",
        "                verbose=True):\n",
        "        \"\"\"\n",
        "        enhance attack\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            self.lambda_ = min_lambda_\n",
        "        else:\n",
        "            self.lambda_ = max_lambda_\n",
        "        adv_x = x.detach().clone().to(torch.double)\n",
        "        while self.lambda_ <= max_lambda_:\n",
        "            _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            pert_x = self._perturb(model, adv_x[~done], label[~done],\n",
        "                                   steps,\n",
        "                                   lambda_=self.lambda_\n",
        "                                   )\n",
        "            adv_x[~done] = pert_x\n",
        "            self.lambda_ *= base\n",
        "            #if not self.check_lambda(model):\n",
        "                #break\n",
        "        with torch.no_grad():\n",
        "            _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            if verbose:\n",
        "                print(f\"pgd l1: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "        return adv_x\n",
        "\n",
        "    def get_perturbation(self, gradients, features, adv_features):\n",
        "        # 1. mask paddings\n",
        "        gradients = gradients * self.padding_mask\n",
        "\n",
        "        # 2. look for allowable position, because only '1--> -' and '0 --> +' are permitted\n",
        "        #    2.1 api insertion\n",
        "        pos_insertion = (adv_features <= 0.5) * 1\n",
        "        grad4insertion = (gradients > 0) * pos_insertion * gradients\n",
        "        #    2.2 api removal\n",
        "        pos_removal = (adv_features > 0.5) * 1\n",
        "        grad4removal = (gradients <= 0) * (pos_removal & self.manipulation_x) * gradients\n",
        "        if self.is_attacker:\n",
        "            #     2.2.1 cope with the interdependent apis\n",
        "            checking_nonexist_api = (pos_removal ^ self.omega) & self.omega\n",
        "            grad4removal[:, self.api_flag] += torch.sum(gradients * checking_nonexist_api, dim=-1, keepdim=True)\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # 3. remove duplications\n",
        "        un_mod = torch.abs(features - adv_features) <= 1e-6\n",
        "        gradients = gradients * un_mod\n",
        "\n",
        "        # 4. look for important position\n",
        "        absolute_grad = torch.abs(gradients).reshape(features.shape[0], -1)\n",
        "        _, position = torch.max(absolute_grad, dim=-1)\n",
        "        perturbations = F.one_hot(position, num_classes=absolute_grad.shape[-1]).double()\n",
        "        perturbations = perturbations.reshape(features.shape)\n",
        "        directions = torch.sign(gradients) * (perturbations > 1e-6)\n",
        "\n",
        "        # 5. tailor the interdependent apis\n",
        "        if self.is_attacker:\n",
        "            perturbations += (torch.any(directions[:, self.api_flag] < 0, dim=-1, keepdim=True)) * checking_nonexist_api\n",
        "            directions += perturbations * self.omega\n",
        "        return perturbations, directions\n",
        "\n",
        "\n",
        "    def get_loss(self, model, adv_x, label, lambda_=None):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(adv_x)\n",
        "        else:\n",
        "            logits_f = model.forward(adv_x)\n",
        "\n",
        "        ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            assert lambda_ is not None\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            if self.is_attacker:\n",
        "                loss_no_reduction = ce + lambda_ * (torch.clamp(tau - prob_g,\n",
        "                                                                max=self.kappa)\n",
        "                                                    )\n",
        "            else:\n",
        "                loss_no_reduction = ce + lambda_ * (tau - prob_g)\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            loss_no_reduction = ce\n",
        "            done = y_pred != label\n",
        "        return loss_no_reduction, done\n",
        "\n",
        "    def get_scores(self, model, pertb_x, label, lmda=1.):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(pertb_x)\n",
        "        else:\n",
        "            logits_f = model.forward(pertb_x)\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            done = y_pred != label\n",
        "        return done"
      ],
      "metadata": {
        "id": "q-FgY4O2hy2g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQzXOfTmAUtp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.load('/content/malware_detection/datasets/train_dataset.pt')\n",
        "train_Loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "validation_dataset = torch.load('/content/malware_detection/datasets/validation_dataset.pt')\n",
        "validation_Loader = data.DataLoader(validation_dataset, batch_size=10000, shuffle=False)\n",
        "test_dataset = torch.load('/content/malware_detection/datasets/test_dataset.pt')\n",
        "test_Loader = data.DataLoader(test_dataset, batch_size=10000, shuffle=False)"
      ],
      "metadata": {
        "id": "MRAX7j_qywXf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/malware_detection/dataset/drebin/data.vocab', 'rb') as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "\n",
        "print('number of vocabs : ',len(loaded_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngwMMct9nohS",
        "outputId": "58b4a672-ade3-450d-ecc8-31797081cd06"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of vocabs :  6693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples,labels = next(iter(test_Loader))\n",
        "x,label = samples[0:5].double(),labels[0:5].long()"
      ],
      "metadata": {
        "id": "Aunmabl3Xb6d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "from functools import partial\n",
        "\n",
        "# A normal function\n",
        "def f(a, b, c, x):\n",
        "    return 1000*a + 100*b + 10*c + x\n",
        "\n",
        "# A partial function that calls f with\n",
        "# a as 3, b as 1 and c as 4.\n",
        "g = partial(f, 3, 1, 4)\n",
        "\n",
        "# Calling g()\n",
        "print(g(5))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XAgKob_UFdxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MalwareDetectionDNN(6693,2)\n",
        "model = model.double().to(device)\n",
        "\n",
        "pgdlinf = PGD(norm='linf', use_random=False,is_attacker=False,manipulation_x=manipulation_x, omega=omega,api_flag=api_flag,device=device)\n",
        "pgdlinf.perturb = partial(pgdlinf.perturb, steps=10, step_length=1.)\n",
        "\n",
        "pgdl2 = PGD(norm='l2', use_random=False, is_attacker=False,manipulation_x=manipulation_x, omega=omega,api_flag=api_flag, device=device)\n",
        "pgdl2.perturb = partial(pgdl2.perturb, steps=10, step_length=1.)\n",
        "\n",
        "pgdl1 = PGDl1(is_attacker=False,manipulation_x=manipulation_x, omega=omega,api_flag=api_flag,device=device)\n",
        "pgdl1.perturb = partial(pgdl1.perturb, steps=10)\n"
      ],
      "metadata": {
        "id": "tHO333FtBO55"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pgdlinf.perturb(model,x,label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIiiATuQC1LD",
        "outputId": "72b1b072-ba9e-4b6e-e553-909f92557efb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd linf: attack effectiveness 100.000%.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.,  ..., 0., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
              "        [0., 0., 1.,  ..., 1., 1., 1.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Max():\n",
        "    \"\"\"\n",
        "    max attack: select results from several attacks iteratively\n",
        "\n",
        "    Parameters\n",
        "    --------\n",
        "    @param attack_list: List, a list of instantiated attack object\n",
        "    @param varepsilon: Float, a scaler for justifying the convergence\n",
        "    \"\"\"\n",
        "    def __init__(self, attack_list, varepsilon=1e-20,is_attacker=True, oblivion=False, kappa=1., manipulation_x=None, omega=None, device=None):\n",
        "        super(Max, self).__init__()\n",
        "        assert len(attack_list) > 0, 'Expect one attack at least.'\n",
        "        self.attack_list = attack_list\n",
        "        self.varepsilon = varepsilon\n",
        "        self.device = device\n",
        "\n",
        "    def perturb(self, model, x, label=None, steps_max=5, min_lambda_=1e-5, max_lambda_=1e5, verbose=False):\n",
        "        \"\"\"\n",
        "        perturb node features\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        @param model, a victim model\n",
        "        @param x: torch.FloatTensor, feature vectors with shape [batch_size, vocab_dim]\n",
        "        @param label: torch.LongTensor, ground truth labels\n",
        "        @param steps_max: Integer, maximum number of iterations\n",
        "        @param min_lambda_: float, balance the importance of adversary detector, if it exists\n",
        "        @param max_lambda_: float, balance the importance of adversary detector, if it exists\n",
        "        @param verbose: Boolean, print verbose log\n",
        "        \"\"\"\n",
        "        if x is None or x.shape[0] <= 0:\n",
        "            return []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            loss, done = self.get_scores(model, x, label) #shape:[samples],[samples]\n",
        "        pre_loss = loss\n",
        "        n, red_n = x.size()[0], x.size()[1:] #samples,features\n",
        "        red_ind = list(range(2, len(x.size()) + 1)) #2\n",
        "        adv_x = x.detach().clone()\n",
        "        stop_flag = torch.zeros(n, dtype=torch.bool, device=self.device) #[samples]\n",
        "        for t in range(steps_max):\n",
        "            num_sample_red = n - torch.sum(stop_flag)\n",
        "            if num_sample_red <= 0:\n",
        "                break\n",
        "\n",
        "            red_label = label[~stop_flag]\n",
        "            pertbx = []\n",
        "            for attack in self.attack_list:\n",
        "                assert 'perturb' in type(attack).__dict__.keys()\n",
        "                if t > 0 and 'use_random' in attack.__dict__.keys():\n",
        "                    attack.use_random = False\n",
        "                if 'Orthogonal' in type(attack).__name__:\n",
        "                    pertbx.append(attack.perturb(model=model, x=adv_x[~stop_flag], label=red_label))\n",
        "                else:\n",
        "                    pertbx.append(attack.perturb(model=model, x=adv_x[~stop_flag], label=red_label,\n",
        "                                                 min_lambda_=1e-5,\n",
        "                                                 max_lambda_=1e5,\n",
        "                                                 ))\n",
        "            # here pertbx.shape = a list of (number of attacks  ,(num_sample_red,features))\n",
        "            pertbx = torch.vstack(pertbx)\n",
        "            # here pertbx.shape = a tensor (num_sample_red*number of attacks samples, features)\n",
        "            with torch.no_grad():\n",
        "                red_label_ext = torch.cat([red_label] * len(self.attack_list)) #(labels*number of attacks )\n",
        "                loss, done = self.get_scores(model, pertbx, red_label_ext) #(labels*number of attacks )\n",
        "                loss = loss.reshape(len(self.attack_list), num_sample_red).permute(1, 0) #(num_sample_red,number of attacks) IMPORTANT: NOT(number of attacks, samples)\n",
        "                done = done.reshape(len(self.attack_list), num_sample_red).permute(1, 0) #(num_sample_red,number of attacks)\n",
        "                success_flag = torch.any(done, dim=-1) #(num_sample_red)\n",
        "                # for a sample, if there is at least one successful attack, we will select the one with maximum loss;\n",
        "                # while if no attacks evade the victim successful, all perturbed examples are reminded for selection\n",
        "                done[~torch.any(done, dim=-1)] = 1 #loss.shape=done.shape=(samples,number of attacks)\n",
        "                loss = (loss * done.to(torch.float)) + torch.min(loss) * (~done).to(torch.float) #(num_sample_red,number of attacks)\n",
        "                pertbx = pertbx.reshape(len(self.attack_list), num_sample_red, *red_n).permute([1, 0, *red_ind])#(num_sample_red,attacks,features)\n",
        "                _, indices = loss.max(dim=-1) # ans:(samples), max loss among attacks which worked, and max loss among all attacks for sample , none of them worked\n",
        "                adv_x[~stop_flag] = pertbx[torch.arange(num_sample_red), indices]\n",
        "                a_loss = loss[torch.arange(num_sample_red), indices]\n",
        "                pre_stop_flag = stop_flag.clone()\n",
        "                stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < self.varepsilon) | success_flag\n",
        "                pre_loss[~pre_stop_flag] = a_loss\n",
        "        if verbose:\n",
        "            with torch.no_grad():\n",
        "                _, done = self.get_scores(model, adv_x, label)\n",
        "                print(f\"max: attack effectiveness {done.sum().item() / x.size()[0] * 100}%.\")\n",
        "        return adv_x\n",
        "\n",
        "    def get_scores(self, model, pertb_x, label):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(pertb_x)\n",
        "        else:\n",
        "            logits_f = model.forward(pertb_x)\n",
        "        ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            loss_no_reduction = -prob_g\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            loss_no_reduction = ce\n",
        "            done = y_pred != label\n",
        "        return loss_no_reduction, done"
      ],
      "metadata": {
        "id": "P00GaglimTRy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack = Max(attack_list=[pgdlinf, pgdl2, pgdl1],varepsilon=1e-9,is_attacker=False,device=device)\n",
        "attack.perturb(model,x,label,steps_max=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqZT2OCgoMI4",
        "outputId": "01c669d4-7ec7-462c-8995-2992af576bfe"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd linf: attack effectiveness 100.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.,  ..., 0., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
              "        [0., 0., 1.,  ..., 1., 1., 1.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PGDAdvTraining(object):\n",
        "    \"\"\"adversarial training incorporating pgd attack\n",
        "\n",
        "    Parameters\n",
        "    -------\n",
        "    @param model, Object,  a model to be protected, e.g., MalwareDetector\n",
        "    @attack_model: Object, adversary's model for generating adversarial malware on the feature space\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, attack=None, attack_param=None):\n",
        "        self.model = model\n",
        "        self.attack = attack\n",
        "        self.attack_param = attack_param\n",
        "\n",
        "        self.name = self.model.name\n",
        "        self.model_save_path =  path.join('/content/malware_detection/save/md_at_pgd' + '_' + 'train','model.pth')\n",
        "\n",
        "    def fit(self, train_data_producer, validation_data_producer=None, adv_epochs=45,beta=0.001,lr=0.005,weight_decay=0, verbose=True):\n",
        "        \"\"\"\n",
        "        Applying adversarial train to enhance the malware detector.\n",
        "\n",
        "        Parameters\n",
        "        -------\n",
        "        @param train_data_producer: Object, an dataloader object for producing a batch of training data\n",
        "        @param validation_data_producer: Object, an dataloader object for producing validation dataset\n",
        "        @param epochs: Integer, epochs for adversarial training\n",
        "        @param adv_epochs: Integer, epochs for adversarial training\n",
        "        @param beta: Float, penalty factor for adversarial loss\n",
        "        @param lr: Float, learning rate of Adam optimizer\n",
        "        @param weight_decay: Float, penalty factor, default value 5e-4\n",
        "        @param verbose: Boolean, whether to show verbose info\n",
        "        \"\"\"\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        total_time = 0.\n",
        "        nbatches = len(train_data_producer)\n",
        "        best_acc_val = 0.\n",
        "        acc_val_adv_be = 0.\n",
        "        best_epoch = 0\n",
        "        for i in range(adv_epochs):\n",
        "            losses, accuracies = [], []\n",
        "            for idx_batch, (x_batch, y_batch) in enumerate(train_data_producer):\n",
        "                x_batch, y_batch = x_batch.double().to(device), y_batch.long().to(device)\n",
        "                batch_size = x_batch.shape[0]\n",
        "                # make data\n",
        "                mal_x_batch, ben_x_batch, mal_y_batch, ben_y_batch = x_batch[y_batch == 1], x_batch[y_batch == 0], y_batch[y_batch == 1], y_batch[y_batch == 0]\n",
        "                start_time = time.time()\n",
        "                self.model.eval()\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "                total_time += time.time() - start_time\n",
        "                x_batch = torch.cat([ben_x_batch, pertb_mal_x], dim=0)\n",
        "                y_batch = torch.cat([ben_y_batch, mal_y_batch])\n",
        "                start_time = time.time()\n",
        "                self.model.train()\n",
        "                optimizer.zero_grad()\n",
        "                logits = self.model.forward(x_batch)\n",
        "                loss_train = self.model.customize_loss(logits,y_batch)\n",
        "\n",
        "                loss_train.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_time += time.time() - start_time\n",
        "                mins, secs = int(total_time / 60), int(total_time % 60)\n",
        "                acc_train = (logits.argmax(1) == y_batch).sum().item()\n",
        "                acc_train /= x_batch.size()[0]\n",
        "                accuracies.append(acc_train)\n",
        "                losses.append(loss_train.item())\n",
        "                if verbose:\n",
        "                    print(f'Mini batch: {i * nbatches + idx_batch + 1}/{adv_epochs * nbatches} | training time in {mins:.0f} minutes, {secs} seconds.')\n",
        "                    print(f'Training loss (batch level): {losses[-1]:.4f} | Train accuracy: {acc_train * 100:.2f}%.')\n",
        "            if verbose:\n",
        "                print(f'Training loss (epoch level): {np.mean(losses):.4f} | Train accuracy: {np.mean(accuracies) * 100:.2f}')\n",
        "\n",
        "            # select model\n",
        "            self.model.eval()\n",
        "            # long-time to train (save the model temporally in case of interruption)\n",
        "            self.save_to_disk(i + 1, optimizer, self.model_save_path + '.tmp')\n",
        "\n",
        "            res_val = []\n",
        "            avg_acc_val = []\n",
        "            for x_val, y_val in validation_data_producer:\n",
        "                x_val, y_val = x_val.double().to(device), y_val.long().to(device)\n",
        "                logits = self.model.forward(x_val)\n",
        "                acc_val = (logits.argmax(1) == y_val).sum().item()\n",
        "                acc_val /= x_val.size()[0]\n",
        "                avg_acc_val.append(acc_val)\n",
        "\n",
        "                mal_x_batch, mal_y_batch = x_val[y_val == 1], y_val[y_val == 1]\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "                y_cent_batch, x_density_batch = self.model.inference_batch_wise(pertb_mal_x)\n",
        "                y_pred = np.argmax(y_cent_batch, axis=-1)\n",
        "                res_val.append(y_pred == 1.)\n",
        "            assert len(res_val) > 0\n",
        "            res_val = np.concatenate(res_val)\n",
        "            acc_val_adv = np.sum(res_val).astype(float) / res_val.shape[0]\n",
        "            acc_val = (np.mean(avg_acc_val) + acc_val_adv) / 2.\n",
        "            # Owing to we look for a new threshold after each epoch, this hinders the convergence of training.\n",
        "            # We save the model's parameters at last several epochs as a well-trained model may be obtained.\n",
        "            if acc_val >= best_acc_val:\n",
        "                best_acc_val = acc_val\n",
        "                acc_val_adv_be = acc_val_adv\n",
        "                best_epoch = i + 1\n",
        "                self.save_to_disk(best_epoch, optimizer, self.model_save_path)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\tVal accuracy {acc_val * 100:.4}% with accuracy {acc_val_adv * 100:.4}% under attack.\")\n",
        "                print(f\"\\tModel select at epoch {best_epoch} with validation accuracy {best_acc_val * 100:.4}% and accuracy {acc_val_adv_be * 100:.4}% under attack.\")\n",
        "                if hasattr(self.model, 'tau'):\n",
        "                    print(f'The threshold is {self.model.tau}.')\n",
        "\n",
        "    #we define here not in model definition, beacause we need attack.pertube()\n",
        "    #also we must not use \"with no grad\" because we need grad for pertubation\n",
        "    def adv_predict(self, test_data_producer):\n",
        "      res_test = []\n",
        "      for x, y in test_data_producer:\n",
        "          x, y = x.double().to(device), y.long().to(device)\n",
        "\n",
        "          mal_x_batch, mal_y_batch = x[y == 1], y[y == 1]\n",
        "          pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "          y_cent_batch, x_density_batch = self.model.inference_batch_wise(pertb_mal_x)\n",
        "          y_pred = np.argmax(y_cent_batch, axis=-1)\n",
        "          res_test.append(y_pred == 1.)\n",
        "      assert len(res_test) > 0\n",
        "      res_test = np.concatenate(res_test)\n",
        "      acc_val_adv = np.sum(res_test).astype(float) / res_test.shape[0]\n",
        "      print(f\"\\tadversarial accuracy  {acc_val_adv * 100:.4}% under attack.\")\n",
        "\n",
        "\n",
        "    def load(self):\n",
        "        assert path.exists(self.model_save_path), 'train model first'\n",
        "        ckpt = torch.load(self.model_save_path)\n",
        "        self.model.load_state_dict(ckpt['model'])\n",
        "\n",
        "    def save_to_disk(self, epoch, optimizer, save_path=None):\n",
        "        if not path.exists(save_path):\n",
        "            os.makedirs(path.dirname(save_path), exist_ok = True)\n",
        "        torch.save({'model': self.model.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'optimizer_state_dict': optimizer.state_dict()\n",
        "                    },\n",
        "                   save_path)"
      ],
      "metadata": {
        "id": "4eqeD6NWvqhF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxAdvTraining(object):\n",
        "    \"\"\"adversarial training incorporating mixture of attacks\n",
        "\n",
        "    Parameters\n",
        "    -------\n",
        "    @param model, Object,  a model to be protected, e.g., MalwareDetector\n",
        "    @attack_model: Object, adversary's model for generating adversarial malware on the feature space\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, attack=None, attack_param=None):\n",
        "        self.model = model\n",
        "        if attack is not None:\n",
        "            #assert isinstance(attack, (Max, StepwiseMax))\n",
        "            if 'is_attacker' in attack.__dict__.keys():\n",
        "                assert not attack.is_attacker\n",
        "        self.attack = attack\n",
        "        self.attack_param = attack_param\n",
        "\n",
        "        self.name = self.model.name\n",
        "        self.model_save_path =  path.join('/content/malware_detection/save/md_at_ma' + '_' + 'train','model.pth')\n",
        "\n",
        "    def fit(self, train_data_producer, validation_data_producer=None, adv_epochs=50,\n",
        "            beta=0.01,\n",
        "            lr=0.001,\n",
        "            under_sampling_ratio=1.,\n",
        "            weight_decay=0, verbose=True):\n",
        "        \"\"\"\n",
        "        Applying adversarial train to enhance the malware detector.\n",
        "\n",
        "        Parameters\n",
        "        -------\n",
        "        @param train_data_producer: Object, an dataloader object for producing a batch of training data\n",
        "        @param validation_data_producer: Object, an dataloader object for producing validation dataset\n",
        "        @param epochs: Integer, epochs for adversarial training\n",
        "        @param adv_epochs: Integer, epochs for adversarial training\n",
        "        @param beta: Float, penalty factor for adversarial loss\n",
        "        @param lr: Float, learning rate of Adam optimizer\n",
        "        @param under_sampling_ratio: [0,1], under-sampling a portion of malware examples for adversarial training\n",
        "        @param weight_decay: Float, penalty factor, default value 5e-4\n",
        "        @param verbose: Boolean, whether to show verbose info\n",
        "        \"\"\"\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        total_time = 0.\n",
        "        nbatches = len(train_data_producer)\n",
        "        print(\"Max adversarial training is starting ...\")\n",
        "        best_acc_val = 0.\n",
        "        acc_val_adv_be = 0.\n",
        "        best_epoch = 0\n",
        "        for i in range(adv_epochs):\n",
        "            random.seed(0)\n",
        "            losses, accuracies = [], []\n",
        "            for idx_batch, (x_batch, y_batch) in enumerate(train_data_producer):\n",
        "                x_batch, y_batch = x_batch.double().to(device), y_batch.long().to(device)\n",
        "                batch_size = x_batch.shape[0]\n",
        "                # make data\n",
        "                mal_x_batch, ben_x_batch, mal_y_batch, ben_y_batch = x_batch[y_batch == 1], x_batch[y_batch == 0], y_batch[y_batch == 1], y_batch[y_batch == 0]\n",
        "                if 0. < under_sampling_ratio < 1.:\n",
        "                    n_mal = mal_x_batch.shape[0]\n",
        "                    n_mal_sampling = int(under_sampling_ratio * n_mal) if int(under_sampling_ratio * n_mal) > 1 else 1\n",
        "                    idx_sampling = random.sample(range(n_mal), n_mal_sampling)\n",
        "                    mal_x_batch, mal_y_batch = mal_x_batch[idx_sampling], mal_y_batch[idx_sampling]\n",
        "\n",
        "                start_time = time.time()\n",
        "                self.model.eval()\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "                pertb_mal_x = round_x(pertb_mal_x, 0.5)\n",
        "                total_time += time.time() - start_time\n",
        "                x_batch = torch.cat([x_batch, pertb_mal_x], dim=0)\n",
        "                y_batch = torch.cat([y_batch, mal_y_batch])\n",
        "                start_time = time.time()\n",
        "                self.model.train()\n",
        "                optimizer.zero_grad()\n",
        "                logits = self.model.forward(x_batch)\n",
        "                loss_train = self.model.customize_loss(logits[:batch_size],y_batch[:batch_size])\n",
        "                loss_train += beta * self.model.customize_loss(logits[batch_size:],y_batch[batch_size:])\n",
        "\n",
        "                loss_train.backward()\n",
        "                optimizer.step()\n",
        "                total_time += time.time() - start_time\n",
        "                mins, secs = int(total_time / 60), int(total_time % 60)\n",
        "                acc_train = (logits.argmax(1) == y_batch).sum().item()\n",
        "                acc_train /= x_batch.size()[0]\n",
        "                accuracies.append(acc_train)\n",
        "                losses.append(loss_train.item())\n",
        "                if verbose:\n",
        "                    print(f'Mini batch: {i * nbatches + idx_batch + 1}/{adv_epochs * nbatches} | training time in {mins:.0f} minutes, {secs} seconds.')\n",
        "                    print(f'Training loss (batch level): {losses[-1]:.4f} | Train accuracy: {acc_train * 100:.2f}%.')\n",
        "            if verbose:\n",
        "                print(f'Training loss (epoch level): {np.mean(losses):.4f} | Train accuracy: {np.mean(accuracies) * 100:.2f}')\n",
        "\n",
        "            # select model\n",
        "            self.model.eval()\n",
        "            # long-time to train (save the model temporally in case of interruption)\n",
        "            self.save_to_disk(i + 1, optimizer, self.model_save_path + '.tmp')\n",
        "            self.attack.is_attacker = True\n",
        "            res_val = []\n",
        "            avg_acc_val = []\n",
        "            for x_val, y_val in validation_data_producer:\n",
        "                x_val, y_val = x_val.double().to(device), y_val.long().to(device)\n",
        "                logits = self.model.forward(x_val)\n",
        "                acc_val = (logits.argmax(1) == y_val).sum().item()\n",
        "                acc_val /= x_val.size()[0]\n",
        "                avg_acc_val.append(acc_val)\n",
        "\n",
        "                mal_x_batch, mal_y_batch = x_val[y_val == 1], y_val[y_val == 1]\n",
        "\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,\n",
        "                                                  **self.attack_param\n",
        "                                                  )\n",
        "                pertb_mal_x = round_x(pertb_mal_x, 0.5)\n",
        "                y_cent_batch, x_density_batch = self.model.inference_batch_wise(pertb_mal_x)\n",
        "                y_pred = np.argmax(y_cent_batch, axis=-1)\n",
        "                res_val.append(y_pred == 1.)\n",
        "            assert len(res_val) > 0\n",
        "            res_val = np.concatenate(res_val)\n",
        "            acc_val_adv = np.sum(res_val).astype(float) / res_val.shape[0]\n",
        "            acc_val = (np.mean(avg_acc_val) + acc_val_adv) / 2.\n",
        "            # Owing to we look for a new threshold after each epoch, this hinders the convergence of training.\n",
        "            # We save the model's parameters at last several epochs as a well-trained model may be obtained.\n",
        "            if acc_val >= best_acc_val:\n",
        "                best_acc_val = acc_val\n",
        "                acc_val_adv_be = acc_val_adv\n",
        "                best_epoch = i + 1\n",
        "                self.save_to_disk(best_epoch, optimizer, self.model_save_path)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\tVal accuracy {acc_val * 100:.4}% with accuracy {acc_val_adv * 100:.4}% under attack.\")\n",
        "                print(f\"\\tModel select at epoch {best_epoch} with validation accuracy {best_acc_val * 100:.4}% and accuracy {acc_val_adv_be * 100:.4}% under attack.\")\n",
        "                if hasattr(self.model, 'tau'):\n",
        "                    print(\n",
        "                        f'The threshold is {self.model.tau}.'\n",
        "                    )\n",
        "            self.attack.is_attacker = False\n",
        "\n",
        "    def load(self):\n",
        "        assert path.exists(self.model_save_path), 'train model first'\n",
        "        ckpt = torch.load(self.model_save_path)\n",
        "        # self.model.tau = ckpt['tau']\n",
        "        # self.model.md_nn_model.load_state_dict(ckpt['md_model'])\n",
        "        # self.model.load_state_dict(ckpt['amd_model'])\n",
        "        self.model.load_state_dict(ckpt['model'])\n",
        "\n",
        "    def save_to_disk(self, epoch, optimizer, save_path=None):\n",
        "        if not path.exists(save_path):\n",
        "            os.makedirs(path.dirname(save_path), exist_ok = True)\n",
        "        # torch.save({'tau': self.model.tau,\n",
        "        #             'md_model': self.model.md_nn_model.state_dict(),\n",
        "        #             'amd_model': self.model.state_dict(),\n",
        "        #             'epoch': epoch,\n",
        "        #             'optimizer_state_dict': optimizer.state_dict()\n",
        "        #             },\n",
        "        #            save_path)\n",
        "        torch.save({'model': self.model.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'optimizer_state_dict': optimizer.state_dict()\n",
        "                    },\n",
        "                   save_path)"
      ],
      "metadata": {
        "id": "ep4KRO56n0nb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack = Max(attack_list=[pgdlinf, pgdl2, pgdl1],varepsilon=1e-9,is_attacker=False,device=device)\n",
        "attack_param = {\n",
        "            'steps_max': 1,  # steps for max attack\n",
        "            'verbose': True\n",
        "              }\n",
        "max_adv_training_model = MaxAdvTraining(model, attack, attack_param)\n",
        "\n",
        "max_adv_training_model.fit(train_Loader,\n",
        "                          validation_Loader,\n",
        "                          adv_epochs=50,\n",
        "                          beta=0.01,\n",
        "                          lr=0.001,\n",
        "                          under_sampling_ratio=1.,\n",
        "                          weight_decay=0.\n",
        "                          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RptuASbsmQly",
        "outputId": "1405d7a0-a9d0-46e6-9eb1-fc80f2a9f3a1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "pgd l1: attack effectiveness 30.159%.\n",
            "max: attack effectiveness 30.158730158730158%.\n",
            "Mini batch: 650/1450 | training time in 14 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0354 | Train accuracy: 89.53%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.769%.\n",
            "max: attack effectiveness 30.76923076923077%.\n",
            "Mini batch: 651/1450 | training time in 15 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0049 | Train accuracy: 90.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 34.848%.\n",
            "max: attack effectiveness 34.84848484848485%.\n",
            "Mini batch: 652/1450 | training time in 15 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0438 | Train accuracy: 91.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.615%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 653/1450 | training time in 15 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0079 | Train accuracy: 92.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 35.211%.\n",
            "max: attack effectiveness 35.2112676056338%.\n",
            "Mini batch: 654/1450 | training time in 15 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0083 | Train accuracy: 89.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.688%.\n",
            "pgd l1: attack effectiveness 26.562%.\n",
            "max: attack effectiveness 26.5625%.\n",
            "Mini batch: 655/1450 | training time in 15 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0056 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.985%.\n",
            "pgd l1: attack effectiveness 31.343%.\n",
            "max: attack effectiveness 31.343283582089555%.\n",
            "Mini batch: 656/1450 | training time in 15 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0076 | Train accuracy: 91.28%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 7.812%.\n",
            "pgd l1: attack effectiveness 40.625%.\n",
            "max: attack effectiveness 40.625%.\n",
            "Mini batch: 657/1450 | training time in 15 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0093 | Train accuracy: 88.54%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 13.514%.\n",
            "pgd l1: attack effectiveness 43.243%.\n",
            "max: attack effectiveness 43.24324324324324%.\n",
            "Mini batch: 658/1450 | training time in 15 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0279 | Train accuracy: 85.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 34.783%.\n",
            "max: attack effectiveness 34.78260869565217%.\n",
            "Mini batch: 659/1450 | training time in 15 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0055 | Train accuracy: 90.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.333%.\n",
            "pgd l1: attack effectiveness 42.667%.\n",
            "max: attack effectiveness 42.66666666666667%.\n",
            "Mini batch: 660/1450 | training time in 15 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0319 | Train accuracy: 85.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 16.176%.\n",
            "pgd l1: attack effectiveness 41.176%.\n",
            "max: attack effectiveness 42.64705882352941%.\n",
            "Mini batch: 661/1450 | training time in 15 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0099 | Train accuracy: 88.27%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 6.780%.\n",
            "pgd l1: attack effectiveness 37.288%.\n",
            "max: attack effectiveness 37.28813559322034%.\n",
            "Mini batch: 662/1450 | training time in 15 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0301 | Train accuracy: 90.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 13.333%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 663/1450 | training time in 15 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0077 | Train accuracy: 85.71%.\n",
            "pgd linf: attack effectiveness 1.449%.\n",
            "pgd l2: attack effectiveness 7.246%.\n",
            "pgd l1: attack effectiveness 37.681%.\n",
            "max: attack effectiveness 37.68115942028986%.\n",
            "Mini batch: 664/1450 | training time in 15 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0150 | Train accuracy: 87.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 9.375%.\n",
            "pgd l1: attack effectiveness 28.125%.\n",
            "max: attack effectiveness 28.125%.\n",
            "Mini batch: 665/1450 | training time in 15 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0259 | Train accuracy: 92.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 25.806%.\n",
            "pgd l1: attack effectiveness 33.871%.\n",
            "max: attack effectiveness 35.483870967741936%.\n",
            "Mini batch: 666/1450 | training time in 15 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0148 | Train accuracy: 90.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 16.667%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 667/1450 | training time in 15 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0054 | Train accuracy: 89.47%.\n",
            "Training loss (epoch level): 0.0144 | Train accuracy: 89.98\n",
            "pgd linf: attack effectiveness 2.151%.\n",
            "pgd l2: attack effectiveness 10.573%.\n",
            "pgd l1: attack effectiveness 42.652%.\n",
            "max: attack effectiveness 42.65232974910394%.\n",
            "\tVal accuracy 77.01% with accuracy 57.35% under attack.\n",
            "\tModel select at epoch 22 with validation accuracy 77.05% and accuracy 57.35% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 9.091%.\n",
            "pgd l1: attack effectiveness 37.879%.\n",
            "max: attack effectiveness 37.878787878787875%.\n",
            "Mini batch: 668/1450 | training time in 15 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0160 | Train accuracy: 88.14%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 11.111%.\n",
            "pgd l1: attack effectiveness 39.683%.\n",
            "max: attack effectiveness 42.857142857142854%.\n",
            "Mini batch: 669/1450 | training time in 15 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0124 | Train accuracy: 91.10%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 10.606%.\n",
            "pgd l1: attack effectiveness 31.818%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 670/1450 | training time in 15 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0168 | Train accuracy: 89.18%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.714%.\n",
            "pgd l1: attack effectiveness 28.571%.\n",
            "max: attack effectiveness 28.57142857142857%.\n",
            "Mini batch: 671/1450 | training time in 15 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0127 | Train accuracy: 91.41%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 8.451%.\n",
            "pgd l1: attack effectiveness 39.437%.\n",
            "max: attack effectiveness 39.436619718309856%.\n",
            "Mini batch: 672/1450 | training time in 15 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0196 | Train accuracy: 88.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 21.053%.\n",
            "pgd l1: attack effectiveness 35.526%.\n",
            "max: attack effectiveness 35.526315789473685%.\n",
            "Mini batch: 673/1450 | training time in 15 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0148 | Train accuracy: 89.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 26.230%.\n",
            "pgd l1: attack effectiveness 29.508%.\n",
            "max: attack effectiveness 29.508196721311474%.\n",
            "Mini batch: 674/1450 | training time in 15 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0166 | Train accuracy: 91.53%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 26.087%.\n",
            "pgd l1: attack effectiveness 31.884%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 675/1450 | training time in 15 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0324 | Train accuracy: 88.32%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "pgd l2: attack effectiveness 27.419%.\n",
            "pgd l1: attack effectiveness 29.032%.\n",
            "max: attack effectiveness 29.03225806451613%.\n",
            "Mini batch: 676/1450 | training time in 15 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0321 | Train accuracy: 90.00%.\n",
            "pgd linf: attack effectiveness 1.667%.\n",
            "pgd l2: attack effectiveness 36.667%.\n",
            "pgd l1: attack effectiveness 38.333%.\n",
            "max: attack effectiveness 40.0%.\n",
            "Mini batch: 677/1450 | training time in 15 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0391 | Train accuracy: 86.70%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 14.815%.\n",
            "pgd l1: attack effectiveness 27.778%.\n",
            "max: attack effectiveness 27.77777777777778%.\n",
            "Mini batch: 678/1450 | training time in 15 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0132 | Train accuracy: 92.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 9.524%.\n",
            "pgd l1: attack effectiveness 34.921%.\n",
            "max: attack effectiveness 34.92063492063492%.\n",
            "Mini batch: 679/1450 | training time in 15 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0163 | Train accuracy: 89.53%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 9.231%.\n",
            "pgd l1: attack effectiveness 33.846%.\n",
            "max: attack effectiveness 33.84615384615385%.\n",
            "Mini batch: 680/1450 | training time in 15 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0089 | Train accuracy: 89.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 31.818%.\n",
            "max: attack effectiveness 31.818181818181817%.\n",
            "Mini batch: 681/1450 | training time in 15 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0311 | Train accuracy: 92.78%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 32.308%.\n",
            "max: attack effectiveness 32.30769230769231%.\n",
            "Mini batch: 682/1450 | training time in 15 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0160 | Train accuracy: 90.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 35.211%.\n",
            "max: attack effectiveness 35.2112676056338%.\n",
            "Mini batch: 683/1450 | training time in 15 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0064 | Train accuracy: 88.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 26.562%.\n",
            "max: attack effectiveness 26.5625%.\n",
            "Mini batch: 684/1450 | training time in 15 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0144 | Train accuracy: 93.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 31.343%.\n",
            "max: attack effectiveness 31.343283582089555%.\n",
            "Mini batch: 685/1450 | training time in 15 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0088 | Train accuracy: 90.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 12.500%.\n",
            "pgd l1: attack effectiveness 42.188%.\n",
            "max: attack effectiveness 42.1875%.\n",
            "Mini batch: 686/1450 | training time in 15 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0149 | Train accuracy: 85.94%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 37.838%.\n",
            "pgd l1: attack effectiveness 43.243%.\n",
            "max: attack effectiveness 44.5945945945946%.\n",
            "Mini batch: 687/1450 | training time in 15 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0357 | Train accuracy: 84.16%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 11.594%.\n",
            "pgd l1: attack effectiveness 34.783%.\n",
            "max: attack effectiveness 34.78260869565217%.\n",
            "Mini batch: 688/1450 | training time in 15 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0103 | Train accuracy: 89.34%.\n",
            "pgd linf: attack effectiveness 1.333%.\n",
            "pgd l2: attack effectiveness 18.667%.\n",
            "pgd l1: attack effectiveness 36.000%.\n",
            "max: attack effectiveness 36.0%.\n",
            "Mini batch: 689/1450 | training time in 15 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0165 | Train accuracy: 87.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 36.765%.\n",
            "pgd l1: attack effectiveness 41.176%.\n",
            "max: attack effectiveness 41.17647058823529%.\n",
            "Mini batch: 690/1450 | training time in 15 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0145 | Train accuracy: 86.73%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 18.644%.\n",
            "pgd l1: attack effectiveness 35.593%.\n",
            "max: attack effectiveness 35.59322033898305%.\n",
            "Mini batch: 691/1450 | training time in 15 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0221 | Train accuracy: 89.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.333%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 692/1450 | training time in 15 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 86.70%.\n",
            "pgd linf: attack effectiveness 1.449%.\n",
            "pgd l2: attack effectiveness 15.942%.\n",
            "pgd l1: attack effectiveness 40.580%.\n",
            "max: attack effectiveness 43.47826086956522%.\n",
            "Mini batch: 693/1450 | training time in 15 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0212 | Train accuracy: 85.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 9.375%.\n",
            "pgd l1: attack effectiveness 26.562%.\n",
            "max: attack effectiveness 26.5625%.\n",
            "Mini batch: 694/1450 | training time in 16 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0337 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 9.677%.\n",
            "pgd l1: attack effectiveness 32.258%.\n",
            "max: attack effectiveness 32.25806451612903%.\n",
            "Mini batch: 695/1450 | training time in 16 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0112 | Train accuracy: 90.53%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 696/1450 | training time in 16 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0178 | Train accuracy: 89.45\n",
            "pgd linf: attack effectiveness 2.151%.\n",
            "pgd l2: attack effectiveness 6.989%.\n",
            "pgd l1: attack effectiveness 45.341%.\n",
            "max: attack effectiveness 45.340501792114694%.\n",
            "\tVal accuracy 75.87% with accuracy 54.66% under attack.\n",
            "\tModel select at epoch 22 with validation accuracy 77.05% and accuracy 57.35% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 6.061%.\n",
            "pgd l1: attack effectiveness 39.394%.\n",
            "max: attack effectiveness 39.39393939393939%.\n",
            "Mini batch: 697/1450 | training time in 16 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0149 | Train accuracy: 87.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.175%.\n",
            "pgd l1: attack effectiveness 42.857%.\n",
            "max: attack effectiveness 42.857142857142854%.\n",
            "Mini batch: 698/1450 | training time in 16 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0118 | Train accuracy: 89.01%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.545%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 699/1450 | training time in 16 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0104 | Train accuracy: 89.69%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.857%.\n",
            "pgd l1: attack effectiveness 34.286%.\n",
            "max: attack effectiveness 34.285714285714285%.\n",
            "Mini batch: 700/1450 | training time in 16 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0076 | Train accuracy: 91.41%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.817%.\n",
            "pgd l1: attack effectiveness 39.437%.\n",
            "max: attack effectiveness 39.436619718309856%.\n",
            "Mini batch: 701/1450 | training time in 16 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0091 | Train accuracy: 87.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.947%.\n",
            "pgd l1: attack effectiveness 35.526%.\n",
            "max: attack effectiveness 35.526315789473685%.\n",
            "Mini batch: 702/1450 | training time in 16 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0081 | Train accuracy: 89.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.639%.\n",
            "pgd l1: attack effectiveness 29.508%.\n",
            "max: attack effectiveness 29.508196721311474%.\n",
            "Mini batch: 703/1450 | training time in 16 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0101 | Train accuracy: 92.59%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 7.246%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 704/1450 | training time in 16 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0154 | Train accuracy: 90.36%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "pgd l2: attack effectiveness 3.226%.\n",
            "pgd l1: attack effectiveness 29.032%.\n",
            "max: attack effectiveness 29.03225806451613%.\n",
            "Mini batch: 705/1450 | training time in 16 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0200 | Train accuracy: 91.05%.\n",
            "pgd linf: attack effectiveness 1.667%.\n",
            "pgd l2: attack effectiveness 6.667%.\n",
            "pgd l1: attack effectiveness 38.333%.\n",
            "max: attack effectiveness 38.333333333333336%.\n",
            "Mini batch: 706/1450 | training time in 16 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0239 | Train accuracy: 88.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.926%.\n",
            "max: attack effectiveness 25.925925925925924%.\n",
            "Mini batch: 707/1450 | training time in 16 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0108 | Train accuracy: 92.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.587%.\n",
            "pgd l1: attack effectiveness 30.159%.\n",
            "max: attack effectiveness 30.158730158730158%.\n",
            "Mini batch: 708/1450 | training time in 16 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0142 | Train accuracy: 92.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 7.692%.\n",
            "pgd l1: attack effectiveness 30.769%.\n",
            "max: attack effectiveness 30.76923076923077%.\n",
            "Mini batch: 709/1450 | training time in 16 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0074 | Train accuracy: 92.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 22.727%.\n",
            "max: attack effectiveness 22.727272727272727%.\n",
            "Mini batch: 710/1450 | training time in 16 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0327 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 13.846%.\n",
            "pgd l1: attack effectiveness 30.769%.\n",
            "max: attack effectiveness 30.76923076923077%.\n",
            "Mini batch: 711/1450 | training time in 16 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0150 | Train accuracy: 92.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 35.211%.\n",
            "max: attack effectiveness 35.2112676056338%.\n",
            "Mini batch: 712/1450 | training time in 16 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0082 | Train accuracy: 90.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.125%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 26.5625%.\n",
            "Mini batch: 713/1450 | training time in 16 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0073 | Train accuracy: 94.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 34.328%.\n",
            "max: attack effectiveness 34.32835820895522%.\n",
            "Mini batch: 714/1450 | training time in 16 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0083 | Train accuracy: 90.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 10.938%.\n",
            "pgd l1: attack effectiveness 40.625%.\n",
            "max: attack effectiveness 40.625%.\n",
            "Mini batch: 715/1450 | training time in 16 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0102 | Train accuracy: 89.58%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 12.162%.\n",
            "pgd l1: attack effectiveness 41.892%.\n",
            "max: attack effectiveness 43.24324324324324%.\n",
            "Mini batch: 716/1450 | training time in 16 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0225 | Train accuracy: 88.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.348%.\n",
            "pgd l1: attack effectiveness 36.232%.\n",
            "max: attack effectiveness 36.231884057971016%.\n",
            "Mini batch: 717/1450 | training time in 16 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0090 | Train accuracy: 91.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 6.667%.\n",
            "pgd l1: attack effectiveness 45.333%.\n",
            "max: attack effectiveness 45.33333333333333%.\n",
            "Mini batch: 718/1450 | training time in 16 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0110 | Train accuracy: 85.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 7.353%.\n",
            "pgd l1: attack effectiveness 42.647%.\n",
            "max: attack effectiveness 42.64705882352941%.\n",
            "Mini batch: 719/1450 | training time in 16 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0072 | Train accuracy: 87.24%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 5.085%.\n",
            "pgd l1: attack effectiveness 37.288%.\n",
            "max: attack effectiveness 37.28813559322034%.\n",
            "Mini batch: 720/1450 | training time in 16 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0242 | Train accuracy: 93.05%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.333%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 721/1450 | training time in 16 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0075 | Train accuracy: 86.21%.\n",
            "pgd linf: attack effectiveness 1.449%.\n",
            "pgd l2: attack effectiveness 4.348%.\n",
            "pgd l1: attack effectiveness 40.580%.\n",
            "max: attack effectiveness 40.57971014492754%.\n",
            "Mini batch: 722/1450 | training time in 16 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0199 | Train accuracy: 88.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.688%.\n",
            "pgd l1: attack effectiveness 31.250%.\n",
            "max: attack effectiveness 31.25%.\n",
            "Mini batch: 723/1450 | training time in 16 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0239 | Train accuracy: 89.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.839%.\n",
            "pgd l1: attack effectiveness 33.871%.\n",
            "max: attack effectiveness 33.87096774193548%.\n",
            "Mini batch: 724/1450 | training time in 16 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0078 | Train accuracy: 91.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 725/1450 | training time in 16 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0131 | Train accuracy: 90.45\n",
            "pgd linf: attack effectiveness 1.971%.\n",
            "pgd l2: attack effectiveness 4.839%.\n",
            "pgd l1: attack effectiveness 42.294%.\n",
            "max: attack effectiveness 42.29390681003584%.\n",
            "\tVal accuracy 77.02% with accuracy 57.71% under attack.\n",
            "\tModel select at epoch 22 with validation accuracy 77.05% and accuracy 57.35% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 36.364%.\n",
            "max: attack effectiveness 36.36363636363637%.\n",
            "Mini batch: 726/1450 | training time in 16 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0119 | Train accuracy: 90.72%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 38.095%.\n",
            "max: attack effectiveness 38.095238095238095%.\n",
            "Mini batch: 727/1450 | training time in 16 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0086 | Train accuracy: 93.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 31.818%.\n",
            "max: attack effectiveness 31.818181818181817%.\n",
            "Mini batch: 728/1450 | training time in 16 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0088 | Train accuracy: 91.24%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 28.571%.\n",
            "max: attack effectiveness 28.57142857142857%.\n",
            "Mini batch: 729/1450 | training time in 16 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0076 | Train accuracy: 91.41%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 39.437%.\n",
            "max: attack effectiveness 39.436619718309856%.\n",
            "Mini batch: 730/1450 | training time in 16 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0168 | Train accuracy: 89.45%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.316%.\n",
            "pgd l1: attack effectiveness 34.211%.\n",
            "max: attack effectiveness 34.21052631578947%.\n",
            "Mini batch: 731/1450 | training time in 16 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 90.20%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.639%.\n",
            "pgd l1: attack effectiveness 24.590%.\n",
            "max: attack effectiveness 24.59016393442623%.\n",
            "Mini batch: 732/1450 | training time in 16 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0085 | Train accuracy: 93.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 733/1450 | training time in 16 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0154 | Train accuracy: 90.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.226%.\n",
            "pgd l1: attack effectiveness 29.032%.\n",
            "max: attack effectiveness 29.03225806451613%.\n",
            "Mini batch: 734/1450 | training time in 16 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0102 | Train accuracy: 91.58%.\n",
            "pgd linf: attack effectiveness 1.667%.\n",
            "pgd l2: attack effectiveness 5.000%.\n",
            "pgd l1: attack effectiveness 38.333%.\n",
            "max: attack effectiveness 38.333333333333336%.\n",
            "Mini batch: 735/1450 | training time in 16 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0153 | Train accuracy: 88.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.926%.\n",
            "max: attack effectiveness 25.925925925925924%.\n",
            "Mini batch: 736/1450 | training time in 16 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 93.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.587%.\n",
            "pgd l1: attack effectiveness 31.746%.\n",
            "max: attack effectiveness 31.746031746031743%.\n",
            "Mini batch: 737/1450 | training time in 16 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0231 | Train accuracy: 92.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 738/1450 | training time in 17 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0058 | Train accuracy: 92.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 21.212%.\n",
            "max: attack effectiveness 21.21212121212121%.\n",
            "Mini batch: 739/1450 | training time in 17 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0352 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 740/1450 | training time in 17 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0093 | Train accuracy: 92.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.803%.\n",
            "max: attack effectiveness 33.80281690140845%.\n",
            "Mini batch: 741/1450 | training time in 17 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0078 | Train accuracy: 88.94%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 742/1450 | training time in 17 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0058 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 31.343%.\n",
            "max: attack effectiveness 31.343283582089555%.\n",
            "Mini batch: 743/1450 | training time in 17 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0088 | Train accuracy: 91.28%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 39.062%.\n",
            "max: attack effectiveness 39.0625%.\n",
            "Mini batch: 744/1450 | training time in 17 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0087 | Train accuracy: 91.15%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 6.757%.\n",
            "pgd l1: attack effectiveness 40.541%.\n",
            "max: attack effectiveness 40.54054054054054%.\n",
            "Mini batch: 745/1450 | training time in 17 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0211 | Train accuracy: 86.14%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.884%.\n",
            "max: attack effectiveness 31.88405797101449%.\n",
            "Mini batch: 746/1450 | training time in 17 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0070 | Train accuracy: 90.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 38.667%.\n",
            "max: attack effectiveness 38.666666666666664%.\n",
            "Mini batch: 747/1450 | training time in 17 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0122 | Train accuracy: 88.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 39.706%.\n",
            "max: attack effectiveness 39.705882352941174%.\n",
            "Mini batch: 748/1450 | training time in 17 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 88.78%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 35.593%.\n",
            "max: attack effectiveness 35.59322033898305%.\n",
            "Mini batch: 749/1450 | training time in 17 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0236 | Train accuracy: 91.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 750/1450 | training time in 17 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 89.16%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 36.232%.\n",
            "max: attack effectiveness 36.231884057971016%.\n",
            "Mini batch: 751/1450 | training time in 17 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0087 | Train accuracy: 91.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.125%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 752/1450 | training time in 17 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0187 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 33.871%.\n",
            "max: attack effectiveness 33.87096774193548%.\n",
            "Mini batch: 753/1450 | training time in 17 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0048 | Train accuracy: 91.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 754/1450 | training time in 17 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0013 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0113 | Train accuracy: 91.44\n",
            "pgd linf: attack effectiveness 1.971%.\n",
            "pgd l2: attack effectiveness 2.509%.\n",
            "pgd l1: attack effectiveness 41.756%.\n",
            "max: attack effectiveness 41.75627240143369%.\n",
            "\tVal accuracy 77.33% with accuracy 58.24% under attack.\n",
            "\tModel select at epoch 26 with validation accuracy 77.33% and accuracy 58.24% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 36.364%.\n",
            "max: attack effectiveness 36.36363636363637%.\n",
            "Mini batch: 755/1450 | training time in 17 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0085 | Train accuracy: 90.72%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 19.048%.\n",
            "max: attack effectiveness 19.047619047619047%.\n",
            "Mini batch: 756/1450 | training time in 17 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0089 | Train accuracy: 94.24%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 757/1450 | training time in 17 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0067 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 17.143%.\n",
            "max: attack effectiveness 17.142857142857142%.\n",
            "Mini batch: 758/1450 | training time in 17 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0065 | Train accuracy: 96.97%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 12.676%.\n",
            "max: attack effectiveness 12.676056338028168%.\n",
            "Mini batch: 759/1450 | training time in 17 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0136 | Train accuracy: 96.48%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 18.421%.\n",
            "max: attack effectiveness 18.421052631578945%.\n",
            "Mini batch: 760/1450 | training time in 17 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 94.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.639%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 761/1450 | training time in 17 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 96.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 20.290%.\n",
            "max: attack effectiveness 20.28985507246377%.\n",
            "Mini batch: 762/1450 | training time in 17 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0088 | Train accuracy: 93.40%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.226%.\n",
            "pgd l1: attack effectiveness 30.645%.\n",
            "max: attack effectiveness 30.64516129032258%.\n",
            "Mini batch: 763/1450 | training time in 17 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0076 | Train accuracy: 94.21%.\n",
            "pgd linf: attack effectiveness 1.667%.\n",
            "pgd l2: attack effectiveness 5.000%.\n",
            "pgd l1: attack effectiveness 38.333%.\n",
            "max: attack effectiveness 38.333333333333336%.\n",
            "Mini batch: 764/1450 | training time in 17 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0152 | Train accuracy: 90.43%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 20.370%.\n",
            "max: attack effectiveness 20.37037037037037%.\n",
            "Mini batch: 765/1450 | training time in 17 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0064 | Train accuracy: 95.05%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.587%.\n",
            "pgd l1: attack effectiveness 20.635%.\n",
            "max: attack effectiveness 20.634920634920633%.\n",
            "Mini batch: 766/1450 | training time in 17 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0122 | Train accuracy: 95.29%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 20.000%.\n",
            "max: attack effectiveness 20.0%.\n",
            "Mini batch: 767/1450 | training time in 17 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 94.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 24.242%.\n",
            "max: attack effectiveness 24.242424242424242%.\n",
            "Mini batch: 768/1450 | training time in 17 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0352 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 30.769%.\n",
            "max: attack effectiveness 30.76923076923077%.\n",
            "Mini batch: 769/1450 | training time in 17 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0107 | Train accuracy: 93.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.986%.\n",
            "max: attack effectiveness 30.985915492957744%.\n",
            "Mini batch: 770/1450 | training time in 17 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 90.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 771/1450 | training time in 17 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0056 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 29.851%.\n",
            "max: attack effectiveness 29.850746268656714%.\n",
            "Mini batch: 772/1450 | training time in 17 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0056 | Train accuracy: 91.28%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 35.938%.\n",
            "max: attack effectiveness 35.9375%.\n",
            "Mini batch: 773/1450 | training time in 17 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0085 | Train accuracy: 91.67%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 9.459%.\n",
            "pgd l1: attack effectiveness 41.892%.\n",
            "max: attack effectiveness 41.891891891891895%.\n",
            "Mini batch: 774/1450 | training time in 17 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0166 | Train accuracy: 85.64%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 31.884%.\n",
            "max: attack effectiveness 31.88405797101449%.\n",
            "Mini batch: 775/1450 | training time in 17 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0084 | Train accuracy: 89.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.333%.\n",
            "pgd l1: attack effectiveness 36.000%.\n",
            "max: attack effectiveness 36.0%.\n",
            "Mini batch: 776/1450 | training time in 18 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0090 | Train accuracy: 88.18%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 39.706%.\n",
            "max: attack effectiveness 39.705882352941174%.\n",
            "Mini batch: 777/1450 | training time in 18 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 91.84%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 33.898%.\n",
            "max: attack effectiveness 33.89830508474576%.\n",
            "Mini batch: 778/1450 | training time in 18 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0183 | Train accuracy: 92.51%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 779/1450 | training time in 18 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0052 | Train accuracy: 90.64%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 36.232%.\n",
            "max: attack effectiveness 36.231884057971016%.\n",
            "Mini batch: 780/1450 | training time in 18 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0121 | Train accuracy: 89.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.125%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 781/1450 | training time in 18 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0253 | Train accuracy: 92.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.645%.\n",
            "max: attack effectiveness 30.64516129032258%.\n",
            "Mini batch: 782/1450 | training time in 18 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0049 | Train accuracy: 94.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 783/1450 | training time in 18 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0015 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0098 | Train accuracy: 92.87\n",
            "pgd linf: attack effectiveness 2.151%.\n",
            "pgd l2: attack effectiveness 4.659%.\n",
            "pgd l1: attack effectiveness 42.115%.\n",
            "max: attack effectiveness 42.29390681003584%.\n",
            "\tVal accuracy 77.39% with accuracy 57.71% under attack.\n",
            "\tModel select at epoch 27 with validation accuracy 77.39% and accuracy 57.71% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 34.848%.\n",
            "max: attack effectiveness 34.84848484848485%.\n",
            "Mini batch: 784/1450 | training time in 18 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0137 | Train accuracy: 90.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 38.095%.\n",
            "max: attack effectiveness 38.095238095238095%.\n",
            "Mini batch: 785/1450 | training time in 18 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0106 | Train accuracy: 89.01%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 30.303%.\n",
            "max: attack effectiveness 30.303030303030305%.\n",
            "Mini batch: 786/1450 | training time in 18 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0054 | Train accuracy: 93.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 28.571%.\n",
            "max: attack effectiveness 28.57142857142857%.\n",
            "Mini batch: 787/1450 | training time in 18 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0048 | Train accuracy: 92.93%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 38.028%.\n",
            "max: attack effectiveness 38.028169014084504%.\n",
            "Mini batch: 788/1450 | training time in 18 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0105 | Train accuracy: 92.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.895%.\n",
            "max: attack effectiveness 32.89473684210527%.\n",
            "Mini batch: 789/1450 | training time in 18 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0062 | Train accuracy: 90.69%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.590%.\n",
            "max: attack effectiveness 24.59016393442623%.\n",
            "Mini batch: 790/1450 | training time in 18 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 95.77%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 791/1450 | training time in 18 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0092 | Train accuracy: 92.39%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 14.516%.\n",
            "max: attack effectiveness 14.516129032258066%.\n",
            "Mini batch: 792/1450 | training time in 18 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 95.79%.\n",
            "pgd linf: attack effectiveness 1.667%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 18.333%.\n",
            "max: attack effectiveness 18.333333333333332%.\n",
            "Mini batch: 793/1450 | training time in 18 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0130 | Train accuracy: 95.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 794/1450 | training time in 18 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 96.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.587%.\n",
            "pgd l1: attack effectiveness 15.873%.\n",
            "max: attack effectiveness 15.873015873015872%.\n",
            "Mini batch: 795/1450 | training time in 18 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0068 | Train accuracy: 95.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 16.923%.\n",
            "max: attack effectiveness 16.923076923076923%.\n",
            "Mini batch: 796/1450 | training time in 18 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 96.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 797/1450 | training time in 18 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0347 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 798/1450 | training time in 18 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0074 | Train accuracy: 93.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.394%.\n",
            "max: attack effectiveness 32.3943661971831%.\n",
            "Mini batch: 799/1450 | training time in 18 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0093 | Train accuracy: 91.46%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 26.562%.\n",
            "max: attack effectiveness 26.5625%.\n",
            "Mini batch: 800/1450 | training time in 18 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0062 | Train accuracy: 94.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 31.343%.\n",
            "max: attack effectiveness 31.343283582089555%.\n",
            "Mini batch: 801/1450 | training time in 18 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 93.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 40.625%.\n",
            "max: attack effectiveness 40.625%.\n",
            "Mini batch: 802/1450 | training time in 18 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0073 | Train accuracy: 89.58%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 8.108%.\n",
            "pgd l1: attack effectiveness 43.243%.\n",
            "max: attack effectiveness 43.24324324324324%.\n",
            "Mini batch: 803/1450 | training time in 18 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0190 | Train accuracy: 89.11%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 804/1450 | training time in 18 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0064 | Train accuracy: 89.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 805/1450 | training time in 18 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0064 | Train accuracy: 88.18%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 41.176%.\n",
            "max: attack effectiveness 41.17647058823529%.\n",
            "Mini batch: 806/1450 | training time in 18 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0055 | Train accuracy: 88.78%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 5.085%.\n",
            "pgd l1: attack effectiveness 33.898%.\n",
            "max: attack effectiveness 33.89830508474576%.\n",
            "Mini batch: 807/1450 | training time in 18 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0245 | Train accuracy: 92.51%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 808/1450 | training time in 18 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 89.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 36.232%.\n",
            "max: attack effectiveness 36.231884057971016%.\n",
            "Mini batch: 809/1450 | training time in 18 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0072 | Train accuracy: 91.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 6.250%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 26.5625%.\n",
            "Mini batch: 810/1450 | training time in 18 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0181 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 35.484%.\n",
            "max: attack effectiveness 35.483870967741936%.\n",
            "Mini batch: 811/1450 | training time in 18 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 94.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 16.667%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 812/1450 | training time in 18 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0099 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0095 | Train accuracy: 92.57\n",
            "pgd linf: attack effectiveness 2.509%.\n",
            "pgd l2: attack effectiveness 4.480%.\n",
            "pgd l1: attack effectiveness 41.577%.\n",
            "max: attack effectiveness 41.57706093189964%.\n",
            "\tVal accuracy 77.63% with accuracy 58.42% under attack.\n",
            "\tModel select at epoch 28 with validation accuracy 77.63% and accuracy 58.42% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 36.364%.\n",
            "max: attack effectiveness 36.36363636363637%.\n",
            "Mini batch: 813/1450 | training time in 18 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 91.24%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 22.222%.\n",
            "max: attack effectiveness 22.22222222222222%.\n",
            "Mini batch: 814/1450 | training time in 18 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 96.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 815/1450 | training time in 18 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 93.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 17.143%.\n",
            "max: attack effectiveness 17.142857142857142%.\n",
            "Mini batch: 816/1450 | training time in 18 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0057 | Train accuracy: 94.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 12.676%.\n",
            "max: attack effectiveness 12.676056338028168%.\n",
            "Mini batch: 817/1450 | training time in 18 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0120 | Train accuracy: 96.48%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.316%.\n",
            "pgd l1: attack effectiveness 18.421%.\n",
            "max: attack effectiveness 18.421052631578945%.\n",
            "Mini batch: 818/1450 | training time in 18 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0040 | Train accuracy: 93.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 819/1450 | training time in 18 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0082 | Train accuracy: 97.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 18.841%.\n",
            "max: attack effectiveness 18.84057971014493%.\n",
            "Mini batch: 820/1450 | training time in 19 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0104 | Train accuracy: 94.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.226%.\n",
            "pgd l1: attack effectiveness 27.419%.\n",
            "max: attack effectiveness 27.419354838709676%.\n",
            "Mini batch: 821/1450 | training time in 19 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0093 | Train accuracy: 93.68%.\n",
            "pgd linf: attack effectiveness 1.667%.\n",
            "pgd l2: attack effectiveness 6.667%.\n",
            "pgd l1: attack effectiveness 38.333%.\n",
            "max: attack effectiveness 38.333333333333336%.\n",
            "Mini batch: 822/1450 | training time in 19 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0207 | Train accuracy: 88.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.926%.\n",
            "max: attack effectiveness 25.925925925925924%.\n",
            "Mini batch: 823/1450 | training time in 19 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 94.51%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.587%.\n",
            "pgd l1: attack effectiveness 28.571%.\n",
            "max: attack effectiveness 28.57142857142857%.\n",
            "Mini batch: 824/1450 | training time in 19 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0075 | Train accuracy: 91.62%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.769%.\n",
            "max: attack effectiveness 30.76923076923077%.\n",
            "Mini batch: 825/1450 | training time in 19 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 94.82%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 30.303%.\n",
            "max: attack effectiveness 30.303030303030305%.\n",
            "Mini batch: 826/1450 | training time in 19 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0330 | Train accuracy: 92.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 30.769%.\n",
            "max: attack effectiveness 30.76923076923077%.\n",
            "Mini batch: 827/1450 | training time in 19 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0077 | Train accuracy: 93.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 40.845%.\n",
            "max: attack effectiveness 40.845070422535215%.\n",
            "Mini batch: 828/1450 | training time in 19 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 89.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.125%.\n",
            "max: attack effectiveness 28.125%.\n",
            "Mini batch: 829/1450 | training time in 19 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 93.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 32.836%.\n",
            "max: attack effectiveness 32.83582089552239%.\n",
            "Mini batch: 830/1450 | training time in 19 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0068 | Train accuracy: 91.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 40.625%.\n",
            "max: attack effectiveness 40.625%.\n",
            "Mini batch: 831/1450 | training time in 19 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0117 | Train accuracy: 90.10%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 6.757%.\n",
            "pgd l1: attack effectiveness 40.541%.\n",
            "max: attack effectiveness 40.54054054054054%.\n",
            "Mini batch: 832/1450 | training time in 19 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0169 | Train accuracy: 87.62%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.435%.\n",
            "max: attack effectiveness 30.434782608695656%.\n",
            "Mini batch: 833/1450 | training time in 19 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0058 | Train accuracy: 89.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.333%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 834/1450 | training time in 19 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0088 | Train accuracy: 91.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 38.235%.\n",
            "max: attack effectiveness 38.23529411764706%.\n",
            "Mini batch: 835/1450 | training time in 19 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 90.31%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 3.390%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 836/1450 | training time in 19 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0183 | Train accuracy: 92.51%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 38.667%.\n",
            "max: attack effectiveness 38.666666666666664%.\n",
            "Mini batch: 837/1450 | training time in 19 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 89.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 36.232%.\n",
            "max: attack effectiveness 36.231884057971016%.\n",
            "Mini batch: 838/1450 | training time in 19 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0086 | Train accuracy: 90.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 18.750%.\n",
            "max: attack effectiveness 18.75%.\n",
            "Mini batch: 839/1450 | training time in 19 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0310 | Train accuracy: 94.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 16.129%.\n",
            "max: attack effectiveness 16.129032258064516%.\n",
            "Mini batch: 840/1450 | training time in 19 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 95.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 841/1450 | training time in 19 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0020 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0095 | Train accuracy: 92.94\n",
            "pgd linf: attack effectiveness 2.151%.\n",
            "pgd l2: attack effectiveness 3.226%.\n",
            "pgd l1: attack effectiveness 41.935%.\n",
            "max: attack effectiveness 41.935483870967744%.\n",
            "\tVal accuracy 77.36% with accuracy 58.06% under attack.\n",
            "\tModel select at epoch 28 with validation accuracy 77.63% and accuracy 58.42% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 36.364%.\n",
            "max: attack effectiveness 36.36363636363637%.\n",
            "Mini batch: 842/1450 | training time in 19 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0098 | Train accuracy: 89.18%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 36.508%.\n",
            "max: attack effectiveness 36.507936507936506%.\n",
            "Mini batch: 843/1450 | training time in 19 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0087 | Train accuracy: 93.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 844/1450 | training time in 19 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0069 | Train accuracy: 89.69%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 28.571%.\n",
            "max: attack effectiveness 28.57142857142857%.\n",
            "Mini batch: 845/1450 | training time in 19 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0048 | Train accuracy: 92.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 40.845%.\n",
            "max: attack effectiveness 40.845070422535215%.\n",
            "Mini batch: 846/1450 | training time in 19 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0103 | Train accuracy: 88.94%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.632%.\n",
            "pgd l1: attack effectiveness 34.211%.\n",
            "max: attack effectiveness 34.21052631578947%.\n",
            "Mini batch: 847/1450 | training time in 19 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0056 | Train accuracy: 89.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.590%.\n",
            "max: attack effectiveness 24.59016393442623%.\n",
            "Mini batch: 848/1450 | training time in 19 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0083 | Train accuracy: 95.24%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 27.536%.\n",
            "max: attack effectiveness 27.536231884057973%.\n",
            "Mini batch: 849/1450 | training time in 19 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0082 | Train accuracy: 93.40%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.419%.\n",
            "max: attack effectiveness 27.419354838709676%.\n",
            "Mini batch: 850/1450 | training time in 19 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 94.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 36.667%.\n",
            "max: attack effectiveness 36.666666666666664%.\n",
            "Mini batch: 851/1450 | training time in 19 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0158 | Train accuracy: 91.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 852/1450 | training time in 19 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0055 | Train accuracy: 96.70%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.873%.\n",
            "max: attack effectiveness 15.873015873015872%.\n",
            "Mini batch: 853/1450 | training time in 19 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0074 | Train accuracy: 97.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 854/1450 | training time in 19 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 96.89%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 21.212%.\n",
            "max: attack effectiveness 21.21212121212121%.\n",
            "Mini batch: 855/1450 | training time in 19 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0266 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 856/1450 | training time in 19 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0079 | Train accuracy: 92.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.803%.\n",
            "max: attack effectiveness 33.80281690140845%.\n",
            "Mini batch: 857/1450 | training time in 19 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 91.46%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 858/1450 | training time in 19 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 31.343%.\n",
            "max: attack effectiveness 31.343283582089555%.\n",
            "Mini batch: 859/1450 | training time in 19 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 90.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.125%.\n",
            "pgd l1: attack effectiveness 37.500%.\n",
            "max: attack effectiveness 37.5%.\n",
            "Mini batch: 860/1450 | training time in 19 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0083 | Train accuracy: 90.10%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 10.811%.\n",
            "pgd l1: attack effectiveness 40.541%.\n",
            "max: attack effectiveness 40.54054054054054%.\n",
            "Mini batch: 861/1450 | training time in 19 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0184 | Train accuracy: 88.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 862/1450 | training time in 19 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0052 | Train accuracy: 89.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 40.000%.\n",
            "max: attack effectiveness 40.0%.\n",
            "Mini batch: 863/1450 | training time in 20 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0076 | Train accuracy: 88.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 8.824%.\n",
            "pgd l1: attack effectiveness 41.176%.\n",
            "max: attack effectiveness 41.17647058823529%.\n",
            "Mini batch: 864/1450 | training time in 20 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 89.80%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 3.390%.\n",
            "pgd l1: attack effectiveness 37.288%.\n",
            "max: attack effectiveness 37.28813559322034%.\n",
            "Mini batch: 865/1450 | training time in 20 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0168 | Train accuracy: 91.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 866/1450 | training time in 20 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 87.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.797%.\n",
            "pgd l1: attack effectiveness 37.681%.\n",
            "max: attack effectiveness 37.68115942028986%.\n",
            "Mini batch: 867/1450 | training time in 20 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0093 | Train accuracy: 90.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.125%.\n",
            "pgd l1: attack effectiveness 28.125%.\n",
            "max: attack effectiveness 28.125%.\n",
            "Mini batch: 868/1450 | training time in 20 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0190 | Train accuracy: 93.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 33.871%.\n",
            "max: attack effectiveness 33.87096774193548%.\n",
            "Mini batch: 869/1450 | training time in 20 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 93.16%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 870/1450 | training time in 20 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0003 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0085 | Train accuracy: 92.26\n",
            "pgd linf: attack effectiveness 2.151%.\n",
            "pgd l2: attack effectiveness 3.405%.\n",
            "pgd l1: attack effectiveness 24.552%.\n",
            "max: attack effectiveness 24.551971326164875%.\n",
            "\tVal accuracy 86.26% with accuracy 75.45% under attack.\n",
            "\tModel select at epoch 30 with validation accuracy 86.26% and accuracy 75.45% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.242%.\n",
            "max: attack effectiveness 24.242424242424242%.\n",
            "Mini batch: 871/1450 | training time in 20 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 92.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 19.048%.\n",
            "max: attack effectiveness 19.047619047619047%.\n",
            "Mini batch: 872/1450 | training time in 20 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 95.29%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 873/1450 | training time in 20 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 95.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 17.143%.\n",
            "max: attack effectiveness 17.142857142857142%.\n",
            "Mini batch: 874/1450 | training time in 20 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0048 | Train accuracy: 95.45%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 12.676%.\n",
            "max: attack effectiveness 12.676056338028168%.\n",
            "Mini batch: 875/1450 | training time in 20 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0307 | Train accuracy: 95.48%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 18.421%.\n",
            "max: attack effectiveness 18.421052631578945%.\n",
            "Mini batch: 876/1450 | training time in 20 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0056 | Train accuracy: 93.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 877/1450 | training time in 20 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0063 | Train accuracy: 97.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 26.087%.\n",
            "max: attack effectiveness 26.08695652173913%.\n",
            "Mini batch: 878/1450 | training time in 20 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0113 | Train accuracy: 93.91%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "pgd l2: attack effectiveness 3.226%.\n",
            "pgd l1: attack effectiveness 25.806%.\n",
            "max: attack effectiveness 25.806451612903224%.\n",
            "Mini batch: 879/1450 | training time in 20 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0063 | Train accuracy: 92.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.000%.\n",
            "pgd l1: attack effectiveness 38.333%.\n",
            "max: attack effectiveness 38.333333333333336%.\n",
            "Mini batch: 880/1450 | training time in 20 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0133 | Train accuracy: 92.02%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.778%.\n",
            "max: attack effectiveness 27.77777777777778%.\n",
            "Mini batch: 881/1450 | training time in 20 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0067 | Train accuracy: 93.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.587%.\n",
            "pgd l1: attack effectiveness 26.984%.\n",
            "max: attack effectiveness 26.984126984126984%.\n",
            "Mini batch: 882/1450 | training time in 20 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0065 | Train accuracy: 94.76%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 883/1450 | training time in 20 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 92.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 30.303%.\n",
            "max: attack effectiveness 30.303030303030305%.\n",
            "Mini batch: 884/1450 | training time in 20 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0175 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 885/1450 | training time in 20 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0080 | Train accuracy: 91.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.986%.\n",
            "max: attack effectiveness 30.985915492957744%.\n",
            "Mini batch: 886/1450 | training time in 20 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0083 | Train accuracy: 91.46%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 887/1450 | training time in 20 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.985%.\n",
            "pgd l1: attack effectiveness 29.851%.\n",
            "max: attack effectiveness 29.850746268656714%.\n",
            "Mini batch: 888/1450 | training time in 20 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0058 | Train accuracy: 92.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 34.375%.\n",
            "max: attack effectiveness 34.375%.\n",
            "Mini batch: 889/1450 | training time in 20 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0088 | Train accuracy: 91.15%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 5.405%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 890/1450 | training time in 20 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0254 | Train accuracy: 87.62%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.435%.\n",
            "max: attack effectiveness 30.434782608695656%.\n",
            "Mini batch: 891/1450 | training time in 20 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 92.39%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 34.667%.\n",
            "max: attack effectiveness 34.66666666666667%.\n",
            "Mini batch: 892/1450 | training time in 20 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0089 | Train accuracy: 90.64%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 36.765%.\n",
            "max: attack effectiveness 36.76470588235294%.\n",
            "Mini batch: 893/1450 | training time in 20 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 91.84%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 3.390%.\n",
            "pgd l1: attack effectiveness 35.593%.\n",
            "max: attack effectiveness 35.59322033898305%.\n",
            "Mini batch: 894/1450 | training time in 20 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0263 | Train accuracy: 91.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 895/1450 | training time in 20 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 90.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 36.232%.\n",
            "max: attack effectiveness 36.231884057971016%.\n",
            "Mini batch: 896/1450 | training time in 20 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0077 | Train accuracy: 92.89%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 26.562%.\n",
            "max: attack effectiveness 26.5625%.\n",
            "Mini batch: 897/1450 | training time in 20 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0213 | Train accuracy: 92.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 35.484%.\n",
            "max: attack effectiveness 35.483870967741936%.\n",
            "Mini batch: 898/1450 | training time in 20 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0049 | Train accuracy: 92.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 899/1450 | training time in 20 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0004 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0093 | Train accuracy: 93.15\n",
            "pgd linf: attack effectiveness 2.330%.\n",
            "pgd l2: attack effectiveness 3.047%.\n",
            "pgd l1: attack effectiveness 24.910%.\n",
            "max: attack effectiveness 24.910394265232974%.\n",
            "\tVal accuracy 85.96% with accuracy 75.09% under attack.\n",
            "\tModel select at epoch 30 with validation accuracy 86.26% and accuracy 75.45% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 22.727%.\n",
            "max: attack effectiveness 22.727272727272727%.\n",
            "Mini batch: 900/1450 | training time in 20 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0137 | Train accuracy: 92.78%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.873%.\n",
            "max: attack effectiveness 15.873015873015872%.\n",
            "Mini batch: 901/1450 | training time in 21 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0087 | Train accuracy: 94.76%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 902/1450 | training time in 21 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 15.714%.\n",
            "max: attack effectiveness 15.714285714285714%.\n",
            "Mini batch: 903/1450 | training time in 21 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0075 | Train accuracy: 96.97%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 7.042%.\n",
            "max: attack effectiveness 7.042253521126761%.\n",
            "Mini batch: 904/1450 | training time in 21 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0316 | Train accuracy: 96.48%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.316%.\n",
            "pgd l1: attack effectiveness 18.421%.\n",
            "max: attack effectiveness 18.421052631578945%.\n",
            "Mini batch: 905/1450 | training time in 21 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 94.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 906/1450 | training time in 21 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0064 | Train accuracy: 96.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 17.391%.\n",
            "max: attack effectiveness 17.391304347826086%.\n",
            "Mini batch: 907/1450 | training time in 21 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0075 | Train accuracy: 96.45%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 14.516%.\n",
            "max: attack effectiveness 14.516129032258066%.\n",
            "Mini batch: 908/1450 | training time in 21 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0108 | Train accuracy: 97.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 6.667%.\n",
            "pgd l1: attack effectiveness 18.333%.\n",
            "max: attack effectiveness 18.333333333333332%.\n",
            "Mini batch: 909/1450 | training time in 21 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0174 | Train accuracy: 94.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 910/1450 | training time in 21 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0040 | Train accuracy: 96.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.571%.\n",
            "max: attack effectiveness 28.57142857142857%.\n",
            "Mini batch: 911/1450 | training time in 21 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 93.72%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 912/1450 | training time in 21 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 93.78%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 30.303%.\n",
            "max: attack effectiveness 30.303030303030305%.\n",
            "Mini batch: 913/1450 | training time in 21 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0235 | Train accuracy: 91.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 914/1450 | training time in 21 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0087 | Train accuracy: 93.78%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.986%.\n",
            "max: attack effectiveness 30.985915492957744%.\n",
            "Mini batch: 915/1450 | training time in 21 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 90.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 916/1450 | training time in 21 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0080 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.851%.\n",
            "max: attack effectiveness 29.850746268656714%.\n",
            "Mini batch: 917/1450 | training time in 21 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 93.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 34.375%.\n",
            "max: attack effectiveness 34.375%.\n",
            "Mini batch: 918/1450 | training time in 21 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0093 | Train accuracy: 91.67%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 1.351%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 919/1450 | training time in 21 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0142 | Train accuracy: 89.11%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 920/1450 | training time in 21 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 93.40%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 32.000%.\n",
            "max: attack effectiveness 32.0%.\n",
            "Mini batch: 921/1450 | training time in 21 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 91.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 36.765%.\n",
            "max: attack effectiveness 36.76470588235294%.\n",
            "Mini batch: 922/1450 | training time in 21 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 94.90%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 3.390%.\n",
            "pgd l1: attack effectiveness 33.898%.\n",
            "max: attack effectiveness 33.89830508474576%.\n",
            "Mini batch: 923/1450 | training time in 21 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0178 | Train accuracy: 90.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 40.000%.\n",
            "max: attack effectiveness 40.0%.\n",
            "Mini batch: 924/1450 | training time in 21 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 87.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 36.232%.\n",
            "max: attack effectiveness 36.231884057971016%.\n",
            "Mini batch: 925/1450 | training time in 21 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0092 | Train accuracy: 88.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 926/1450 | training time in 21 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0149 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.032%.\n",
            "max: attack effectiveness 29.03225806451613%.\n",
            "Mini batch: 927/1450 | training time in 21 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 93.16%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 928/1450 | training time in 21 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0017 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0090 | Train accuracy: 93.49\n",
            "pgd linf: attack effectiveness 2.509%.\n",
            "pgd l2: attack effectiveness 2.867%.\n",
            "pgd l1: attack effectiveness 41.219%.\n",
            "max: attack effectiveness 41.21863799283154%.\n",
            "\tVal accuracy 77.81% with accuracy 58.78% under attack.\n",
            "\tModel select at epoch 30 with validation accuracy 86.26% and accuracy 75.45% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 34.848%.\n",
            "max: attack effectiveness 34.84848484848485%.\n",
            "Mini batch: 929/1450 | training time in 21 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 91.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 930/1450 | training time in 21 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0082 | Train accuracy: 92.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 18.181818181818183%.\n",
            "Mini batch: 931/1450 | training time in 21 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 12.857%.\n",
            "max: attack effectiveness 12.857142857142856%.\n",
            "Mini batch: 932/1450 | training time in 21 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 95.45%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 9.859%.\n",
            "max: attack effectiveness 9.859154929577464%.\n",
            "Mini batch: 933/1450 | training time in 21 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0117 | Train accuracy: 96.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.789%.\n",
            "max: attack effectiveness 15.789473684210526%.\n",
            "Mini batch: 934/1450 | training time in 21 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 95.59%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 935/1450 | training time in 21 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0073 | Train accuracy: 96.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.493%.\n",
            "max: attack effectiveness 14.492753623188406%.\n",
            "Mini batch: 936/1450 | training time in 21 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0071 | Train accuracy: 96.45%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.290%.\n",
            "max: attack effectiveness 11.29032258064516%.\n",
            "Mini batch: 937/1450 | training time in 21 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 97.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.667%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 938/1450 | training time in 21 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0125 | Train accuracy: 94.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 939/1450 | training time in 21 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0063 | Train accuracy: 96.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 12.698%.\n",
            "max: attack effectiveness 12.698412698412698%.\n",
            "Mini batch: 940/1450 | training time in 21 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0055 | Train accuracy: 95.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 941/1450 | training time in 22 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0025 | Train accuracy: 95.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 942/1450 | training time in 22 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0348 | Train accuracy: 93.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 943/1450 | training time in 22 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 92.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.986%.\n",
            "max: attack effectiveness 30.985915492957744%.\n",
            "Mini batch: 944/1450 | training time in 22 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0106 | Train accuracy: 91.46%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 945/1450 | training time in 22 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 94.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.343%.\n",
            "max: attack effectiveness 31.343283582089555%.\n",
            "Mini batch: 946/1450 | training time in 22 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 92.82%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 37.500%.\n",
            "max: attack effectiveness 37.5%.\n",
            "Mini batch: 947/1450 | training time in 22 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0056 | Train accuracy: 88.54%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 2.703%.\n",
            "pgd l1: attack effectiveness 40.541%.\n",
            "max: attack effectiveness 40.54054054054054%.\n",
            "Mini batch: 948/1450 | training time in 22 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0131 | Train accuracy: 88.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.884%.\n",
            "max: attack effectiveness 31.88405797101449%.\n",
            "Mini batch: 949/1450 | training time in 22 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 91.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.667%.\n",
            "pgd l1: attack effectiveness 36.000%.\n",
            "max: attack effectiveness 36.0%.\n",
            "Mini batch: 950/1450 | training time in 22 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 88.18%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 41.176%.\n",
            "max: attack effectiveness 41.17647058823529%.\n",
            "Mini batch: 951/1450 | training time in 22 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 92.86%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 5.085%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 952/1450 | training time in 22 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0250 | Train accuracy: 92.51%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.667%.\n",
            "pgd l1: attack effectiveness 40.000%.\n",
            "max: attack effectiveness 40.0%.\n",
            "Mini batch: 953/1450 | training time in 22 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 91.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 36.232%.\n",
            "max: attack effectiveness 36.231884057971016%.\n",
            "Mini batch: 954/1450 | training time in 22 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0072 | Train accuracy: 90.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.062%.\n",
            "max: attack effectiveness 14.0625%.\n",
            "Mini batch: 955/1450 | training time in 22 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0160 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 17.742%.\n",
            "max: attack effectiveness 17.741935483870968%.\n",
            "Mini batch: 956/1450 | training time in 22 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 96.32%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 957/1450 | training time in 22 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0025 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0080 | Train accuracy: 93.67\n",
            "pgd linf: attack effectiveness 2.688%.\n",
            "pgd l2: attack effectiveness 3.226%.\n",
            "pgd l1: attack effectiveness 23.835%.\n",
            "max: attack effectiveness 23.835125448028673%.\n",
            "\tVal accuracy 86.5% with accuracy 76.16% under attack.\n",
            "\tModel select at epoch 33 with validation accuracy 86.5% and accuracy 76.16% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 22.727%.\n",
            "max: attack effectiveness 22.727272727272727%.\n",
            "Mini batch: 958/1450 | training time in 22 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0054 | Train accuracy: 93.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.873%.\n",
            "max: attack effectiveness 15.873015873015872%.\n",
            "Mini batch: 959/1450 | training time in 22 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 97.38%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 960/1450 | training time in 22 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 17.143%.\n",
            "max: attack effectiveness 17.142857142857142%.\n",
            "Mini batch: 961/1450 | training time in 22 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 95.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.859%.\n",
            "max: attack effectiveness 9.859154929577464%.\n",
            "Mini batch: 962/1450 | training time in 22 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0167 | Train accuracy: 96.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.316%.\n",
            "pgd l1: attack effectiveness 17.105%.\n",
            "max: attack effectiveness 17.105263157894736%.\n",
            "Mini batch: 963/1450 | training time in 22 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 94.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 964/1450 | training time in 22 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0066 | Train accuracy: 96.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 14.493%.\n",
            "max: attack effectiveness 14.492753623188406%.\n",
            "Mini batch: 965/1450 | training time in 22 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0112 | Train accuracy: 95.43%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 12.903%.\n",
            "max: attack effectiveness 12.903225806451612%.\n",
            "Mini batch: 966/1450 | training time in 22 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 95.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.667%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 967/1450 | training time in 22 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0106 | Train accuracy: 95.74%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.926%.\n",
            "max: attack effectiveness 25.925925925925924%.\n",
            "Mini batch: 968/1450 | training time in 22 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 93.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.873%.\n",
            "max: attack effectiveness 15.873015873015872%.\n",
            "Mini batch: 969/1450 | training time in 22 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 96.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 970/1450 | training time in 22 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 96.89%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 30.303%.\n",
            "max: attack effectiveness 30.303030303030305%.\n",
            "Mini batch: 971/1450 | training time in 22 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0278 | Train accuracy: 91.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.077%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 972/1450 | training time in 22 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0057 | Train accuracy: 92.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.394%.\n",
            "max: attack effectiveness 32.3943661971831%.\n",
            "Mini batch: 973/1450 | training time in 22 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 92.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 974/1450 | training time in 22 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0040 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 975/1450 | training time in 22 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 92.82%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 34.375%.\n",
            "max: attack effectiveness 34.375%.\n",
            "Mini batch: 976/1450 | training time in 22 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0065 | Train accuracy: 94.79%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 4.054%.\n",
            "pgd l1: attack effectiveness 40.541%.\n",
            "max: attack effectiveness 40.54054054054054%.\n",
            "Mini batch: 977/1450 | training time in 22 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0164 | Train accuracy: 87.13%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.884%.\n",
            "max: attack effectiveness 31.88405797101449%.\n",
            "Mini batch: 978/1450 | training time in 22 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 93.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 36.000%.\n",
            "max: attack effectiveness 36.0%.\n",
            "Mini batch: 979/1450 | training time in 22 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0069 | Train accuracy: 92.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 35.294%.\n",
            "max: attack effectiveness 35.294117647058826%.\n",
            "Mini batch: 980/1450 | training time in 22 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 91.33%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 981/1450 | training time in 22 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0277 | Train accuracy: 93.05%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 982/1450 | training time in 22 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 89.16%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 983/1450 | training time in 23 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0078 | Train accuracy: 90.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 26.562%.\n",
            "max: attack effectiveness 26.5625%.\n",
            "Mini batch: 984/1450 | training time in 23 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0196 | Train accuracy: 91.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.419%.\n",
            "max: attack effectiveness 27.419354838709676%.\n",
            "Mini batch: 985/1450 | training time in 23 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 94.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 986/1450 | training time in 23 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0003 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0079 | Train accuracy: 93.98\n",
            "pgd linf: attack effectiveness 1.792%.\n",
            "pgd l2: attack effectiveness 2.151%.\n",
            "pgd l1: attack effectiveness 21.685%.\n",
            "max: attack effectiveness 21.68458781362007%.\n",
            "\tVal accuracy 87.61% with accuracy 78.32% under attack.\n",
            "\tModel select at epoch 34 with validation accuracy 87.61% and accuracy 78.32% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 21.212%.\n",
            "max: attack effectiveness 21.21212121212121%.\n",
            "Mini batch: 987/1450 | training time in 23 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0067 | Train accuracy: 93.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.286%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 988/1450 | training time in 23 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 97.38%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 18.181818181818183%.\n",
            "Mini batch: 989/1450 | training time in 23 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0048 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 12.857%.\n",
            "max: attack effectiveness 12.857142857142856%.\n",
            "Mini batch: 990/1450 | training time in 23 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 97.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 11.268%.\n",
            "max: attack effectiveness 11.267605633802818%.\n",
            "Mini batch: 991/1450 | training time in 23 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0126 | Train accuracy: 96.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.632%.\n",
            "pgd l1: attack effectiveness 15.789%.\n",
            "max: attack effectiveness 15.789473684210526%.\n",
            "Mini batch: 992/1450 | training time in 23 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 94.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 993/1450 | training time in 23 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 97.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 13.043%.\n",
            "max: attack effectiveness 13.043478260869565%.\n",
            "Mini batch: 994/1450 | training time in 23 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0066 | Train accuracy: 96.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.290%.\n",
            "max: attack effectiveness 11.29032258064516%.\n",
            "Mini batch: 995/1450 | training time in 23 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 96.84%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 996/1450 | training time in 23 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0103 | Train accuracy: 96.28%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.111%.\n",
            "max: attack effectiveness 11.11111111111111%.\n",
            "Mini batch: 997/1450 | training time in 23 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 97.25%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 12.698%.\n",
            "max: attack effectiveness 12.698412698412698%.\n",
            "Mini batch: 998/1450 | training time in 23 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0073 | Train accuracy: 95.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 999/1450 | training time in 23 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 96.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1000/1450 | training time in 23 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0251 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.077%.\n",
            "pgd l1: attack effectiveness 18.462%.\n",
            "max: attack effectiveness 18.461538461538463%.\n",
            "Mini batch: 1001/1450 | training time in 23 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0058 | Train accuracy: 95.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1002/1450 | training time in 23 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0065 | Train accuracy: 90.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 23.438%.\n",
            "max: attack effectiveness 23.4375%.\n",
            "Mini batch: 1003/1450 | training time in 23 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 94.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 29.851%.\n",
            "max: attack effectiveness 29.850746268656714%.\n",
            "Mini batch: 1004/1450 | training time in 23 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 93.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 34.375%.\n",
            "max: attack effectiveness 34.375%.\n",
            "Mini batch: 1005/1450 | training time in 23 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0073 | Train accuracy: 89.58%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 4.054%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 1006/1450 | training time in 23 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0151 | Train accuracy: 89.11%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.435%.\n",
            "max: attack effectiveness 30.434782608695656%.\n",
            "Mini batch: 1007/1450 | training time in 23 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 93.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 34.667%.\n",
            "max: attack effectiveness 34.66666666666667%.\n",
            "Mini batch: 1008/1450 | training time in 23 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0080 | Train accuracy: 88.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 35.294%.\n",
            "max: attack effectiveness 35.294117647058826%.\n",
            "Mini batch: 1009/1450 | training time in 23 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 91.84%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 33.898%.\n",
            "max: attack effectiveness 33.89830508474576%.\n",
            "Mini batch: 1010/1450 | training time in 23 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0156 | Train accuracy: 90.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 40.000%.\n",
            "max: attack effectiveness 40.0%.\n",
            "Mini batch: 1011/1450 | training time in 23 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0049 | Train accuracy: 87.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1012/1450 | training time in 23 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 90.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 21.875%.\n",
            "max: attack effectiveness 21.875%.\n",
            "Mini batch: 1013/1450 | training time in 23 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0114 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.419%.\n",
            "max: attack effectiveness 27.419354838709676%.\n",
            "Mini batch: 1014/1450 | training time in 23 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 92.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1015/1450 | training time in 23 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0018 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0069 | Train accuracy: 93.87\n",
            "pgd linf: attack effectiveness 2.330%.\n",
            "pgd l2: attack effectiveness 3.584%.\n",
            "pgd l1: attack effectiveness 25.806%.\n",
            "max: attack effectiveness 25.806451612903224%.\n",
            "\tVal accuracy 85.64% with accuracy 74.19% under attack.\n",
            "\tModel select at epoch 34 with validation accuracy 87.61% and accuracy 78.32% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 22.727%.\n",
            "max: attack effectiveness 22.727272727272727%.\n",
            "Mini batch: 1016/1450 | training time in 23 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 19.048%.\n",
            "max: attack effectiveness 19.047619047619047%.\n",
            "Mini batch: 1017/1450 | training time in 23 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0058 | Train accuracy: 94.76%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 1018/1450 | training time in 23 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 95.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.714%.\n",
            "max: attack effectiveness 15.714285714285714%.\n",
            "Mini batch: 1019/1450 | training time in 23 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 94.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 9.859%.\n",
            "max: attack effectiveness 9.859154929577464%.\n",
            "Mini batch: 1020/1450 | training time in 23 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0115 | Train accuracy: 95.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 17.105%.\n",
            "max: attack effectiveness 17.105263157894736%.\n",
            "Mini batch: 1021/1450 | training time in 23 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 94.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1022/1450 | training time in 23 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0048 | Train accuracy: 96.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.493%.\n",
            "max: attack effectiveness 14.492753623188406%.\n",
            "Mini batch: 1023/1450 | training time in 23 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 94.92%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.290%.\n",
            "max: attack effectiveness 11.29032258064516%.\n",
            "Mini batch: 1024/1450 | training time in 24 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 97.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 15.000%.\n",
            "max: attack effectiveness 15.0%.\n",
            "Mini batch: 1025/1450 | training time in 24 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0103 | Train accuracy: 96.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.259%.\n",
            "max: attack effectiveness 9.25925925925926%.\n",
            "Mini batch: 1026/1450 | training time in 24 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 98.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.286%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 1027/1450 | training time in 24 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0084 | Train accuracy: 95.29%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 1028/1450 | training time in 24 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 95.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 1029/1450 | training time in 24 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0174 | Train accuracy: 93.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.077%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 1030/1450 | training time in 24 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0063 | Train accuracy: 93.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1031/1450 | training time in 24 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 93.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 1032/1450 | training time in 24 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 93.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1033/1450 | training time in 24 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 93.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 34.375%.\n",
            "max: attack effectiveness 34.375%.\n",
            "Mini batch: 1034/1450 | training time in 24 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 92.19%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 6.757%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 1035/1450 | training time in 24 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0138 | Train accuracy: 89.11%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 1036/1450 | training time in 24 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 91.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 32.000%.\n",
            "max: attack effectiveness 32.0%.\n",
            "Mini batch: 1037/1450 | training time in 24 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0055 | Train accuracy: 89.16%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.824%.\n",
            "max: attack effectiveness 33.82352941176471%.\n",
            "Mini batch: 1038/1450 | training time in 24 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 91.33%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 1039/1450 | training time in 24 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0199 | Train accuracy: 91.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 38.667%.\n",
            "max: attack effectiveness 38.666666666666664%.\n",
            "Mini batch: 1040/1450 | training time in 24 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 90.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 15.942%.\n",
            "max: attack effectiveness 15.942028985507244%.\n",
            "Mini batch: 1041/1450 | training time in 24 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0089 | Train accuracy: 94.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 10.938%.\n",
            "max: attack effectiveness 10.9375%.\n",
            "Mini batch: 1042/1450 | training time in 24 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0192 | Train accuracy: 95.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 11.290%.\n",
            "max: attack effectiveness 12.903225806451612%.\n",
            "Mini batch: 1043/1450 | training time in 24 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 96.84%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 1044/1450 | training time in 24 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0001 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0066 | Train accuracy: 94.29\n",
            "pgd linf: attack effectiveness 1.792%.\n",
            "pgd l2: attack effectiveness 3.405%.\n",
            "pgd l1: attack effectiveness 22.939%.\n",
            "max: attack effectiveness 22.939068100358423%.\n",
            "\tVal accuracy 86.86% with accuracy 77.06% under attack.\n",
            "\tModel select at epoch 34 with validation accuracy 87.61% and accuracy 78.32% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 18.181818181818183%.\n",
            "Mini batch: 1045/1450 | training time in 24 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.286%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 1046/1450 | training time in 24 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0048 | Train accuracy: 96.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 6.061%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 18.181818181818183%.\n",
            "Mini batch: 1047/1450 | training time in 24 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0062 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 11.429%.\n",
            "max: attack effectiveness 11.428571428571429%.\n",
            "Mini batch: 1048/1450 | training time in 24 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 97.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.859%.\n",
            "max: attack effectiveness 9.859154929577464%.\n",
            "Mini batch: 1049/1450 | training time in 24 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0080 | Train accuracy: 97.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.947%.\n",
            "pgd l1: attack effectiveness 15.789%.\n",
            "max: attack effectiveness 15.789473684210526%.\n",
            "Mini batch: 1050/1450 | training time in 24 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 94.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1051/1450 | training time in 24 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 97.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 15.942%.\n",
            "max: attack effectiveness 15.942028985507244%.\n",
            "Mini batch: 1052/1450 | training time in 24 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0057 | Train accuracy: 95.43%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.613%.\n",
            "pgd l1: attack effectiveness 25.806%.\n",
            "max: attack effectiveness 25.806451612903224%.\n",
            "Mini batch: 1053/1450 | training time in 24 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 94.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.000%.\n",
            "pgd l1: attack effectiveness 18.333%.\n",
            "max: attack effectiveness 18.333333333333332%.\n",
            "Mini batch: 1054/1450 | training time in 24 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0125 | Train accuracy: 95.74%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1055/1450 | training time in 24 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 96.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.111%.\n",
            "max: attack effectiveness 11.11111111111111%.\n",
            "Mini batch: 1056/1450 | training time in 24 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 97.38%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 1057/1450 | training time in 24 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 96.89%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1058/1450 | training time in 24 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0176 | Train accuracy: 95.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 1059/1450 | training time in 24 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0077 | Train accuracy: 93.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1060/1450 | training time in 24 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0070 | Train accuracy: 90.45%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 21.875%.\n",
            "max: attack effectiveness 21.875%.\n",
            "Mini batch: 1061/1450 | training time in 24 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0064 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1062/1450 | training time in 24 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 91.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 32.812%.\n",
            "max: attack effectiveness 32.8125%.\n",
            "Mini batch: 1063/1450 | training time in 24 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0054 | Train accuracy: 90.62%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.351%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 1064/1450 | training time in 25 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0096 | Train accuracy: 89.60%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 1065/1450 | training time in 25 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 94.92%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.000%.\n",
            "max: attack effectiveness 32.0%.\n",
            "Mini batch: 1066/1450 | training time in 25 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 94.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.824%.\n",
            "max: attack effectiveness 33.82352941176471%.\n",
            "Mini batch: 1067/1450 | training time in 25 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 94.90%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 13.559%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 1068/1450 | training time in 25 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0268 | Train accuracy: 91.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.333%.\n",
            "pgd l1: attack effectiveness 40.000%.\n",
            "max: attack effectiveness 40.0%.\n",
            "Mini batch: 1069/1450 | training time in 25 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0064 | Train accuracy: 88.18%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 15.942%.\n",
            "max: attack effectiveness 15.942028985507244%.\n",
            "Mini batch: 1070/1450 | training time in 25 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0052 | Train accuracy: 94.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 17.188%.\n",
            "max: attack effectiveness 17.1875%.\n",
            "Mini batch: 1071/1450 | training time in 25 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0137 | Train accuracy: 94.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.516%.\n",
            "max: attack effectiveness 14.516129032258066%.\n",
            "Mini batch: 1072/1450 | training time in 25 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 95.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 1073/1450 | training time in 25 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0007 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0064 | Train accuracy: 94.68\n",
            "pgd linf: attack effectiveness 2.151%.\n",
            "pgd l2: attack effectiveness 3.405%.\n",
            "pgd l1: attack effectiveness 40.502%.\n",
            "max: attack effectiveness 40.50179211469534%.\n",
            "\tVal accuracy 78.33% with accuracy 59.5% under attack.\n",
            "\tModel select at epoch 34 with validation accuracy 87.61% and accuracy 78.32% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1074/1450 | training time in 25 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0054 | Train accuracy: 89.69%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.873%.\n",
            "max: attack effectiveness 15.873015873015872%.\n",
            "Mini batch: 1075/1450 | training time in 25 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0067 | Train accuracy: 96.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 1076/1450 | training time in 25 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 14.286%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 1077/1450 | training time in 25 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0048 | Train accuracy: 95.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.817%.\n",
            "pgd l1: attack effectiveness 9.859%.\n",
            "max: attack effectiveness 9.859154929577464%.\n",
            "Mini batch: 1078/1450 | training time in 25 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0109 | Train accuracy: 97.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 17.105%.\n",
            "max: attack effectiveness 17.105263157894736%.\n",
            "Mini batch: 1079/1450 | training time in 25 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 94.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1080/1450 | training time in 25 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 97.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 17.391%.\n",
            "max: attack effectiveness 17.391304347826086%.\n",
            "Mini batch: 1081/1450 | training time in 25 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0065 | Train accuracy: 95.43%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.290%.\n",
            "max: attack effectiveness 11.29032258064516%.\n",
            "Mini batch: 1082/1450 | training time in 25 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 97.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1083/1450 | training time in 25 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0132 | Train accuracy: 95.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.815%.\n",
            "max: attack effectiveness 14.814814814814813%.\n",
            "Mini batch: 1084/1450 | training time in 25 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 96.70%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.175%.\n",
            "pgd l1: attack effectiveness 12.698%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 1085/1450 | training time in 25 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 97.38%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 1086/1450 | training time in 25 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 95.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1087/1450 | training time in 25 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0212 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 18.462%.\n",
            "max: attack effectiveness 18.461538461538463%.\n",
            "Mini batch: 1088/1450 | training time in 25 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0090 | Train accuracy: 94.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1089/1450 | training time in 25 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 90.45%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 21.875%.\n",
            "max: attack effectiveness 21.875%.\n",
            "Mini batch: 1090/1450 | training time in 25 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 94.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1091/1450 | training time in 25 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 93.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.250%.\n",
            "max: attack effectiveness 31.25%.\n",
            "Mini batch: 1092/1450 | training time in 25 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0091 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.054%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 1093/1450 | training time in 25 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0119 | Train accuracy: 90.59%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 1094/1450 | training time in 25 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 94.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 30.667%.\n",
            "max: attack effectiveness 30.666666666666664%.\n",
            "Mini batch: 1095/1450 | training time in 25 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0049 | Train accuracy: 91.13%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 38.235%.\n",
            "max: attack effectiveness 38.23529411764706%.\n",
            "Mini batch: 1096/1450 | training time in 25 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 89.29%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 33.898%.\n",
            "max: attack effectiveness 33.89830508474576%.\n",
            "Mini batch: 1097/1450 | training time in 25 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0190 | Train accuracy: 89.84%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 40.000%.\n",
            "max: attack effectiveness 40.0%.\n",
            "Mini batch: 1098/1450 | training time in 25 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 87.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 36.232%.\n",
            "max: attack effectiveness 36.231884057971016%.\n",
            "Mini batch: 1099/1450 | training time in 25 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0066 | Train accuracy: 91.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 20.312%.\n",
            "max: attack effectiveness 20.3125%.\n",
            "Mini batch: 1100/1450 | training time in 25 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0144 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.806%.\n",
            "max: attack effectiveness 25.806451612903224%.\n",
            "Mini batch: 1101/1450 | training time in 25 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 94.74%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1102/1450 | training time in 25 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0016 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0067 | Train accuracy: 93.85\n",
            "pgd linf: attack effectiveness 2.330%.\n",
            "pgd l2: attack effectiveness 3.226%.\n",
            "pgd l1: attack effectiveness 23.118%.\n",
            "max: attack effectiveness 23.118279569892472%.\n",
            "\tVal accuracy 86.94% with accuracy 76.88% under attack.\n",
            "\tModel select at epoch 34 with validation accuracy 87.61% and accuracy 78.32% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1103/1450 | training time in 25 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0073 | Train accuracy: 96.39%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 17.460%.\n",
            "max: attack effectiveness 17.46031746031746%.\n",
            "Mini batch: 1104/1450 | training time in 25 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0074 | Train accuracy: 94.76%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 18.181818181818183%.\n",
            "Mini batch: 1105/1450 | training time in 26 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0029 | Train accuracy: 96.39%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 10.000%.\n",
            "max: attack effectiveness 10.0%.\n",
            "Mini batch: 1106/1450 | training time in 26 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0021 | Train accuracy: 97.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 7.042%.\n",
            "max: attack effectiveness 7.042253521126761%.\n",
            "Mini batch: 1107/1450 | training time in 26 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0112 | Train accuracy: 97.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 6.579%.\n",
            "max: attack effectiveness 6.578947368421052%.\n",
            "Mini batch: 1108/1450 | training time in 26 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0013 | Train accuracy: 97.55%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1109/1450 | training time in 26 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 97.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 13.043%.\n",
            "max: attack effectiveness 13.043478260869565%.\n",
            "Mini batch: 1110/1450 | training time in 26 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0069 | Train accuracy: 95.94%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.677%.\n",
            "max: attack effectiveness 9.67741935483871%.\n",
            "Mini batch: 1111/1450 | training time in 26 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 97.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 13.333%.\n",
            "max: attack effectiveness 13.333333333333334%.\n",
            "Mini batch: 1112/1450 | training time in 26 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0094 | Train accuracy: 96.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.259%.\n",
            "max: attack effectiveness 9.25925925925926%.\n",
            "Mini batch: 1113/1450 | training time in 26 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 97.25%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.524%.\n",
            "max: attack effectiveness 9.523809523809524%.\n",
            "Mini batch: 1114/1450 | training time in 26 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 96.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 1115/1450 | training time in 26 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 95.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1116/1450 | training time in 26 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0204 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 1117/1450 | training time in 26 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 93.78%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1118/1450 | training time in 26 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 93.97%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 23.438%.\n",
            "max: attack effectiveness 23.4375%.\n",
            "Mini batch: 1119/1450 | training time in 26 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1120/1450 | training time in 26 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 93.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.812%.\n",
            "max: attack effectiveness 32.8125%.\n",
            "Mini batch: 1121/1450 | training time in 26 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 91.67%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 1.351%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 1122/1450 | training time in 26 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0145 | Train accuracy: 90.59%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.435%.\n",
            "max: attack effectiveness 30.434782608695656%.\n",
            "Mini batch: 1123/1450 | training time in 26 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 94.92%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 30.667%.\n",
            "max: attack effectiveness 30.666666666666664%.\n",
            "Mini batch: 1124/1450 | training time in 26 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 92.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 35.294%.\n",
            "max: attack effectiveness 35.294117647058826%.\n",
            "Mini batch: 1125/1450 | training time in 26 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 91.84%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 13.559%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 1126/1450 | training time in 26 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0314 | Train accuracy: 91.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.333%.\n",
            "pgd l1: attack effectiveness 38.667%.\n",
            "max: attack effectiveness 38.666666666666664%.\n",
            "Mini batch: 1127/1450 | training time in 26 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 90.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1128/1450 | training time in 26 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0070 | Train accuracy: 90.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.625%.\n",
            "max: attack effectiveness 15.625%.\n",
            "Mini batch: 1129/1450 | training time in 26 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0105 | Train accuracy: 95.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.677%.\n",
            "max: attack effectiveness 9.67741935483871%.\n",
            "Mini batch: 1130/1450 | training time in 26 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0023 | Train accuracy: 96.84%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 1131/1450 | training time in 26 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0000 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0066 | Train accuracy: 94.95\n",
            "pgd linf: attack effectiveness 2.509%.\n",
            "pgd l2: attack effectiveness 3.405%.\n",
            "pgd l1: attack effectiveness 20.789%.\n",
            "max: attack effectiveness 20.78853046594982%.\n",
            "\tVal accuracy 88.15% with accuracy 79.21% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 1132/1450 | training time in 26 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 93.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.286%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 1133/1450 | training time in 26 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0056 | Train accuracy: 95.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 18.181818181818183%.\n",
            "Mini batch: 1134/1450 | training time in 26 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 96.39%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 8.571%.\n",
            "max: attack effectiveness 8.571428571428571%.\n",
            "Mini batch: 1135/1450 | training time in 26 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0019 | Train accuracy: 97.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 5.634%.\n",
            "max: attack effectiveness 5.633802816901409%.\n",
            "Mini batch: 1136/1450 | training time in 26 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0133 | Train accuracy: 97.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.474%.\n",
            "max: attack effectiveness 14.473684210526317%.\n",
            "Mini batch: 1137/1450 | training time in 26 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 96.08%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 1.639%.\n",
            "max: attack effectiveness 1.639344262295082%.\n",
            "Mini batch: 1138/1450 | training time in 26 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 99.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 4.348%.\n",
            "max: attack effectiveness 4.3478260869565215%.\n",
            "Mini batch: 1139/1450 | training time in 26 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0080 | Train accuracy: 98.48%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.677%.\n",
            "max: attack effectiveness 9.67741935483871%.\n",
            "Mini batch: 1140/1450 | training time in 26 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 97.89%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 10.000%.\n",
            "max: attack effectiveness 10.0%.\n",
            "Mini batch: 1141/1450 | training time in 26 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0127 | Train accuracy: 97.87%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.852%.\n",
            "pgd l1: attack effectiveness 9.259%.\n",
            "max: attack effectiveness 9.25925925925926%.\n",
            "Mini batch: 1142/1450 | training time in 26 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0040 | Train accuracy: 97.80%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.524%.\n",
            "max: attack effectiveness 9.523809523809524%.\n",
            "Mini batch: 1143/1450 | training time in 26 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 97.38%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 1144/1450 | training time in 26 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 95.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1145/1450 | training time in 26 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0228 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 1146/1450 | training time in 26 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0064 | Train accuracy: 95.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1147/1450 | training time in 27 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 90.45%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 23.438%.\n",
            "max: attack effectiveness 23.4375%.\n",
            "Mini batch: 1148/1450 | training time in 27 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0029 | Train accuracy: 94.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1149/1450 | training time in 27 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0040 | Train accuracy: 92.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.812%.\n",
            "max: attack effectiveness 32.8125%.\n",
            "Mini batch: 1150/1450 | training time in 27 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0085 | Train accuracy: 90.62%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 4.054%.\n",
            "pgd l1: attack effectiveness 40.541%.\n",
            "max: attack effectiveness 40.54054054054054%.\n",
            "Mini batch: 1151/1450 | training time in 27 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0142 | Train accuracy: 90.59%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.797%.\n",
            "pgd l1: attack effectiveness 31.884%.\n",
            "max: attack effectiveness 31.88405797101449%.\n",
            "Mini batch: 1152/1450 | training time in 27 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 91.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1153/1450 | training time in 27 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0040 | Train accuracy: 91.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 36.765%.\n",
            "max: attack effectiveness 36.76470588235294%.\n",
            "Mini batch: 1154/1450 | training time in 27 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 91.84%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 33.898%.\n",
            "max: attack effectiveness 33.89830508474576%.\n",
            "Mini batch: 1155/1450 | training time in 27 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0196 | Train accuracy: 91.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.000%.\n",
            "pgd l1: attack effectiveness 37.333%.\n",
            "max: attack effectiveness 37.333333333333336%.\n",
            "Mini batch: 1156/1450 | training time in 27 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 92.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 34.783%.\n",
            "max: attack effectiveness 34.78260869565217%.\n",
            "Mini batch: 1157/1450 | training time in 27 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 90.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 17.188%.\n",
            "max: attack effectiveness 17.1875%.\n",
            "Mini batch: 1158/1450 | training time in 27 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0136 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.194%.\n",
            "max: attack effectiveness 24.193548387096776%.\n",
            "Mini batch: 1159/1450 | training time in 27 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0029 | Train accuracy: 93.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1160/1450 | training time in 27 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0017 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0066 | Train accuracy: 94.58\n",
            "pgd linf: attack effectiveness 2.151%.\n",
            "pgd l2: attack effectiveness 3.047%.\n",
            "pgd l1: attack effectiveness 37.276%.\n",
            "max: attack effectiveness 37.27598566308244%.\n",
            "\tVal accuracy 79.9% with accuracy 62.72% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.788%.\n",
            "max: attack effectiveness 28.78787878787879%.\n",
            "Mini batch: 1161/1450 | training time in 27 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0070 | Train accuracy: 92.78%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.746%.\n",
            "max: attack effectiveness 31.746031746031743%.\n",
            "Mini batch: 1162/1450 | training time in 27 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 92.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1163/1450 | training time in 27 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0029 | Train accuracy: 96.39%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 8.571%.\n",
            "max: attack effectiveness 8.571428571428571%.\n",
            "Mini batch: 1164/1450 | training time in 27 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 97.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 8.451%.\n",
            "max: attack effectiveness 8.450704225352112%.\n",
            "Mini batch: 1165/1450 | training time in 27 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0188 | Train accuracy: 97.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 17.105%.\n",
            "max: attack effectiveness 17.105263157894736%.\n",
            "Mini batch: 1166/1450 | training time in 27 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0018 | Train accuracy: 96.57%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.590%.\n",
            "max: attack effectiveness 24.59016393442623%.\n",
            "Mini batch: 1167/1450 | training time in 27 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 95.77%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 23.188%.\n",
            "max: attack effectiveness 23.18840579710145%.\n",
            "Mini batch: 1168/1450 | training time in 27 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0055 | Train accuracy: 91.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.194%.\n",
            "max: attack effectiveness 24.193548387096776%.\n",
            "Mini batch: 1169/1450 | training time in 27 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0101 | Train accuracy: 94.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 38.333%.\n",
            "max: attack effectiveness 38.333333333333336%.\n",
            "Mini batch: 1170/1450 | training time in 27 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0122 | Train accuracy: 90.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.074%.\n",
            "max: attack effectiveness 24.074074074074073%.\n",
            "Mini batch: 1171/1450 | training time in 27 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 95.60%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.524%.\n",
            "max: attack effectiveness 9.523809523809524%.\n",
            "Mini batch: 1172/1450 | training time in 27 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0016 | Train accuracy: 98.43%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 1173/1450 | training time in 27 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 92.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1174/1450 | training time in 27 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0136 | Train accuracy: 95.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 1175/1450 | training time in 27 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0067 | Train accuracy: 96.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.268%.\n",
            "max: attack effectiveness 11.267605633802818%.\n",
            "Mini batch: 1176/1450 | training time in 27 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 97.99%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.062%.\n",
            "max: attack effectiveness 14.0625%.\n",
            "Mini batch: 1177/1450 | training time in 27 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 96.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.925%.\n",
            "max: attack effectiveness 14.925373134328357%.\n",
            "Mini batch: 1178/1450 | training time in 27 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 95.90%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.250%.\n",
            "max: attack effectiveness 31.25%.\n",
            "Mini batch: 1179/1450 | training time in 27 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0117 | Train accuracy: 92.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 1180/1450 | training time in 27 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0118 | Train accuracy: 89.11%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 1181/1450 | training time in 27 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 92.39%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 12.000%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1182/1450 | training time in 27 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0079 | Train accuracy: 88.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 8.824%.\n",
            "pgd l1: attack effectiveness 36.765%.\n",
            "max: attack effectiveness 36.76470588235294%.\n",
            "Mini batch: 1183/1450 | training time in 27 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 90.31%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 35.593%.\n",
            "max: attack effectiveness 35.59322033898305%.\n",
            "Mini batch: 1184/1450 | training time in 27 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0173 | Train accuracy: 90.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.333%.\n",
            "pgd l1: attack effectiveness 41.333%.\n",
            "max: attack effectiveness 41.333333333333336%.\n",
            "Mini batch: 1185/1450 | training time in 27 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 90.64%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 39.130%.\n",
            "max: attack effectiveness 39.130434782608695%.\n",
            "Mini batch: 1186/1450 | training time in 27 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0081 | Train accuracy: 89.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 29.688%.\n",
            "max: attack effectiveness 29.6875%.\n",
            "Mini batch: 1187/1450 | training time in 28 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0129 | Train accuracy: 92.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.839%.\n",
            "pgd l1: attack effectiveness 27.419%.\n",
            "max: attack effectiveness 27.419354838709676%.\n",
            "Mini batch: 1188/1450 | training time in 28 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 93.16%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 16.667%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1189/1450 | training time in 28 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 89.47%.\n",
            "Training loss (epoch level): 0.0070 | Train accuracy: 93.58\n",
            "pgd linf: attack effectiveness 2.151%.\n",
            "pgd l2: attack effectiveness 3.047%.\n",
            "pgd l1: attack effectiveness 39.427%.\n",
            "max: attack effectiveness 39.42652329749104%.\n",
            "\tVal accuracy 78.91% with accuracy 60.57% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.303%.\n",
            "max: attack effectiveness 30.303030303030305%.\n",
            "Mini batch: 1190/1450 | training time in 28 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 91.24%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 7.937%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1191/1450 | training time in 28 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0082 | Train accuracy: 90.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1192/1450 | training time in 28 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 95.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 7.143%.\n",
            "max: attack effectiveness 7.142857142857142%.\n",
            "Mini batch: 1193/1450 | training time in 28 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0021 | Train accuracy: 97.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 8.451%.\n",
            "max: attack effectiveness 8.450704225352112%.\n",
            "Mini batch: 1194/1450 | training time in 28 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0195 | Train accuracy: 97.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 17.105%.\n",
            "max: attack effectiveness 17.105263157894736%.\n",
            "Mini batch: 1195/1450 | training time in 28 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 94.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1196/1450 | training time in 28 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 99.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 14.493%.\n",
            "max: attack effectiveness 14.492753623188406%.\n",
            "Mini batch: 1197/1450 | training time in 28 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0062 | Train accuracy: 95.43%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 22.581%.\n",
            "max: attack effectiveness 22.58064516129032%.\n",
            "Mini batch: 1198/1450 | training time in 28 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0082 | Train accuracy: 93.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1199/1450 | training time in 28 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0113 | Train accuracy: 93.09%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.074%.\n",
            "max: attack effectiveness 24.074074074074073%.\n",
            "Mini batch: 1200/1450 | training time in 28 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 95.05%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.397%.\n",
            "max: attack effectiveness 25.396825396825395%.\n",
            "Mini batch: 1201/1450 | training time in 28 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 95.29%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 1202/1450 | training time in 28 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 92.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 25.758%.\n",
            "max: attack effectiveness 25.757575757575758%.\n",
            "Mini batch: 1203/1450 | training time in 28 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0165 | Train accuracy: 92.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 1204/1450 | training time in 28 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 93.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 11.268%.\n",
            "max: attack effectiveness 12.676056338028168%.\n",
            "Mini batch: 1205/1450 | training time in 28 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 96.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 20.312%.\n",
            "max: attack effectiveness 20.3125%.\n",
            "Mini batch: 1206/1450 | training time in 28 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 94.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1207/1450 | training time in 28 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 92.82%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.625%.\n",
            "max: attack effectiveness 15.625%.\n",
            "Mini batch: 1208/1450 | training time in 28 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0117 | Train accuracy: 95.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 1209/1450 | training time in 28 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0080 | Train accuracy: 90.59%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 1210/1450 | training time in 28 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 89.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.333%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1211/1450 | training time in 28 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 91.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 36.765%.\n",
            "max: attack effectiveness 36.76470588235294%.\n",
            "Mini batch: 1212/1450 | training time in 28 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 92.35%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 1213/1450 | training time in 28 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0221 | Train accuracy: 93.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 40.000%.\n",
            "max: attack effectiveness 40.0%.\n",
            "Mini batch: 1214/1450 | training time in 28 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 90.64%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1215/1450 | training time in 28 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0062 | Train accuracy: 88.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 21.875%.\n",
            "max: attack effectiveness 21.875%.\n",
            "Mini batch: 1216/1450 | training time in 28 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0136 | Train accuracy: 93.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.839%.\n",
            "pgd l1: attack effectiveness 30.645%.\n",
            "max: attack effectiveness 30.64516129032258%.\n",
            "Mini batch: 1217/1450 | training time in 28 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0065 | Train accuracy: 92.11%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1218/1450 | training time in 28 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0021 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0069 | Train accuracy: 93.62\n",
            "pgd linf: attack effectiveness 2.688%.\n",
            "pgd l2: attack effectiveness 4.122%.\n",
            "pgd l1: attack effectiveness 22.401%.\n",
            "max: attack effectiveness 22.401433691756274%.\n",
            "\tVal accuracy 87.3% with accuracy 77.6% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 1219/1450 | training time in 28 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0052 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.286%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 1220/1450 | training time in 28 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0082 | Train accuracy: 96.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1221/1450 | training time in 28 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 10.000%.\n",
            "max: attack effectiveness 10.0%.\n",
            "Mini batch: 1222/1450 | training time in 28 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 96.97%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 9.859%.\n",
            "max: attack effectiveness 9.859154929577464%.\n",
            "Mini batch: 1223/1450 | training time in 28 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0089 | Train accuracy: 97.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.632%.\n",
            "pgd l1: attack effectiveness 14.474%.\n",
            "max: attack effectiveness 14.473684210526317%.\n",
            "Mini batch: 1224/1450 | training time in 29 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 96.57%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1225/1450 | training time in 29 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 98.94%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 13.043%.\n",
            "max: attack effectiveness 13.043478260869565%.\n",
            "Mini batch: 1226/1450 | training time in 29 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0065 | Train accuracy: 95.94%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.677%.\n",
            "max: attack effectiveness 9.67741935483871%.\n",
            "Mini batch: 1227/1450 | training time in 29 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 98.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.000%.\n",
            "pgd l1: attack effectiveness 11.667%.\n",
            "max: attack effectiveness 11.666666666666666%.\n",
            "Mini batch: 1228/1450 | training time in 29 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0099 | Train accuracy: 96.28%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.259%.\n",
            "max: attack effectiveness 9.25925925925926%.\n",
            "Mini batch: 1229/1450 | training time in 29 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 97.25%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.587%.\n",
            "pgd l1: attack effectiveness 9.524%.\n",
            "max: attack effectiveness 9.523809523809524%.\n",
            "Mini batch: 1230/1450 | training time in 29 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 97.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 1231/1450 | training time in 29 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 97.93%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1232/1450 | training time in 29 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0188 | Train accuracy: 95.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 1233/1450 | training time in 29 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0068 | Train accuracy: 91.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1234/1450 | training time in 29 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0054 | Train accuracy: 91.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 23.438%.\n",
            "max: attack effectiveness 23.4375%.\n",
            "Mini batch: 1235/1450 | training time in 29 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 10.448%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1236/1450 | training time in 29 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 94.87%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.250%.\n",
            "max: attack effectiveness 31.25%.\n",
            "Mini batch: 1237/1450 | training time in 29 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.351%.\n",
            "pgd l1: attack effectiveness 37.838%.\n",
            "max: attack effectiveness 37.83783783783784%.\n",
            "Mini batch: 1238/1450 | training time in 29 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0083 | Train accuracy: 90.10%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 1239/1450 | training time in 29 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 93.40%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.333%.\n",
            "pgd l1: attack effectiveness 30.667%.\n",
            "max: attack effectiveness 32.0%.\n",
            "Mini batch: 1240/1450 | training time in 29 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0049 | Train accuracy: 91.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.824%.\n",
            "max: attack effectiveness 33.82352941176471%.\n",
            "Mini batch: 1241/1450 | training time in 29 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 90.82%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 1242/1450 | training time in 29 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0240 | Train accuracy: 91.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.333%.\n",
            "pgd l1: attack effectiveness 38.667%.\n",
            "max: attack effectiveness 38.666666666666664%.\n",
            "Mini batch: 1243/1450 | training time in 29 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 88.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 31.884%.\n",
            "max: attack effectiveness 31.88405797101449%.\n",
            "Mini batch: 1244/1450 | training time in 29 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 92.89%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 18.750%.\n",
            "max: attack effectiveness 18.75%.\n",
            "Mini batch: 1245/1450 | training time in 29 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0114 | Train accuracy: 96.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 8.065%.\n",
            "max: attack effectiveness 8.064516129032258%.\n",
            "Mini batch: 1246/1450 | training time in 29 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0016 | Train accuracy: 98.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 1247/1450 | training time in 29 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0002 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0060 | Train accuracy: 95.08\n",
            "pgd linf: attack effectiveness 1.254%.\n",
            "pgd l2: attack effectiveness 2.688%.\n",
            "pgd l1: attack effectiveness 22.043%.\n",
            "max: attack effectiveness 22.401433691756274%.\n",
            "\tVal accuracy 87.09% with accuracy 77.6% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.545%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1248/1450 | training time in 29 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0094 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 12.698%.\n",
            "max: attack effectiveness 12.698412698412698%.\n",
            "Mini batch: 1249/1450 | training time in 29 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 96.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.545%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1250/1450 | training time in 29 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 95.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 7.143%.\n",
            "max: attack effectiveness 7.142857142857142%.\n",
            "Mini batch: 1251/1450 | training time in 29 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 97.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 9.859%.\n",
            "max: attack effectiveness 9.859154929577464%.\n",
            "Mini batch: 1252/1450 | training time in 29 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0148 | Train accuracy: 95.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.316%.\n",
            "pgd l1: attack effectiveness 14.474%.\n",
            "max: attack effectiveness 14.473684210526317%.\n",
            "Mini batch: 1253/1450 | training time in 29 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 95.10%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1254/1450 | training time in 29 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 97.35%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 13.043%.\n",
            "max: attack effectiveness 13.043478260869565%.\n",
            "Mini batch: 1255/1450 | training time in 29 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 95.43%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.677%.\n",
            "max: attack effectiveness 9.67741935483871%.\n",
            "Mini batch: 1256/1450 | training time in 29 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0023 | Train accuracy: 97.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 13.333%.\n",
            "max: attack effectiveness 13.333333333333334%.\n",
            "Mini batch: 1257/1450 | training time in 29 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0093 | Train accuracy: 97.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.259%.\n",
            "max: attack effectiveness 9.25925925925926%.\n",
            "Mini batch: 1258/1450 | training time in 29 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 98.90%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.175%.\n",
            "pgd l1: attack effectiveness 9.524%.\n",
            "max: attack effectiveness 12.698412698412698%.\n",
            "Mini batch: 1259/1450 | training time in 29 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 97.38%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 1260/1450 | training time in 29 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 95.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 10.606%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1261/1450 | training time in 29 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0203 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 16.923%.\n",
            "max: attack effectiveness 16.923076923076923%.\n",
            "Mini batch: 1262/1450 | training time in 29 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0066 | Train accuracy: 94.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1263/1450 | training time in 29 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 94.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 21.875%.\n",
            "max: attack effectiveness 21.875%.\n",
            "Mini batch: 1264/1450 | training time in 29 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0057 | Train accuracy: 94.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1265/1450 | training time in 30 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 93.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.250%.\n",
            "max: attack effectiveness 31.25%.\n",
            "Mini batch: 1266/1450 | training time in 30 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0049 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.703%.\n",
            "pgd l1: attack effectiveness 39.189%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 1267/1450 | training time in 30 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0088 | Train accuracy: 89.11%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 1268/1450 | training time in 30 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 92.39%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.667%.\n",
            "pgd l1: attack effectiveness 29.333%.\n",
            "max: attack effectiveness 29.333333333333332%.\n",
            "Mini batch: 1269/1450 | training time in 30 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 92.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.941%.\n",
            "max: attack effectiveness 27.941176470588236%.\n",
            "Mini batch: 1270/1450 | training time in 30 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 91.84%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 11.864%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 1271/1450 | training time in 30 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0192 | Train accuracy: 93.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 22.667%.\n",
            "pgd l1: attack effectiveness 36.000%.\n",
            "max: attack effectiveness 36.0%.\n",
            "Mini batch: 1272/1450 | training time in 30 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 92.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 31.884%.\n",
            "max: attack effectiveness 31.88405797101449%.\n",
            "Mini batch: 1273/1450 | training time in 30 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0052 | Train accuracy: 90.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 18.750%.\n",
            "max: attack effectiveness 18.75%.\n",
            "Mini batch: 1274/1450 | training time in 30 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0119 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.194%.\n",
            "max: attack effectiveness 24.193548387096776%.\n",
            "Mini batch: 1275/1450 | training time in 30 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0025 | Train accuracy: 94.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1276/1450 | training time in 30 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0010 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0059 | Train accuracy: 94.83\n",
            "pgd linf: attack effectiveness 1.792%.\n",
            "pgd l2: attack effectiveness 2.688%.\n",
            "pgd l1: attack effectiveness 22.043%.\n",
            "max: attack effectiveness 22.043010752688172%.\n",
            "\tVal accuracy 86.94% with accuracy 77.96% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1277/1450 | training time in 30 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.286%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 1278/1450 | training time in 30 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 96.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1279/1450 | training time in 30 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 7.143%.\n",
            "max: attack effectiveness 7.142857142857142%.\n",
            "Mini batch: 1280/1450 | training time in 30 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0016 | Train accuracy: 98.48%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 8.451%.\n",
            "max: attack effectiveness 8.450704225352112%.\n",
            "Mini batch: 1281/1450 | training time in 30 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0078 | Train accuracy: 96.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.316%.\n",
            "pgd l1: attack effectiveness 14.474%.\n",
            "max: attack effectiveness 14.473684210526317%.\n",
            "Mini batch: 1282/1450 | training time in 30 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 96.57%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1283/1450 | training time in 30 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 96.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 15.942%.\n",
            "max: attack effectiveness 15.942028985507244%.\n",
            "Mini batch: 1284/1450 | training time in 30 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0072 | Train accuracy: 96.45%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.677%.\n",
            "max: attack effectiveness 9.67741935483871%.\n",
            "Mini batch: 1285/1450 | training time in 30 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 97.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1286/1450 | training time in 30 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0133 | Train accuracy: 95.74%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.815%.\n",
            "max: attack effectiveness 14.814814814814813%.\n",
            "Mini batch: 1287/1450 | training time in 30 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 96.70%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.524%.\n",
            "max: attack effectiveness 9.523809523809524%.\n",
            "Mini batch: 1288/1450 | training time in 30 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 97.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 15.385%.\n",
            "max: attack effectiveness 15.384615384615385%.\n",
            "Mini batch: 1289/1450 | training time in 30 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0023 | Train accuracy: 96.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 24.242%.\n",
            "max: attack effectiveness 24.242424242424242%.\n",
            "Mini batch: 1290/1450 | training time in 30 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0145 | Train accuracy: 93.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 1291/1450 | training time in 30 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 93.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 30.985915492957744%.\n",
            "Mini batch: 1292/1450 | training time in 30 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 94.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 21.875%.\n",
            "max: attack effectiveness 21.875%.\n",
            "Mini batch: 1293/1450 | training time in 30 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 94.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.493%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1294/1450 | training time in 30 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 92.82%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.250%.\n",
            "max: attack effectiveness 31.25%.\n",
            "Mini batch: 1295/1450 | training time in 30 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 94.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 35.135%.\n",
            "max: attack effectiveness 35.13513513513514%.\n",
            "Mini batch: 1296/1450 | training time in 30 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0077 | Train accuracy: 91.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 1297/1450 | training time in 30 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 91.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1298/1450 | training time in 30 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0087 | Train accuracy: 89.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 38.235%.\n",
            "max: attack effectiveness 38.23529411764706%.\n",
            "Mini batch: 1299/1450 | training time in 30 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 88.27%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 33.898%.\n",
            "max: attack effectiveness 33.89830508474576%.\n",
            "Mini batch: 1300/1450 | training time in 30 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0222 | Train accuracy: 90.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 38.667%.\n",
            "max: attack effectiveness 38.666666666666664%.\n",
            "Mini batch: 1301/1450 | training time in 30 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 90.64%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1302/1450 | training time in 30 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 90.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.688%.\n",
            "pgd l1: attack effectiveness 25.000%.\n",
            "max: attack effectiveness 25.0%.\n",
            "Mini batch: 1303/1450 | training time in 30 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0195 | Train accuracy: 93.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.839%.\n",
            "pgd l1: attack effectiveness 27.419%.\n",
            "max: attack effectiveness 27.419354838709676%.\n",
            "Mini batch: 1304/1450 | training time in 30 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0040 | Train accuracy: 91.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 1305/1450 | training time in 30 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0001 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0059 | Train accuracy: 94.41\n",
            "pgd linf: attack effectiveness 2.688%.\n",
            "pgd l2: attack effectiveness 3.405%.\n",
            "pgd l1: attack effectiveness 22.401%.\n",
            "max: attack effectiveness 22.401433691756274%.\n",
            "\tVal accuracy 87.42% with accuracy 77.6% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 1306/1450 | training time in 30 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 7.937%.\n",
            "pgd l1: attack effectiveness 12.698%.\n",
            "max: attack effectiveness 12.698412698412698%.\n",
            "Mini batch: 1307/1450 | training time in 31 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0068 | Train accuracy: 95.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.788%.\n",
            "max: attack effectiveness 28.78787878787879%.\n",
            "Mini batch: 1308/1450 | training time in 31 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 92.78%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 2.857%.\n",
            "max: attack effectiveness 2.857142857142857%.\n",
            "Mini batch: 1309/1450 | training time in 31 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 99.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.408%.\n",
            "pgd l1: attack effectiveness 5.634%.\n",
            "max: attack effectiveness 5.633802816901409%.\n",
            "Mini batch: 1310/1450 | training time in 31 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0099 | Train accuracy: 97.99%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.316%.\n",
            "pgd l1: attack effectiveness 5.263%.\n",
            "max: attack effectiveness 5.263157894736842%.\n",
            "Mini batch: 1311/1450 | training time in 31 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0018 | Train accuracy: 98.04%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 3.279%.\n",
            "max: attack effectiveness 3.278688524590164%.\n",
            "Mini batch: 1312/1450 | training time in 31 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 98.94%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 5.797%.\n",
            "max: attack effectiveness 5.797101449275362%.\n",
            "Mini batch: 1313/1450 | training time in 31 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0055 | Train accuracy: 97.46%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 4.839%.\n",
            "max: attack effectiveness 4.838709677419355%.\n",
            "Mini batch: 1314/1450 | training time in 31 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0019 | Train accuracy: 98.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.667%.\n",
            "pgd l1: attack effectiveness 10.000%.\n",
            "max: attack effectiveness 10.0%.\n",
            "Mini batch: 1315/1450 | training time in 31 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0094 | Train accuracy: 97.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 3.704%.\n",
            "max: attack effectiveness 3.7037037037037033%.\n",
            "Mini batch: 1316/1450 | training time in 31 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 98.90%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.111%.\n",
            "max: attack effectiveness 11.11111111111111%.\n",
            "Mini batch: 1317/1450 | training time in 31 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0054 | Train accuracy: 96.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 16.923%.\n",
            "max: attack effectiveness 16.923076923076923%.\n",
            "Mini batch: 1318/1450 | training time in 31 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 96.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1319/1450 | training time in 31 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0182 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.077%.\n",
            "pgd l1: attack effectiveness 16.923%.\n",
            "max: attack effectiveness 16.923076923076923%.\n",
            "Mini batch: 1320/1450 | training time in 31 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0062 | Train accuracy: 95.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1321/1450 | training time in 31 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 91.46%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 23.438%.\n",
            "max: attack effectiveness 23.4375%.\n",
            "Mini batch: 1322/1450 | training time in 31 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 94.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1323/1450 | training time in 31 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 95.38%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.812%.\n",
            "max: attack effectiveness 32.8125%.\n",
            "Mini batch: 1324/1450 | training time in 31 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 93.23%.\n",
            "pgd linf: attack effectiveness 1.351%.\n",
            "pgd l2: attack effectiveness 4.054%.\n",
            "pgd l1: attack effectiveness 37.838%.\n",
            "max: attack effectiveness 39.189189189189186%.\n",
            "Mini batch: 1325/1450 | training time in 31 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0120 | Train accuracy: 89.60%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.884%.\n",
            "max: attack effectiveness 31.88405797101449%.\n",
            "Mini batch: 1326/1450 | training time in 31 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 91.37%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 14.667%.\n",
            "pgd l1: attack effectiveness 32.000%.\n",
            "max: attack effectiveness 32.0%.\n",
            "Mini batch: 1327/1450 | training time in 31 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0054 | Train accuracy: 91.63%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 36.765%.\n",
            "max: attack effectiveness 36.76470588235294%.\n",
            "Mini batch: 1328/1450 | training time in 31 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 92.35%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 33.898%.\n",
            "max: attack effectiveness 33.89830508474576%.\n",
            "Mini batch: 1329/1450 | training time in 31 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0194 | Train accuracy: 93.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 37.333%.\n",
            "max: attack effectiveness 37.333333333333336%.\n",
            "Mini batch: 1330/1450 | training time in 31 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 90.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 34.783%.\n",
            "max: attack effectiveness 34.78260869565217%.\n",
            "Mini batch: 1331/1450 | training time in 31 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0052 | Train accuracy: 89.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 20.312%.\n",
            "max: attack effectiveness 20.3125%.\n",
            "Mini batch: 1332/1450 | training time in 31 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0088 | Train accuracy: 94.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.226%.\n",
            "pgd l1: attack effectiveness 25.806%.\n",
            "max: attack effectiveness 27.419354838709676%.\n",
            "Mini batch: 1333/1450 | training time in 31 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0083 | Train accuracy: 93.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1334/1450 | training time in 31 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0013 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0059 | Train accuracy: 94.96\n",
            "pgd linf: attack effectiveness 1.792%.\n",
            "pgd l2: attack effectiveness 3.584%.\n",
            "pgd l1: attack effectiveness 37.814%.\n",
            "max: attack effectiveness 37.81362007168459%.\n",
            "\tVal accuracy 79.47% with accuracy 62.19% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.545%.\n",
            "pgd l1: attack effectiveness 31.818%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1335/1450 | training time in 31 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 93.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1336/1450 | training time in 31 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0055 | Train accuracy: 93.19%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 9.091%.\n",
            "pgd l1: attack effectiveness 27.273%.\n",
            "max: attack effectiveness 28.78787878787879%.\n",
            "Mini batch: 1337/1450 | training time in 31 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0070 | Train accuracy: 91.24%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.429%.\n",
            "pgd l1: attack effectiveness 8.571%.\n",
            "max: attack effectiveness 8.571428571428571%.\n",
            "Mini batch: 1338/1450 | training time in 31 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 96.97%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.817%.\n",
            "pgd l1: attack effectiveness 35.211%.\n",
            "max: attack effectiveness 36.61971830985916%.\n",
            "Mini batch: 1339/1450 | training time in 31 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0092 | Train accuracy: 91.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.947%.\n",
            "pgd l1: attack effectiveness 21.053%.\n",
            "max: attack effectiveness 21.052631578947366%.\n",
            "Mini batch: 1340/1450 | training time in 31 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 93.14%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.754%.\n",
            "max: attack effectiveness 14.754098360655737%.\n",
            "Mini batch: 1341/1450 | training time in 31 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 96.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 13.043%.\n",
            "pgd l1: attack effectiveness 17.391%.\n",
            "max: attack effectiveness 17.391304347826086%.\n",
            "Mini batch: 1342/1450 | training time in 31 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0085 | Train accuracy: 94.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 4.839%.\n",
            "pgd l1: attack effectiveness 12.903%.\n",
            "max: attack effectiveness 12.903225806451612%.\n",
            "Mini batch: 1343/1450 | training time in 31 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 96.32%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.000%.\n",
            "pgd l1: attack effectiveness 15.000%.\n",
            "max: attack effectiveness 15.0%.\n",
            "Mini batch: 1344/1450 | training time in 31 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0150 | Train accuracy: 95.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.852%.\n",
            "pgd l1: attack effectiveness 9.259%.\n",
            "max: attack effectiveness 9.25925925925926%.\n",
            "Mini batch: 1345/1450 | training time in 31 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 97.25%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.286%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 1346/1450 | training time in 31 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 95.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.923%.\n",
            "max: attack effectiveness 16.923076923076923%.\n",
            "Mini batch: 1347/1450 | training time in 31 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 95.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1348/1450 | training time in 31 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0145 | Train accuracy: 93.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 16.923%.\n",
            "max: attack effectiveness 16.923076923076923%.\n",
            "Mini batch: 1349/1450 | training time in 31 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0071 | Train accuracy: 95.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.817%.\n",
            "pgd l1: attack effectiveness 32.394%.\n",
            "max: attack effectiveness 32.3943661971831%.\n",
            "Mini batch: 1350/1450 | training time in 32 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0065 | Train accuracy: 90.95%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 6.250%.\n",
            "pgd l1: attack effectiveness 12.500%.\n",
            "max: attack effectiveness 12.5%.\n",
            "Mini batch: 1351/1450 | training time in 32 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0025 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1352/1450 | training time in 32 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0049 | Train accuracy: 91.28%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 6.250%.\n",
            "pgd l1: attack effectiveness 32.812%.\n",
            "max: attack effectiveness 32.8125%.\n",
            "Mini batch: 1353/1450 | training time in 32 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0084 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 8.108%.\n",
            "pgd l1: attack effectiveness 18.919%.\n",
            "max: attack effectiveness 20.27027027027027%.\n",
            "Mini batch: 1354/1450 | training time in 32 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 96.04%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 5.797%.\n",
            "pgd l1: attack effectiveness 30.435%.\n",
            "max: attack effectiveness 30.434782608695656%.\n",
            "Mini batch: 1355/1450 | training time in 32 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0064 | Train accuracy: 91.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 8.000%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1356/1450 | training time in 32 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 90.15%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 8.824%.\n",
            "pgd l1: attack effectiveness 33.824%.\n",
            "max: attack effectiveness 33.82352941176471%.\n",
            "Mini batch: 1357/1450 | training time in 32 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0095 | Train accuracy: 90.82%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 30.508%.\n",
            "max: attack effectiveness 30.508474576271187%.\n",
            "Mini batch: 1358/1450 | training time in 32 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0164 | Train accuracy: 94.65%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 40.000%.\n",
            "max: attack effectiveness 40.0%.\n",
            "Mini batch: 1359/1450 | training time in 32 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 89.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 14.493%.\n",
            "max: attack effectiveness 14.492753623188406%.\n",
            "Mini batch: 1360/1450 | training time in 32 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0025 | Train accuracy: 95.94%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.062%.\n",
            "max: attack effectiveness 14.0625%.\n",
            "Mini batch: 1361/1450 | training time in 32 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0147 | Train accuracy: 95.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.677%.\n",
            "max: attack effectiveness 9.67741935483871%.\n",
            "Mini batch: 1362/1450 | training time in 32 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0020 | Train accuracy: 97.89%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 1363/1450 | training time in 32 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0000 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0064 | Train accuracy: 94.34\n",
            "pgd linf: attack effectiveness 1.971%.\n",
            "pgd l2: attack effectiveness 3.047%.\n",
            "pgd l1: attack effectiveness 22.401%.\n",
            "max: attack effectiveness 22.401433691756274%.\n",
            "\tVal accuracy 87.3% with accuracy 77.6% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 19.697%.\n",
            "max: attack effectiveness 19.696969696969695%.\n",
            "Mini batch: 1364/1450 | training time in 32 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 95.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.286%.\n",
            "max: attack effectiveness 14.285714285714285%.\n",
            "Mini batch: 1365/1450 | training time in 32 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 96.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 18.181818181818183%.\n",
            "Mini batch: 1366/1450 | training time in 32 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 8.571%.\n",
            "max: attack effectiveness 8.571428571428571%.\n",
            "Mini batch: 1367/1450 | training time in 32 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0016 | Train accuracy: 97.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.268%.\n",
            "max: attack effectiveness 11.267605633802818%.\n",
            "Mini batch: 1368/1450 | training time in 32 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0072 | Train accuracy: 97.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 17.105%.\n",
            "max: attack effectiveness 17.105263157894736%.\n",
            "Mini batch: 1369/1450 | training time in 32 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 94.12%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.590%.\n",
            "max: attack effectiveness 24.59016393442623%.\n",
            "Mini batch: 1370/1450 | training time in 32 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0077 | Train accuracy: 96.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.638%.\n",
            "max: attack effectiveness 24.637681159420293%.\n",
            "Mini batch: 1371/1450 | training time in 32 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0100 | Train accuracy: 93.40%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.194%.\n",
            "max: attack effectiveness 24.193548387096776%.\n",
            "Mini batch: 1372/1450 | training time in 32 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 94.21%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 35.000%.\n",
            "max: attack effectiveness 35.0%.\n",
            "Mini batch: 1373/1450 | training time in 32 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0111 | Train accuracy: 93.09%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.852%.\n",
            "pgd l1: attack effectiveness 24.074%.\n",
            "max: attack effectiveness 24.074074074074073%.\n",
            "Mini batch: 1374/1450 | training time in 32 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 95.60%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 23.810%.\n",
            "max: attack effectiveness 23.809523809523807%.\n",
            "Mini batch: 1375/1450 | training time in 32 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 95.81%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 1376/1450 | training time in 32 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 92.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 18.181818181818183%.\n",
            "Mini batch: 1377/1450 | training time in 32 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0202 | Train accuracy: 94.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 1378/1450 | training time in 32 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0071 | Train accuracy: 91.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.394%.\n",
            "max: attack effectiveness 32.3943661971831%.\n",
            "Mini batch: 1379/1450 | training time in 32 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 93.97%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 20.312%.\n",
            "max: attack effectiveness 20.3125%.\n",
            "Mini batch: 1380/1450 | training time in 32 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 26.866%.\n",
            "max: attack effectiveness 26.865671641791046%.\n",
            "Mini batch: 1381/1450 | training time in 32 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 95.90%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 31.250%.\n",
            "max: attack effectiveness 31.25%.\n",
            "Mini batch: 1382/1450 | training time in 32 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0080 | Train accuracy: 94.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.351%.\n",
            "pgd l1: attack effectiveness 35.135%.\n",
            "max: attack effectiveness 35.13513513513514%.\n",
            "Mini batch: 1383/1450 | training time in 32 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0066 | Train accuracy: 90.59%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.536%.\n",
            "max: attack effectiveness 27.536231884057973%.\n",
            "Mini batch: 1384/1450 | training time in 32 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 93.40%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.000%.\n",
            "max: attack effectiveness 32.0%.\n",
            "Mini batch: 1385/1450 | training time in 32 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 93.10%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 8.824%.\n",
            "pgd l1: attack effectiveness 27.941%.\n",
            "max: attack effectiveness 27.941176470588236%.\n",
            "Mini batch: 1386/1450 | training time in 32 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 90.31%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 15.254%.\n",
            "pgd l1: attack effectiveness 28.814%.\n",
            "max: attack effectiveness 28.8135593220339%.\n",
            "Mini batch: 1387/1450 | training time in 32 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0257 | Train accuracy: 93.05%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 37.333%.\n",
            "max: attack effectiveness 37.333333333333336%.\n",
            "Mini batch: 1388/1450 | training time in 32 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 90.64%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 30.435%.\n",
            "max: attack effectiveness 30.434782608695656%.\n",
            "Mini batch: 1389/1450 | training time in 32 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0065 | Train accuracy: 91.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 20.312%.\n",
            "max: attack effectiveness 20.3125%.\n",
            "Mini batch: 1390/1450 | training time in 32 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0099 | Train accuracy: 94.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.194%.\n",
            "max: attack effectiveness 24.193548387096776%.\n",
            "Mini batch: 1391/1450 | training time in 32 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0029 | Train accuracy: 93.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.000%.\n",
            "max: attack effectiveness 0.0%.\n",
            "Mini batch: 1392/1450 | training time in 32 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0000 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0060 | Train accuracy: 94.37\n",
            "pgd linf: attack effectiveness 1.971%.\n",
            "pgd l2: attack effectiveness 3.405%.\n",
            "pgd l1: attack effectiveness 23.118%.\n",
            "max: attack effectiveness 23.476702508960575%.\n",
            "\tVal accuracy 86.51% with accuracy 76.52% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1393/1450 | training time in 33 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.111%.\n",
            "max: attack effectiveness 11.11111111111111%.\n",
            "Mini batch: 1394/1450 | training time in 33 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 96.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 15.152%.\n",
            "max: attack effectiveness 15.151515151515152%.\n",
            "Mini batch: 1395/1450 | training time in 33 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 95.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 7.143%.\n",
            "max: attack effectiveness 7.142857142857142%.\n",
            "Mini batch: 1396/1450 | training time in 33 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0015 | Train accuracy: 97.47%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 4.225%.\n",
            "max: attack effectiveness 4.225352112676056%.\n",
            "Mini batch: 1397/1450 | training time in 33 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0098 | Train accuracy: 98.49%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.474%.\n",
            "max: attack effectiveness 14.473684210526317%.\n",
            "Mini batch: 1398/1450 | training time in 33 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0025 | Train accuracy: 96.57%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1399/1450 | training time in 33 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0023 | Train accuracy: 98.41%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 13.043%.\n",
            "max: attack effectiveness 13.043478260869565%.\n",
            "Mini batch: 1400/1450 | training time in 33 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 95.43%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.677%.\n",
            "max: attack effectiveness 9.67741935483871%.\n",
            "Mini batch: 1401/1450 | training time in 33 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0023 | Train accuracy: 97.89%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 13.333%.\n",
            "max: attack effectiveness 13.333333333333334%.\n",
            "Mini batch: 1402/1450 | training time in 33 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0066 | Train accuracy: 95.74%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.259%.\n",
            "max: attack effectiveness 9.25925925925926%.\n",
            "Mini batch: 1403/1450 | training time in 33 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0021 | Train accuracy: 97.25%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 11.111%.\n",
            "max: attack effectiveness 11.11111111111111%.\n",
            "Mini batch: 1404/1450 | training time in 33 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 96.34%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 1405/1450 | training time in 33 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0040 | Train accuracy: 93.26%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 21.212%.\n",
            "max: attack effectiveness 21.21212121212121%.\n",
            "Mini batch: 1406/1450 | training time in 33 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0190 | Train accuracy: 94.33%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.538%.\n",
            "pgd l1: attack effectiveness 24.615%.\n",
            "max: attack effectiveness 24.615384615384617%.\n",
            "Mini batch: 1407/1450 | training time in 33 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0076 | Train accuracy: 92.23%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 26.761%.\n",
            "max: attack effectiveness 26.76056338028169%.\n",
            "Mini batch: 1408/1450 | training time in 33 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 91.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 20.312%.\n",
            "max: attack effectiveness 20.3125%.\n",
            "Mini batch: 1409/1450 | training time in 33 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1410/1450 | training time in 33 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0074 | Train accuracy: 93.85%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.125%.\n",
            "max: attack effectiveness 28.125%.\n",
            "Mini batch: 1411/1450 | training time in 33 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0122 | Train accuracy: 92.71%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.351%.\n",
            "pgd l1: attack effectiveness 33.784%.\n",
            "max: attack effectiveness 33.78378378378378%.\n",
            "Mini batch: 1412/1450 | training time in 33 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0084 | Train accuracy: 91.58%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.536%.\n",
            "max: attack effectiveness 27.536231884057973%.\n",
            "Mini batch: 1413/1450 | training time in 33 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0029 | Train accuracy: 95.94%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.333%.\n",
            "max: attack effectiveness 29.333333333333332%.\n",
            "Mini batch: 1414/1450 | training time in 33 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 93.60%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 27.941%.\n",
            "max: attack effectiveness 27.941176470588236%.\n",
            "Mini batch: 1415/1450 | training time in 33 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 92.86%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 30.508%.\n",
            "max: attack effectiveness 30.508474576271187%.\n",
            "Mini batch: 1416/1450 | training time in 33 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0165 | Train accuracy: 93.05%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 34.667%.\n",
            "max: attack effectiveness 34.66666666666667%.\n",
            "Mini batch: 1417/1450 | training time in 33 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 92.61%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1418/1450 | training time in 33 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 90.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 21.875%.\n",
            "max: attack effectiveness 21.875%.\n",
            "Mini batch: 1419/1450 | training time in 33 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0169 | Train accuracy: 94.27%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 30.645%.\n",
            "max: attack effectiveness 30.64516129032258%.\n",
            "Mini batch: 1420/1450 | training time in 33 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 93.16%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1421/1450 | training time in 33 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0014 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0058 | Train accuracy: 94.72\n",
            "pgd linf: attack effectiveness 3.226%.\n",
            "pgd l2: attack effectiveness 5.018%.\n",
            "pgd l1: attack effectiveness 39.427%.\n",
            "max: attack effectiveness 39.42652329749104%.\n",
            "\tVal accuracy 78.79% with accuracy 60.57% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n",
            "pgd linf: attack effectiveness 1.515%.\n",
            "pgd l2: attack effectiveness 3.030%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1422/1450 | training time in 33 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0121 | Train accuracy: 91.24%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1423/1450 | training time in 33 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 93.72%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.515%.\n",
            "pgd l1: attack effectiveness 18.182%.\n",
            "max: attack effectiveness 18.181818181818183%.\n",
            "Mini batch: 1424/1450 | training time in 33 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0037 | Train accuracy: 95.36%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 7.143%.\n",
            "max: attack effectiveness 7.142857142857142%.\n",
            "Mini batch: 1425/1450 | training time in 33 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0118 | Train accuracy: 96.97%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 8.451%.\n",
            "max: attack effectiveness 8.450704225352112%.\n",
            "Mini batch: 1426/1450 | training time in 33 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0117 | Train accuracy: 96.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.316%.\n",
            "pgd l1: attack effectiveness 17.105%.\n",
            "max: attack effectiveness 17.105263157894736%.\n",
            "Mini batch: 1427/1450 | training time in 33 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 95.59%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 9.836%.\n",
            "max: attack effectiveness 9.836065573770492%.\n",
            "Mini batch: 1428/1450 | training time in 33 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 96.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 2.899%.\n",
            "pgd l1: attack effectiveness 23.188%.\n",
            "max: attack effectiveness 23.18840579710145%.\n",
            "Mini batch: 1429/1450 | training time in 33 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0078 | Train accuracy: 94.42%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.194%.\n",
            "max: attack effectiveness 24.193548387096776%.\n",
            "Mini batch: 1430/1450 | training time in 33 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0032 | Train accuracy: 96.84%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.333%.\n",
            "pgd l1: attack effectiveness 36.667%.\n",
            "max: attack effectiveness 36.666666666666664%.\n",
            "Mini batch: 1431/1450 | training time in 33 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0118 | Train accuracy: 90.43%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.852%.\n",
            "pgd l1: attack effectiveness 24.074%.\n",
            "max: attack effectiveness 24.074074074074073%.\n",
            "Mini batch: 1432/1450 | training time in 33 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 93.96%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 23.810%.\n",
            "max: attack effectiveness 23.809523809523807%.\n",
            "Mini batch: 1433/1450 | training time in 33 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 92.67%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.231%.\n",
            "max: attack effectiveness 29.230769230769234%.\n",
            "Mini batch: 1434/1450 | training time in 33 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 94.82%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 25.758%.\n",
            "max: attack effectiveness 25.757575757575758%.\n",
            "Mini batch: 1435/1450 | training time in 33 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0126 | Train accuracy: 93.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 3.077%.\n",
            "pgd l1: attack effectiveness 27.692%.\n",
            "max: attack effectiveness 27.692307692307693%.\n",
            "Mini batch: 1436/1450 | training time in 34 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 94.30%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 29.577%.\n",
            "max: attack effectiveness 29.577464788732392%.\n",
            "Mini batch: 1437/1450 | training time in 34 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 93.97%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 20.312%.\n",
            "max: attack effectiveness 20.3125%.\n",
            "Mini batch: 1438/1450 | training time in 34 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 95.83%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.358%.\n",
            "max: attack effectiveness 28.35820895522388%.\n",
            "Mini batch: 1439/1450 | training time in 34 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 91.79%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 14.062%.\n",
            "max: attack effectiveness 14.0625%.\n",
            "Mini batch: 1440/1450 | training time in 34 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0076 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.351%.\n",
            "pgd l1: attack effectiveness 36.486%.\n",
            "max: attack effectiveness 36.486486486486484%.\n",
            "Mini batch: 1441/1450 | training time in 34 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0121 | Train accuracy: 90.59%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 28.986%.\n",
            "max: attack effectiveness 28.985507246376812%.\n",
            "Mini batch: 1442/1450 | training time in 34 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0034 | Train accuracy: 93.91%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.000%.\n",
            "max: attack effectiveness 32.0%.\n",
            "Mini batch: 1443/1450 | training time in 34 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0042 | Train accuracy: 89.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 32.353%.\n",
            "max: attack effectiveness 32.35294117647059%.\n",
            "Mini batch: 1444/1450 | training time in 34 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0038 | Train accuracy: 91.84%.\n",
            "pgd linf: attack effectiveness 1.695%.\n",
            "pgd l2: attack effectiveness 1.695%.\n",
            "pgd l1: attack effectiveness 32.203%.\n",
            "max: attack effectiveness 32.20338983050847%.\n",
            "Mini batch: 1445/1450 | training time in 34 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0225 | Train accuracy: 91.98%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 37.333%.\n",
            "max: attack effectiveness 37.333333333333336%.\n",
            "Mini batch: 1446/1450 | training time in 34 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 89.16%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.449%.\n",
            "pgd l1: attack effectiveness 33.333%.\n",
            "max: attack effectiveness 33.33333333333333%.\n",
            "Mini batch: 1447/1450 | training time in 34 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0056 | Train accuracy: 90.86%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 1.562%.\n",
            "pgd l1: attack effectiveness 20.312%.\n",
            "max: attack effectiveness 20.3125%.\n",
            "Mini batch: 1448/1450 | training time in 34 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0190 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 24.194%.\n",
            "max: attack effectiveness 24.193548387096776%.\n",
            "Mini batch: 1449/1450 | training time in 34 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 93.68%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 16.667%.\n",
            "max: attack effectiveness 16.666666666666664%.\n",
            "Mini batch: 1450/1450 | training time in 34 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 94.74%.\n",
            "Training loss (epoch level): 0.0070 | Train accuracy: 93.66\n",
            "pgd linf: attack effectiveness 2.330%.\n",
            "pgd l2: attack effectiveness 2.688%.\n",
            "pgd l1: attack effectiveness 38.172%.\n",
            "max: attack effectiveness 38.17204301075269%.\n",
            "\tVal accuracy 79.54% with accuracy 61.83% under attack.\n",
            "\tModel select at epoch 39 with validation accuracy 88.15% and accuracy 79.21% under attack.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5i4V5h61BLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erNp0A5_mW98",
        "outputId": "de53c007-0baf-463c-9d63-c9f74b4a77e0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Max at 0x790930df1780>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6KZfsBWmmr1",
        "outputId": "ee66efc3-7865-4af4-b8ed-5a08ffd2caeb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 6693])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_list = [pgdlinf,pgdl2,pgdl1]\n",
        "varepsilon=1e-20\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    loss, done = get_scores(model, x, label)\n",
        "    print(loss.shape)\n",
        "    print(done.shape)\n",
        "pre_loss = loss\n",
        "n, red_n = x.size()[0], x.size()[1:]\n",
        "print(n)\n",
        "print(red_n)\n",
        "red_ind = list(range(2, len(x.size()) + 1))\n",
        "adv_x = x.detach().clone()\n",
        "stop_flag = torch.zeros(n, dtype=torch.bool)\n",
        "for t in range(5):\n",
        "    num_sample_red = n - torch.sum(stop_flag)\n",
        "    if num_sample_red <= 0:\n",
        "        break\n",
        "\n",
        "    red_label = label[~stop_flag]\n",
        "    pertbx = []\n",
        "    for attack in attack_list:\n",
        "        assert 'perturb' in type(attack).__dict__.keys()\n",
        "        if t > 0 and 'use_random' in attack.__dict__.keys():\n",
        "            attack.use_random = False\n",
        "        if 'Orthogonal' in type(attack).__name__:\n",
        "            pertbx.append(attack.perturb(model=model, x=adv_x[~stop_flag], label=red_label))\n",
        "        else:\n",
        "            pertbx.append(attack.perturb(model=model, x=adv_x[~stop_flag], label=red_label,min_lambda_=1e-5,max_lambda_=1e5))\n",
        "\n",
        "    pertbx = torch.vstack(pertbx)\n",
        "    with torch.no_grad():\n",
        "        red_label_ext = torch.cat([red_label] * len(attack_list))\n",
        "        loss, done = get_scores(model, pertbx, red_label_ext)\n",
        "        loss = loss.reshape(len(attack_list), num_sample_red).permute(1, 0)\n",
        "        done = done.reshape(len(attack_list), num_sample_red).permute(1, 0)\n",
        "        print(done)\n",
        "        success_flag = torch.any(done, dim=-1)\n",
        "        print('success_flag : ',success_flag)\n",
        "        done[~torch.any(done, dim=-1)] = 1\n",
        "        print('done : ',done)\n",
        "        print('loss :',loss.shape)\n",
        "        print(torch.min(loss) * (~done).to(torch.float))\n",
        "        loss = (loss * done.to(torch.float)) + torch.min(loss) * (~done).to(torch.float)\n",
        "        print('loss : ',loss)\n",
        "        pertbx = pertbx.reshape(len(attack_list), num_sample_red, *red_n).permute([1, 0, *red_ind])\n",
        "        print(pertbx.shape)\n",
        "        _, indices = loss.max(dim=-1)\n",
        "        print(indices.shape)\n",
        "        adv_x[~stop_flag] = pertbx[torch.arange(num_sample_red), indices]\n",
        "        a_loss = loss[torch.arange(num_sample_red), indices]\n",
        "        pre_stop_flag = stop_flag.clone()\n",
        "        stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < varepsilon) | success_flag\n",
        "        pre_loss[~pre_stop_flag] = a_loss\n",
        "\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NZZ-fH7i2eO",
        "outputId": "514715ef-726f-4af7-87aa-9ef27a2bc6b0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5])\n",
            "torch.Size([5])\n",
            "5\n",
            "torch.Size([6693])\n",
            "pgd linf: attack effectiveness 100.000%.\n",
            "pgd l2: attack effectiveness 0.000%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "tensor([[ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False]])\n",
            "success_flag :  tensor([True, True, True, True, True])\n",
            "done :  tensor([[ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True, False, False]])\n",
            "loss : torch.Size([5, 3])\n",
            "tensor([[0.0000, 0.6474, 0.6474],\n",
            "        [0.0000, 0.6474, 0.6474],\n",
            "        [0.0000, 0.6474, 0.6474],\n",
            "        [0.0000, 0.6474, 0.6474],\n",
            "        [0.0000, 0.6474, 0.6474]])\n",
            "loss :  tensor([[2.4045, 0.6474, 0.6474],\n",
            "        [2.2561, 0.6474, 0.6474],\n",
            "        [2.4190, 0.6474, 0.6474],\n",
            "        [2.6429, 0.6474, 0.6474],\n",
            "        [2.5728, 0.6474, 0.6474]], dtype=torch.float64)\n",
            "torch.Size([5, 3, 6693])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(success_flag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GFeA5yHtx9T",
        "outputId": "aca837f6-be74-446a-9257-99554e48b7a1"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True, True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scores(model, pertb_x, label):\n",
        "    if hasattr(model, 'is_detector_enabled'):\n",
        "        logits_f, prob_g = model.forward(pertb_x)\n",
        "    else:\n",
        "        logits_f = model.forward(pertb_x)\n",
        "    ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "    y_pred = logits_f.argmax(1)\n",
        "    if hasattr(model, 'is_detector_enabled') and (not oblivion):\n",
        "        tau = model.get_tau_sample_wise(y_pred)\n",
        "        loss_no_reduction = -prob_g\n",
        "        done = (y_pred != label) & (prob_g <= tau)\n",
        "    else:\n",
        "        loss_no_reduction = ce\n",
        "        done = y_pred != label\n",
        "    return loss_no_reduction, done"
      ],
      "metadata": {
        "id": "HsN_g4cAa1vg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    with torch.no_grad():\n",
        "        red_label_ext = torch.cat([red_label] * len(self.attack_list))\n",
        "        loss, done = self.get_scores(model, pertbx, red_label_ext)\n",
        "        loss = loss.reshape(len(self.attack_list), num_sample_red).permute(1, 0)\n",
        "        done = done.reshape(len(self.attack_list), num_sample_red).permute(1, 0)\n",
        "        success_flag = torch.any(done, dim=-1)\n",
        "        # for a sample, if there is at least one successful attack, we will select the one with maximum loss;\n",
        "        # while if no attacks evade the victim successful, all perturbed examples are reminded for selection\n",
        "        done[~torch.any(done, dim=-1)] = 1\n",
        "        loss = (loss * done.to(torch.float)) + torch.min(loss) * (~done).to(torch.float)\n",
        "        pertbx = pertbx.reshape(len(self.attack_list), num_sample_red, *red_n).permute([1, 0, *red_ind])\n",
        "        _, indices = loss.max(dim=-1)\n",
        "        adv_x[~stop_flag] = pertbx[torch.arange(num_sample_red), indices]\n",
        "        a_loss = loss[torch.arange(num_sample_red), indices]\n",
        "        pre_stop_flag = stop_flag.clone()\n",
        "        stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < self.varepsilon) | success_flag\n",
        "        pre_loss[~pre_stop_flag] = a_loss"
      ],
      "metadata": {
        "id": "XL8VfeMPjpJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pgdlinf.__dict__.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tuSIr3rLLJD",
        "outputId": "d62856bc-dcf5-4b61-fb40-cba4e6ac800d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['norm', 'use_random', 'round_threshold', 'is_attacker', 'lambda_', 'omeag', 'manipulation_x', 'api_flag', 'device', 'perturb'])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(pgdlinf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGTSGoAhdqr8",
        "outputId": "33b04a99-e6cf-4034-9b06-79bb8864edb4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.PGD"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(pgdlinf).__name__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NizOQFdNdc3H",
        "outputId": "cd602d65-38f3-497f-d5aa-43b19b14d39a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PGD'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(pgdl1).__name__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rdu47DVMdiK5",
        "outputId": "3d3c79f4-942a-4e3a-87f3-492e6de884e3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PGDl1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "done = torch.tensor(([[1,0,0],[1,1,0],[1,1,1],[0,0,0]]))\n",
        "done.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU_NJiNQgxaA",
        "outputId": "66e90305-8316-4238-e5b6-4390c0924cf5"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.any(done, dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqTJkWjwgxKc",
        "outputId": "4c9d95df-1054-45c7-f974-f58a0cc7da2a"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True,  True,  True, False])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "done[torch.any(done, dim=-1)]=1\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaFyT1Gzhwhf",
        "outputId": "0419c4c9-8d3c-4e89-a998-c4c7e0f37b14"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1],\n",
              "        [1, 1, 1],\n",
              "        [1, 1, 1],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.min(done) * done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0U9zY6PiO3A",
        "outputId": "acd2238f-1772-4baf-b6bb-f7a6b29b0e0e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQYOiHP8LLFj",
        "outputId": "73eed89c-b835-484f-a97b-f4c08fb1d36a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6693])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(2, len(x.size()) + 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2zW5PsRLLAB",
        "outputId": "e6c93e7b-2b8c-4c46-d870-0a6ee9a7d4f4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0DxLH5uEZEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MalwareDetectionDNN(3216,2)\n",
        "#IMPORTANT: model must be double means that its weights must be double for getting float results\n",
        "model = model.double().to(device)\n",
        "#IMPORTANT : we can use pretrain model to check how much pgd attack is effective against normal pretrained model\n",
        "model.load_state_dict(torch.load('/content/malware_detection/MalwareDetectionDNN.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5e2HDkPD-om",
        "outputId": "c3e802d0-99c3-4400-9f5d-f073e9a9001a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MalwareDetectionDNN(\n",
              "  (nn_model_layer_0): Linear(in_features=3216, out_features=200, bias=True)\n",
              "  (nn_model_layer_1): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (nn_model_layer_2): Linear(in_features=200, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS2dGm5kDCRu",
        "outputId": "92f58d6a-f869-4e67-fc52-531e7dcf8205"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6693])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attack.shape)\n",
        "print(torch.sum(x,dim=1))\n",
        "print(torch.sum(attack,dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6jYZnksC4DF",
        "outputId": "de6b5dd0-43d4-4657-c6b3-1392573d947f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6693])\n",
            "tensor([35., 57.])\n",
            "tensor([35., 57.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1PN5wN0ZCmh4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}