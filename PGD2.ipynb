{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyWVcO7hJbgJ3ZBP3xXa/V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/a_malware/blob/main/PGD2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install androguard\n",
        "import androguard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdWcbQsbDh0u",
        "outputId": "1715134f-d066-43b0-ae9a-6cfad4b6ab7b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting androguard\n",
            "  Downloading androguard-3.3.5-py3-none-any.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.4/922.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto>=0.24.0 (from androguard)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from androguard) (8.1.7)\n",
            "Collecting colorama (from androguard)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from androguard) (0.18.3)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from androguard) (7.34.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from androguard) (4.9.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from androguard) (3.7.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from androguard) (3.2)\n",
            "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from androguard) (1.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from androguard) (2.16.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->androguard)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (3.0.39)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->androguard) (4.8.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot>=1.4.1->androguard) (3.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->androguard) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->androguard) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.0.0->androguard) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->androguard) (0.2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->androguard) (1.16.0)\n",
            "Installing collected packages: asn1crypto, jedi, colorama, androguard\n",
            "Successfully installed androguard-3.3.5 asn1crypto-1.5.1 colorama-0.4.6 jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eRx10YoAiFg",
        "outputId": "27a45773-cd95-46b2-f35f-3df8924d46f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Cloning into 'malware_detection'...\n",
            "remote: Enumerating objects: 16013, done.\u001b[K\n",
            "remote: Counting objects: 100% (2165/2165), done.\u001b[K\n",
            "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
            "remote: Total 16013 (delta 2008), reused 2151 (delta 1998), pack-reused 13848\u001b[K\n",
            "Receiving objects: 100% (16013/16013), 207.57 MiB | 26.42 MiB/s, done.\n",
            "Resolving deltas: 100% (12938/12938), done.\n",
            "Updating files: 100% (144/144), done.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os.path as path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "!git clone https://github.com/mostafa-ja/malware_detection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = [(torch.tensor(data, dtype=torch.float32), label) for data, label in samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.samples[index]"
      ],
      "metadata": {
        "id": "a4E-Kgs2AoJG"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MalwareDetectionDNN(nn.Module):\n",
        "    def __init__(self, input_size, n_classes, device='cpu', name='DNN', dense_hidden_units=[200, 200], dropout=0.6, smooth=False):\n",
        "        \"\"\"\n",
        "        Construct malware detector\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        @param input_size: Integer, the dimentionality number of input vector\n",
        "        @param n_classes: Integer, the number of classes, n=2\n",
        "        @param device: String, 'cpu' or 'cuda'\n",
        "        @param name: String, model name\n",
        "        \"\"\"\n",
        "        super(MalwareDetectionDNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.n_classes = n_classes\n",
        "        self.name = name\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.smooth = smooth\n",
        "        self.dense_hidden_units = dense_hidden_units\n",
        "\n",
        "        self.dense_layers = []\n",
        "        if len(self.dense_hidden_units) >= 1:\n",
        "            self.dense_layers.append(nn.Linear(self.input_size, self.dense_hidden_units[0]))\n",
        "        else:\n",
        "            raise ValueError(\"Expect at least one hidden layer.\")\n",
        "        for i in range(len(self.dense_hidden_units[0:-1])):\n",
        "            self.dense_layers.append(nn.Linear(self.dense_hidden_units[i],  # start from idx=1\n",
        "                                               self.dense_hidden_units[i + 1]))\n",
        "        self.dense_layers.append(nn.Linear(self.dense_hidden_units[-1], self.n_classes))\n",
        "\n",
        "        # registration\n",
        "        for idx_i, dense_layer in enumerate(self.dense_layers):\n",
        "            self.add_module('nn_model_layer_{}'.format(idx_i), dense_layer)\n",
        "\n",
        "        if self.smooth:\n",
        "            self.activation_func = F.selu  # partial(F.elu, alpha=self.alpha_)\n",
        "        else:\n",
        "            self.activation_func = F.relu\n",
        "\n",
        "        self.model_save_path = path.join('/content/malware_detection/save/drebin/md_dnn'+ '_' + self.name,'model.pth')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Go through the neural network\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        @param x: 2D tensor, feature representation\n",
        "        \"\"\"\n",
        "        for dense_layer in self.dense_layers[:-1]:\n",
        "            x = self.activation_func(dense_layer(x))\n",
        "\n",
        "        latent_representation = F.dropout(x, self.dropout, training=self.training)\n",
        "        logits = self.dense_layers[-1](latent_representation)\n",
        "        return logits\n",
        "\n",
        "    def customize_loss(self, logits, gt_labels, representation = None, mini_batch_idx=None):\n",
        "        return F.cross_entropy(logits, gt_labels)\n",
        "\n",
        "    # for predict pertubed malicious and create a y_true same size for theam\n",
        "    def inference_batch_wise(self, x):\n",
        "        \"\"\"\n",
        "        support malware samples solely\n",
        "        \"\"\"\n",
        "        assert isinstance(x, torch.Tensor)\n",
        "        logit = self.forward(x)\n",
        "        return torch.softmax(logit, dim=-1).detach().cpu().numpy(), np.ones((logit.size()[0],))\n",
        "\n",
        "    def inference(self, test_data_producer):\n",
        "        confidences = []\n",
        "        gt_labels = []\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_data_producer: # for large dataset we have to consider test dataset with batches\n",
        "                x, y = x.double().to(device), y.long().to(device)\n",
        "                logits = self.forward(x)\n",
        "                confidences.append(F.softmax(logits, dim=-1))\n",
        "                gt_labels.append(y)\n",
        "        confidences = torch.vstack(confidences) #[[1,2,3],[4,5,6]] > below each other\n",
        "        gt_labels = torch.cat(gt_labels, dim=0) #[[1,2,3],[4,5,6]] > [1,2,3,4,5,6]\n",
        "        return confidences, gt_labels\n",
        "\n",
        "\n",
        "    def predict(self, test_data_producer, indicator_masking=True):\n",
        "        \"\"\"\n",
        "        predict labels and conduct evaluation\n",
        "\n",
        "        Parameters\n",
        "        --------\n",
        "        @param test_data_producer, torch.DataLoader\n",
        "        \"\"\"\n",
        "        # evaluation\n",
        "        confidence, y_true = self.inference(test_data_producer)\n",
        "        y_pred = confidence.argmax(1).cpu().numpy()\n",
        "        y_true = y_true.cpu().numpy()\n",
        "        from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        b_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "        print(\"The accuracy on the test dataset is {:.5f}%\".format(accuracy * 100))\n",
        "        print(\"The balanced accuracy on the test dataset is {:.5f}%\".format(b_accuracy * 100))\n",
        "\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        fpr = fp / float(tn + fp)\n",
        "        fnr = fn / float(tp + fn)\n",
        "        f1 = f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "        print(\"Other evaluation metrics we may need:\")\n",
        "        print(\"False Negative Rate (FNR) is {:.5f}%, False Positive Rate (FPR) is {:.5f}%, F1 score is {:.5f}%\".format(fnr * 100, fpr * 100, f1 * 100))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "IBNqn_OEAuV-"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/malware_detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRRx7oWaDnGY",
        "outputId": "f7936443-c69c-42b9-bce6-078591df8831"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/malware_detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projected gradient descent (ascent)."
      ],
      "metadata": {
        "id": "-45R7q66ygAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from core.droidfeature import InverseDroidFeature\n",
        "\n",
        "inverse_feature = InverseDroidFeature()\n",
        "\n",
        "# manipulation_x : removable features\n",
        "manipulation_x = inverse_feature.get_manipulation()\n",
        "manipulation_x = torch.LongTensor(manipulation_x).to(device)\n",
        "\n",
        "# index of vocab which are interdependent\n",
        "omega = inverse_feature.get_interdependent_apis()\n",
        "omega = torch.sum(F.one_hot(torch.tensor(omega), num_classes=len(inverse_feature.vocab)),dim=0).to(device)\n",
        "\n",
        "# a list of vocab length : if the type of feature is api , index of that feature will be true\n",
        "api_flag = inverse_feature.get_api_flag()"
      ],
      "metadata": {
        "id": "sY7SRwnGYixM"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_x0(x, rounding_threshold=0.5, is_sample=False):\n",
        "    \"\"\"\n",
        "    Helper function to randomly initialize the the inner maximizer algos\n",
        "    randomize such that the functionality is preserved.\n",
        "    :param x: training sample\n",
        "    :param is_sample: flag to sample randomly from feasible area or return just x\n",
        "    :return: randomly sampled feasible version of x\n",
        "    \"\"\"\n",
        "    if is_sample:\n",
        "        rand_x = round_x(torch.rand(x.size()), alpha=rounding_threshold)\n",
        "        if x.is_cuda:\n",
        "            rand_x = rand_x.cuda()\n",
        "        return (rand_x.byte() | x.byte()).double()\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "\n",
        "def round_x(x, alpha=0.5):\n",
        "    \"\"\"\n",
        "    rounds x by thresholding it according to alpha which can be a scalar or vector\n",
        "    :param x:\n",
        "    :param alpha: threshold parameter\n",
        "    :return: a double tensor of 0s and 1s.\n",
        "    \"\"\"\n",
        "    return (x >= alpha).double()"
      ],
      "metadata": {
        "id": "Ye0d01eYuWZv"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PGD():\n",
        "    \"\"\"\n",
        "    Projected gradient descent (ascent).\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    @param norm, 'l2' or 'linf'\n",
        "    @param use_random, Boolean,  whether use random start point\n",
        "    @param rounding_threshold, float, a threshold for rounding real scalars\n",
        "    @param is_attacker, Boolean, play the role of attacker (note: the defender conducts adversarial training)\n",
        "    @param oblivion, Boolean, whether know the adversary indicator or not\n",
        "    @param kappa, attack confidence\n",
        "    @param manipulation_x, manipulations\n",
        "    @param omega, the indices of interdependent apis corresponding to each api\n",
        "    @param device, 'cpu' or 'cuda'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, norm, use_random=False, rounding_threshold=0.5,\n",
        "                 is_attacker=True, manipulation_x=None, omega=None,api_flag=None, device=None):\n",
        "        super(PGD, self).__init__()\n",
        "        assert norm == 'l1' or norm == 'l2' or norm == 'linf', \"Expect 'l1', 'l2' or 'linf'.\"\n",
        "        self.norm = norm\n",
        "        self.use_random = use_random\n",
        "        self.round_threshold = rounding_threshold\n",
        "        self.is_attacker = is_attacker\n",
        "        self.lambda_ = 1.\n",
        "        self.omeag = omega\n",
        "        self.manipulation_x = manipulation_x\n",
        "        self.api_flag = api_flag\n",
        "        self.device = device\n",
        "    def _perturb(self, model, x, label=None,\n",
        "                 steps=10,\n",
        "                 step_length=1.,\n",
        "                 lambda_=1.,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        perturb node feature vectors\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        @param model, a victim model\n",
        "        @param x: torch.FloatTensor, node feature vectors (each represents the occurrences of apis in a graph) with shape [batch_size, vocab_dim]\n",
        "        @param label: torch.LongTensor, ground truth labels\n",
        "        @param steps: Integer, maximum number of iterations\n",
        "        @param step_length: float, the step length in each iteration\n",
        "        @param lambda_, float, penalty factor\n",
        "        \"\"\"\n",
        "        adv_x = x\n",
        "        self.lambda_ = lambda_\n",
        "        model.eval()\n",
        "        loss_natural = 0.\n",
        "        for t in range(steps):\n",
        "            if t == 0 and self.use_random:\n",
        "                adv_x = get_x0(adv_x, rounding_threshold=self.round_threshold, is_sample=True)\n",
        "            var_adv_x = torch.autograd.Variable(adv_x, requires_grad=True)\n",
        "            loss, done = self.get_loss(model, var_adv_x, label, self.lambda_)\n",
        "            if t == 0:\n",
        "                loss_natural = loss\n",
        "            grad = torch.autograd.grad(torch.mean(loss), var_adv_x)[0].detach().data\n",
        "            perturbation = self.get_perturbation(grad, x, adv_x)\n",
        "            adv_x = torch.clamp(adv_x + perturbation * step_length, min=0., max=1.)\n",
        "        # round\n",
        "        if self.norm == 'linf' and (not hasattr(model, 'is_detector_enabled')):\n",
        "            round_threshold = torch.rand(x.size()).to(self.device)\n",
        "        else:\n",
        "            round_threshold = self.round_threshold\n",
        "        adv_x = round_x(adv_x, round_threshold)\n",
        "        loss_adv, _1 = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "        replace_flag = (loss_adv < loss_natural).unsqueeze(1).expand_as(adv_x)\n",
        "        adv_x[replace_flag] = x[replace_flag]\n",
        "        return adv_x\n",
        "\n",
        "    def perturb(self, model, x, label=None,\n",
        "                steps=10,\n",
        "                step_length=1.,\n",
        "                min_lambda_=1e-5,\n",
        "                max_lambda_=1e5,\n",
        "                base=10.,\n",
        "                verbose=False):\n",
        "        \"\"\"\n",
        "        enhance attack\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            self.lambda_ = min_lambda_\n",
        "        else:\n",
        "            self.lambda_ = max_lambda_\n",
        "\n",
        "        adv_x = x.detach().clone().to(torch.double)\n",
        "        while self.lambda_ <= max_lambda_:\n",
        "            with torch.no_grad():\n",
        "                _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            pert_x = self._perturb(model, adv_x[~done], label[~done],\n",
        "                                   steps,\n",
        "                                   step_length,\n",
        "                                   lambda_=self.lambda_\n",
        "                                   )\n",
        "            adv_x[~done] = pert_x\n",
        "            self.lambda_ *= base\n",
        "        with torch.no_grad():\n",
        "            _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            print(f\"pgd {self.norm}: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "        return adv_x\n",
        "\n",
        "    def get_perturbation(self, gradients, features, adv_features):\n",
        "        # look for allowable position, because only '1--> -' and '0 --> +' are permitted\n",
        "        # api insertion\n",
        "        pos_insertion = (adv_features <= 0.5) * 1 * (adv_features >= 0.)\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "        # grad4insertion = (gradients > 0) * gradients\n",
        "        # api removal\n",
        "        pos_removal = (adv_features > 0.5) * 1\n",
        "        grad4removal = (gradients < 0) * (pos_removal & self.manipulation_x) * gradients\n",
        "        if self.is_attacker:\n",
        "            # cope with the interdependent apis\n",
        "            checking_nonexist_api = (pos_removal ^ self.omega) & self.omega\n",
        "            grad4removal[:, self.api_flag] += torch.sum(gradients * checking_nonexist_api, dim=-1, keepdim=True)\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # norm\n",
        "        if self.norm == 'linf':\n",
        "            perturbation = torch.sign(gradients)\n",
        "        elif self.norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = torch.minimum(\n",
        "                torch.tensor(1., dtype=features.dtype, device=features.device),\n",
        "                gradients / l2norm\n",
        "            )\n",
        "            perturbation = torch.where(torch.isnan(perturbation), 0., perturbation)\n",
        "            perturbation = torch.where(torch.isinf(perturbation), 1., perturbation)\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # add the extra perturbation owing to the interdependent apis\n",
        "        if self.norm == 'linf' and self.is_attacker:\n",
        "            perturbation += torch.any(perturbation[:, self.api_flag] < 0, dim=-1,\n",
        "                                      keepdim=True) * checking_nonexist_api\n",
        "        if self.norm == 'l2' and self.is_attacker:\n",
        "            min_val = torch.amin(perturbation, dim=-1, keepdim=True).clamp_(max=0.)\n",
        "            perturbation += (torch.any(perturbation[:, self.api_flag] < 0, dim=-1,\n",
        "                                       keepdim=True) * torch.abs(min_val) * checking_nonexist_api)\n",
        "        return perturbation\n",
        "\n",
        "    def get_loss(self, model, adv_x, label, lambda_=None):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(adv_x)\n",
        "        else:\n",
        "            logits_f = model.forward(adv_x)\n",
        "\n",
        "        ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            assert lambda_ is not None\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            if self.is_attacker:\n",
        "                loss_no_reduction = ce + lambda_ * (torch.clamp(tau - prob_g,\n",
        "                                                                max=self.kappa)\n",
        "                                                    )\n",
        "            else:\n",
        "                loss_no_reduction = ce + lambda_ * (tau - prob_g)\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            loss_no_reduction = ce\n",
        "            done = y_pred != label\n",
        "        return loss_no_reduction, done"
      ],
      "metadata": {
        "id": "Wd8Q6mNduaya"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PGDAdvTraining(object):\n",
        "    \"\"\"adversarial training incorporating pgd attack\n",
        "\n",
        "    Parameters\n",
        "    -------\n",
        "    @param model, Object,  a model to be protected, e.g., MalwareDetector\n",
        "    @attack_model: Object, adversary's model for generating adversarial malware on the feature space\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, attack=None, attack_param=None):\n",
        "        self.model = model\n",
        "        self.attack = attack\n",
        "        self.attack_param = attack_param\n",
        "\n",
        "        self.name = self.model.name\n",
        "        self.model_save_path =  path.join('/content/malware_detection/save/md_at_pgd' + '_' + 'train','model.pth')\n",
        "\n",
        "    def fit(self, train_data_producer, validation_data_producer=None, adv_epochs=45,beta=0.001,lr=0.005,weight_decay=0, verbose=True):\n",
        "        \"\"\"\n",
        "        Applying adversarial train to enhance the malware detector.\n",
        "\n",
        "        Parameters\n",
        "        -------\n",
        "        @param train_data_producer: Object, an dataloader object for producing a batch of training data\n",
        "        @param validation_data_producer: Object, an dataloader object for producing validation dataset\n",
        "        @param epochs: Integer, epochs for adversarial training\n",
        "        @param adv_epochs: Integer, epochs for adversarial training\n",
        "        @param beta: Float, penalty factor for adversarial loss\n",
        "        @param lr: Float, learning rate of Adam optimizer\n",
        "        @param weight_decay: Float, penalty factor, default value 5e-4\n",
        "        @param verbose: Boolean, whether to show verbose info\n",
        "        \"\"\"\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        total_time = 0.\n",
        "        nbatches = len(train_data_producer)\n",
        "        best_acc_val = 0.\n",
        "        acc_val_adv_be = 0.\n",
        "        best_epoch = 0\n",
        "        for i in range(adv_epochs):\n",
        "            losses, accuracies = [], []\n",
        "            for idx_batch, (x_batch, y_batch) in enumerate(train_data_producer):\n",
        "                x_batch, y_batch = x_batch.double().to(device), y_batch.long().to(device)\n",
        "                batch_size = x_batch.shape[0]\n",
        "                # make data\n",
        "                mal_x_batch, ben_x_batch, mal_y_batch, ben_y_batch = x_batch[y_batch == 1], x_batch[y_batch == 0], y_batch[y_batch == 1], y_batch[y_batch == 0]\n",
        "                start_time = time.time()\n",
        "                self.model.eval()\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "                total_time += time.time() - start_time\n",
        "                x_batch = torch.cat([ben_x_batch, pertb_mal_x], dim=0)\n",
        "                y_batch = torch.cat([ben_y_batch, mal_y_batch])\n",
        "                start_time = time.time()\n",
        "                self.model.train()\n",
        "                optimizer.zero_grad()\n",
        "                logits = self.model.forward(x_batch)\n",
        "                loss_train = self.model.customize_loss(logits,y_batch)\n",
        "\n",
        "                loss_train.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_time += time.time() - start_time\n",
        "                mins, secs = int(total_time / 60), int(total_time % 60)\n",
        "                acc_train = (logits.argmax(1) == y_batch).sum().item()\n",
        "                acc_train /= x_batch.size()[0]\n",
        "                accuracies.append(acc_train)\n",
        "                losses.append(loss_train.item())\n",
        "                if verbose:\n",
        "                    print(f'Mini batch: {i * nbatches + idx_batch + 1}/{adv_epochs * nbatches} | training time in {mins:.0f} minutes, {secs} seconds.')\n",
        "                    print(f'Training loss (batch level): {losses[-1]:.4f} | Train accuracy: {acc_train * 100:.2f}%.')\n",
        "            if verbose:\n",
        "                print(f'Training loss (epoch level): {np.mean(losses):.4f} | Train accuracy: {np.mean(accuracies) * 100:.2f}')\n",
        "\n",
        "            # select model\n",
        "            self.model.eval()\n",
        "            # long-time to train (save the model temporally in case of interruption)\n",
        "            self.save_to_disk(i + 1, optimizer, self.model_save_path + '.tmp')\n",
        "\n",
        "            res_val = []\n",
        "            avg_acc_val = []\n",
        "            for x_val, y_val in validation_data_producer:\n",
        "                x_val, y_val = x_val.double().to(device), y_val.long().to(device)\n",
        "                logits = self.model.forward(x_val)\n",
        "                acc_val = (logits.argmax(1) == y_val).sum().item()\n",
        "                acc_val /= x_val.size()[0]\n",
        "                avg_acc_val.append(acc_val)\n",
        "\n",
        "                mal_x_batch, mal_y_batch = x_val[y_val == 1], y_val[y_val == 1]\n",
        "                pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "                y_cent_batch, x_density_batch = self.model.inference_batch_wise(pertb_mal_x)\n",
        "                y_pred = np.argmax(y_cent_batch, axis=-1)\n",
        "                res_val.append(y_pred == 1.)\n",
        "            assert len(res_val) > 0\n",
        "            res_val = np.concatenate(res_val)\n",
        "            acc_val_adv = np.sum(res_val).astype(float) / res_val.shape[0]\n",
        "            acc_val = (np.mean(avg_acc_val) + acc_val_adv) / 2.\n",
        "            # Owing to we look for a new threshold after each epoch, this hinders the convergence of training.\n",
        "            # We save the model's parameters at last several epochs as a well-trained model may be obtained.\n",
        "            if acc_val >= best_acc_val:\n",
        "                best_acc_val = acc_val\n",
        "                acc_val_adv_be = acc_val_adv\n",
        "                best_epoch = i + 1\n",
        "                self.save_to_disk(best_epoch, optimizer, self.model_save_path)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\tVal accuracy {acc_val * 100:.4}% with accuracy {acc_val_adv * 100:.4}% under attack.\")\n",
        "                print(f\"\\tModel select at epoch {best_epoch} with validation accuracy {best_acc_val * 100:.4}% and accuracy {acc_val_adv_be * 100:.4}% under attack.\")\n",
        "                if hasattr(self.model, 'tau'):\n",
        "                    print(f'The threshold is {self.model.tau}.')\n",
        "\n",
        "    #we define here not in model definition, beacause we need attack.pertube()\n",
        "    #also we must not use \"with no grad\" because we need grad for pertubation\n",
        "    def adv_predict(self, test_data_producer):\n",
        "      res_test = []\n",
        "      for x, y in test_data_producer:\n",
        "          x, y = x.double().to(device), y.long().to(device)\n",
        "\n",
        "          mal_x_batch, mal_y_batch = x[y == 1], y[y == 1]\n",
        "          pertb_mal_x = self.attack.perturb(self.model, mal_x_batch, mal_y_batch,**self.attack_param)\n",
        "          y_cent_batch, x_density_batch = self.model.inference_batch_wise(pertb_mal_x)\n",
        "          y_pred = np.argmax(y_cent_batch, axis=-1)\n",
        "          res_test.append(y_pred == 1.)\n",
        "      assert len(res_test) > 0\n",
        "      res_test = np.concatenate(res_test)\n",
        "      acc_val_adv = np.sum(res_test).astype(float) / res_test.shape[0]\n",
        "      print(f\"\\tadversarial accuracy  {acc_val_adv * 100:.4}% under attack.\")\n",
        "\n",
        "\n",
        "    def load(self):\n",
        "        assert path.exists(self.model_save_path), 'train model first'\n",
        "        ckpt = torch.load(self.model_save_path)\n",
        "        self.model.load_state_dict(ckpt['model'])\n",
        "\n",
        "    def save_to_disk(self, epoch, optimizer, save_path=None):\n",
        "        if not path.exists(save_path):\n",
        "            os.makedirs(path.dirname(save_path), exist_ok = True)\n",
        "        torch.save({'model': self.model.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'optimizer_state_dict': optimizer.state_dict()\n",
        "                    },\n",
        "                   save_path)"
      ],
      "metadata": {
        "id": "4eqeD6NWvqhF"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying adversarial train to enhance the malware detector."
      ],
      "metadata": {
        "id": "He1Ak73MyYtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.load('/content/malware_detection/datasets/train_dataset.pt')\n",
        "train_Loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "validation_dataset = torch.load('/content/malware_detection/datasets/validation_dataset.pt')\n",
        "validation_Loader = data.DataLoader(validation_dataset, batch_size=10000, shuffle=False)\n",
        "test_dataset = torch.load('/content/malware_detection/datasets/test_dataset.pt')\n",
        "test_Loader = data.DataLoader(test_dataset, batch_size=10000, shuffle=False)"
      ],
      "metadata": {
        "id": "MRAX7j_qywXf"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# norm = 'linf'\n",
        "\n",
        "model = MalwareDetectionDNN(3216,2)\n",
        "model = model.double().to(device)\n",
        "attack = PGD(norm='linf', use_random=False,rounding_threshold=0.5,is_attacker=False,manipulation_x=manipulation_x, omega=omega,api_flag=api_flag,device=device)\n",
        "attack_param = {}\n",
        "max_adv_training_model = PGDAdvTraining(model, attack, attack_param)\n",
        "max_adv_training_model.fit(train_Loader,validation_Loader,adv_epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41DiMLnjfGBG",
        "outputId": "5c0f4f44-0eaa-4d05-d153-4705c55260d8"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd linf: attack effectiveness 100.000%.\n",
            "Mini batch: 1/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 1.1899 | Train accuracy: 31.25%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 2/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.7276 | Train accuracy: 50.00%.\n",
            "pgd linf: attack effectiveness 35.821%.\n",
            "Mini batch: 3/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.6929 | Train accuracy: 52.34%.\n",
            "pgd linf: attack effectiveness 58.824%.\n",
            "Mini batch: 4/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.6744 | Train accuracy: 53.91%.\n",
            "pgd linf: attack effectiveness 45.161%.\n",
            "Mini batch: 5/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.7139 | Train accuracy: 46.88%.\n",
            "pgd linf: attack effectiveness 43.636%.\n",
            "Mini batch: 6/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.6584 | Train accuracy: 54.69%.\n",
            "pgd linf: attack effectiveness 48.485%.\n",
            "Mini batch: 7/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.6235 | Train accuracy: 67.19%.\n",
            "pgd linf: attack effectiveness 45.902%.\n",
            "Mini batch: 8/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.6682 | Train accuracy: 66.41%.\n",
            "pgd linf: attack effectiveness 48.529%.\n",
            "Mini batch: 9/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.5906 | Train accuracy: 67.19%.\n",
            "pgd linf: attack effectiveness 52.381%.\n",
            "Mini batch: 10/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.5937 | Train accuracy: 70.21%.\n",
            "Training loss (epoch level): 0.7133 | Train accuracy: 56.01\n",
            "pgd linf: attack effectiveness 48.259%.\n",
            "\tVal accuracy 63.25% with accuracy 51.74% under attack.\n",
            "\tModel select at epoch 1 with validation accuracy 63.25% and accuracy 51.74% under attack.\n",
            "pgd linf: attack effectiveness 43.077%.\n",
            "Mini batch: 11/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.5463 | Train accuracy: 76.56%.\n",
            "pgd linf: attack effectiveness 45.312%.\n",
            "Mini batch: 12/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.5747 | Train accuracy: 72.66%.\n",
            "pgd linf: attack effectiveness 47.761%.\n",
            "Mini batch: 13/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.6112 | Train accuracy: 75.78%.\n",
            "pgd linf: attack effectiveness 48.529%.\n",
            "Mini batch: 14/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.6187 | Train accuracy: 66.41%.\n",
            "pgd linf: attack effectiveness 53.226%.\n",
            "Mini batch: 15/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.6971 | Train accuracy: 57.03%.\n",
            "pgd linf: attack effectiveness 43.636%.\n",
            "Mini batch: 16/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.6597 | Train accuracy: 59.38%.\n",
            "pgd linf: attack effectiveness 46.970%.\n",
            "Mini batch: 17/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.5898 | Train accuracy: 67.19%.\n",
            "pgd linf: attack effectiveness 45.902%.\n",
            "Mini batch: 18/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.6054 | Train accuracy: 74.22%.\n",
            "pgd linf: attack effectiveness 48.529%.\n",
            "Mini batch: 19/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.5362 | Train accuracy: 73.44%.\n",
            "pgd linf: attack effectiveness 57.143%.\n",
            "Mini batch: 20/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.5473 | Train accuracy: 76.60%.\n",
            "Training loss (epoch level): 0.5986 | Train accuracy: 69.93\n",
            "pgd linf: attack effectiveness 50.249%.\n",
            "\tVal accuracy 68.0% with accuracy 49.75% under attack.\n",
            "\tModel select at epoch 2 with validation accuracy 68.0% and accuracy 49.75% under attack.\n",
            "pgd linf: attack effectiveness 49.231%.\n",
            "Mini batch: 21/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.5535 | Train accuracy: 68.75%.\n",
            "pgd linf: attack effectiveness 28.125%.\n",
            "Mini batch: 22/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.5293 | Train accuracy: 76.56%.\n",
            "pgd linf: attack effectiveness 13.433%.\n",
            "Mini batch: 23/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.4935 | Train accuracy: 83.59%.\n",
            "pgd linf: attack effectiveness 20.588%.\n",
            "Mini batch: 24/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.4865 | Train accuracy: 81.25%.\n",
            "pgd linf: attack effectiveness 20.968%.\n",
            "Mini batch: 25/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.4654 | Train accuracy: 82.03%.\n",
            "pgd linf: attack effectiveness 25.455%.\n",
            "Mini batch: 26/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.4359 | Train accuracy: 82.81%.\n",
            "pgd linf: attack effectiveness 25.758%.\n",
            "Mini batch: 27/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.3781 | Train accuracy: 83.59%.\n",
            "pgd linf: attack effectiveness 27.869%.\n",
            "Mini batch: 28/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.4787 | Train accuracy: 81.25%.\n",
            "pgd linf: attack effectiveness 26.471%.\n",
            "Mini batch: 29/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.3700 | Train accuracy: 85.16%.\n",
            "pgd linf: attack effectiveness 23.810%.\n",
            "Mini batch: 30/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.3977 | Train accuracy: 80.85%.\n",
            "Training loss (epoch level): 0.4589 | Train accuracy: 80.59\n",
            "pgd linf: attack effectiveness 14.428%.\n",
            "\tVal accuracy 86.54% with accuracy 85.57% under attack.\n",
            "\tModel select at epoch 3 with validation accuracy 86.54% and accuracy 85.57% under attack.\n",
            "pgd linf: attack effectiveness 9.231%.\n",
            "Mini batch: 31/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.3593 | Train accuracy: 82.81%.\n",
            "pgd linf: attack effectiveness 15.625%.\n",
            "Mini batch: 32/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.3905 | Train accuracy: 82.03%.\n",
            "pgd linf: attack effectiveness 13.433%.\n",
            "Mini batch: 33/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.3225 | Train accuracy: 89.06%.\n",
            "pgd linf: attack effectiveness 19.118%.\n",
            "Mini batch: 34/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.3312 | Train accuracy: 86.72%.\n",
            "pgd linf: attack effectiveness 16.129%.\n",
            "Mini batch: 35/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.3302 | Train accuracy: 87.50%.\n",
            "pgd linf: attack effectiveness 23.636%.\n",
            "Mini batch: 36/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.3714 | Train accuracy: 86.72%.\n",
            "pgd linf: attack effectiveness 21.212%.\n",
            "Mini batch: 37/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.3013 | Train accuracy: 85.94%.\n",
            "pgd linf: attack effectiveness 16.393%.\n",
            "Mini batch: 38/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.4103 | Train accuracy: 82.81%.\n",
            "pgd linf: attack effectiveness 19.118%.\n",
            "Mini batch: 39/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.3555 | Train accuracy: 87.50%.\n",
            "pgd linf: attack effectiveness 14.286%.\n",
            "Mini batch: 40/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.3314 | Train accuracy: 85.11%.\n",
            "Training loss (epoch level): 0.3504 | Train accuracy: 85.62\n",
            "pgd linf: attack effectiveness 13.930%.\n",
            "\tVal accuracy 86.78% with accuracy 86.07% under attack.\n",
            "\tModel select at epoch 4 with validation accuracy 86.78% and accuracy 86.07% under attack.\n",
            "pgd linf: attack effectiveness 6.154%.\n",
            "Mini batch: 41/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.2531 | Train accuracy: 89.06%.\n",
            "pgd linf: attack effectiveness 12.500%.\n",
            "Mini batch: 42/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.2889 | Train accuracy: 85.94%.\n",
            "pgd linf: attack effectiveness 11.940%.\n",
            "Mini batch: 43/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.2356 | Train accuracy: 91.41%.\n",
            "pgd linf: attack effectiveness 13.235%.\n",
            "Mini batch: 44/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.2236 | Train accuracy: 89.84%.\n",
            "pgd linf: attack effectiveness 14.516%.\n",
            "Mini batch: 45/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.2739 | Train accuracy: 92.19%.\n",
            "pgd linf: attack effectiveness 20.000%.\n",
            "Mini batch: 46/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.2686 | Train accuracy: 91.41%.\n",
            "pgd linf: attack effectiveness 21.212%.\n",
            "Mini batch: 47/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.2545 | Train accuracy: 87.50%.\n",
            "pgd linf: attack effectiveness 18.033%.\n",
            "Mini batch: 48/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.3408 | Train accuracy: 88.28%.\n",
            "pgd linf: attack effectiveness 17.647%.\n",
            "Mini batch: 49/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.3470 | Train accuracy: 86.72%.\n",
            "pgd linf: attack effectiveness 14.286%.\n",
            "Mini batch: 50/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.3500 | Train accuracy: 87.23%.\n",
            "Training loss (epoch level): 0.2836 | Train accuracy: 88.96\n",
            "pgd linf: attack effectiveness 5.473%.\n",
            "\tVal accuracy 90.89% with accuracy 94.53% under attack.\n",
            "\tModel select at epoch 5 with validation accuracy 90.89% and accuracy 94.53% under attack.\n",
            "pgd linf: attack effectiveness 3.077%.\n",
            "Mini batch: 51/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.3357 | Train accuracy: 87.50%.\n",
            "pgd linf: attack effectiveness 7.812%.\n",
            "Mini batch: 52/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.3399 | Train accuracy: 84.38%.\n",
            "pgd linf: attack effectiveness 5.970%.\n",
            "Mini batch: 53/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.2256 | Train accuracy: 92.97%.\n",
            "pgd linf: attack effectiveness 11.765%.\n",
            "Mini batch: 54/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.2312 | Train accuracy: 91.41%.\n",
            "pgd linf: attack effectiveness 9.677%.\n",
            "Mini batch: 55/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.1813 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 25.455%.\n",
            "Mini batch: 56/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.2714 | Train accuracy: 87.50%.\n",
            "pgd linf: attack effectiveness 19.697%.\n",
            "Mini batch: 57/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.2041 | Train accuracy: 88.28%.\n",
            "pgd linf: attack effectiveness 18.033%.\n",
            "Mini batch: 58/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.3181 | Train accuracy: 90.62%.\n",
            "pgd linf: attack effectiveness 19.118%.\n",
            "Mini batch: 59/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.2683 | Train accuracy: 89.06%.\n",
            "pgd linf: attack effectiveness 14.286%.\n",
            "Mini batch: 60/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.1947 | Train accuracy: 89.36%.\n",
            "Training loss (epoch level): 0.2570 | Train accuracy: 89.64\n",
            "pgd linf: attack effectiveness 2.488%.\n",
            "\tVal accuracy 93.13% with accuracy 97.51% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 93.13% and accuracy 97.51% under attack.\n",
            "pgd linf: attack effectiveness 3.077%.\n",
            "Mini batch: 61/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.2966 | Train accuracy: 89.06%.\n",
            "pgd linf: attack effectiveness 3.125%.\n",
            "Mini batch: 62/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.2912 | Train accuracy: 84.38%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 63/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.2471 | Train accuracy: 90.62%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 64/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.1605 | Train accuracy: 92.19%.\n",
            "pgd linf: attack effectiveness 4.839%.\n",
            "Mini batch: 65/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.1592 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 7.273%.\n",
            "Mini batch: 66/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.1507 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 9.091%.\n",
            "Mini batch: 67/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.1431 | Train accuracy: 94.53%.\n",
            "pgd linf: attack effectiveness 11.475%.\n",
            "Mini batch: 68/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.1663 | Train accuracy: 92.19%.\n",
            "pgd linf: attack effectiveness 16.176%.\n",
            "Mini batch: 69/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.2363 | Train accuracy: 91.41%.\n",
            "pgd linf: attack effectiveness 14.286%.\n",
            "Mini batch: 70/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.2591 | Train accuracy: 93.62%.\n",
            "Training loss (epoch level): 0.2110 | Train accuracy: 91.71\n",
            "pgd linf: attack effectiveness 7.463%.\n",
            "\tVal accuracy 93.52% with accuracy 92.54% under attack.\n",
            "\tModel select at epoch 7 with validation accuracy 93.52% and accuracy 92.54% under attack.\n",
            "pgd linf: attack effectiveness 6.154%.\n",
            "Mini batch: 71/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.1455 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 6.250%.\n",
            "Mini batch: 72/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.1241 | Train accuracy: 96.09%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 73/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.1042 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 74/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0801 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 3.226%.\n",
            "Mini batch: 75/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.1201 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 1.818%.\n",
            "Mini batch: 76/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.1922 | Train accuracy: 96.09%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 77/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.1381 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 3.279%.\n",
            "Mini batch: 78/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.1351 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 7.353%.\n",
            "Mini batch: 79/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.1206 | Train accuracy: 93.75%.\n",
            "pgd linf: attack effectiveness 14.286%.\n",
            "Mini batch: 80/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.1872 | Train accuracy: 93.62%.\n",
            "Training loss (epoch level): 0.1347 | Train accuracy: 95.53\n",
            "pgd linf: attack effectiveness 7.463%.\n",
            "\tVal accuracy 93.64% with accuracy 92.54% under attack.\n",
            "\tModel select at epoch 8 with validation accuracy 93.64% and accuracy 92.54% under attack.\n",
            "pgd linf: attack effectiveness 6.154%.\n",
            "Mini batch: 81/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.1445 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 6.250%.\n",
            "Mini batch: 82/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.2051 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 1.493%.\n",
            "Mini batch: 83/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0868 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 1.471%.\n",
            "Mini batch: 84/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0597 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 3.226%.\n",
            "Mini batch: 85/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0961 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 3.636%.\n",
            "Mini batch: 86/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.1241 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 1.515%.\n",
            "Mini batch: 87/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0849 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 3.279%.\n",
            "Mini batch: 88/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.1858 | Train accuracy: 96.09%.\n",
            "pgd linf: attack effectiveness 2.941%.\n",
            "Mini batch: 89/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.1569 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 4.762%.\n",
            "Mini batch: 90/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.1247 | Train accuracy: 95.74%.\n",
            "Training loss (epoch level): 0.1269 | Train accuracy: 96.53\n",
            "pgd linf: attack effectiveness 2.985%.\n",
            "\tVal accuracy 95.63% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 9 with validation accuracy 95.63% and accuracy 97.01% under attack.\n",
            "pgd linf: attack effectiveness 3.077%.\n",
            "Mini batch: 91/300 | training time in 0 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.1288 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 1.562%.\n",
            "Mini batch: 92/300 | training time in 0 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.1499 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 93/300 | training time in 0 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0907 | Train accuracy: 95.31%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 94/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0370 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 3.226%.\n",
            "Mini batch: 95/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0859 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 5.455%.\n",
            "Mini batch: 96/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0907 | Train accuracy: 96.09%.\n",
            "pgd linf: attack effectiveness 4.545%.\n",
            "Mini batch: 97/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0741 | Train accuracy: 96.88%.\n",
            "pgd linf: attack effectiveness 6.557%.\n",
            "Mini batch: 98/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0886 | Train accuracy: 96.09%.\n",
            "pgd linf: attack effectiveness 5.882%.\n",
            "Mini batch: 99/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0787 | Train accuracy: 96.09%.\n",
            "pgd linf: attack effectiveness 4.762%.\n",
            "Mini batch: 100/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.1092 | Train accuracy: 95.74%.\n",
            "Training loss (epoch level): 0.0934 | Train accuracy: 96.45\n",
            "pgd linf: attack effectiveness 2.985%.\n",
            "\tVal accuracy 96.63% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 10 with validation accuracy 96.63% and accuracy 97.01% under attack.\n",
            "pgd linf: attack effectiveness 3.077%.\n",
            "Mini batch: 101/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0883 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 102/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0242 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 103/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0448 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 104/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0343 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "Mini batch: 105/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0521 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.818%.\n",
            "Mini batch: 106/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0934 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 107/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0307 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 3.279%.\n",
            "Mini batch: 108/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0676 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 109/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0326 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 4.762%.\n",
            "Mini batch: 110/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0785 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0546 | Train accuracy: 98.77\n",
            "pgd linf: attack effectiveness 3.483%.\n",
            "\tVal accuracy 96.26% with accuracy 96.52% under attack.\n",
            "\tModel select at epoch 10 with validation accuracy 96.63% and accuracy 97.01% under attack.\n",
            "pgd linf: attack effectiveness 3.077%.\n",
            "Mini batch: 111/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0678 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 1.562%.\n",
            "Mini batch: 112/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0129 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 113/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0161 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 114/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0136 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "Mini batch: 115/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0340 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.818%.\n",
            "Mini batch: 116/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.1150 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 117/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0322 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 118/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0514 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 119/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0325 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 4.762%.\n",
            "Mini batch: 120/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0437 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0419 | Train accuracy: 98.85\n",
            "pgd linf: attack effectiveness 2.985%.\n",
            "\tVal accuracy 96.51% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 10 with validation accuracy 96.63% and accuracy 97.01% under attack.\n",
            "pgd linf: attack effectiveness 3.077%.\n",
            "Mini batch: 121/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0594 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 1.562%.\n",
            "Mini batch: 122/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0142 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 123/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0162 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.471%.\n",
            "Mini batch: 124/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0231 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "Mini batch: 125/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0426 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.818%.\n",
            "Mini batch: 126/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0715 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 127/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0139 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 128/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0312 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 129/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0387 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 130/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0086 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0319 | Train accuracy: 99.30\n",
            "pgd linf: attack effectiveness 3.483%.\n",
            "\tVal accuracy 96.51% with accuracy 96.52% under attack.\n",
            "\tModel select at epoch 10 with validation accuracy 96.63% and accuracy 97.01% under attack.\n",
            "pgd linf: attack effectiveness 3.077%.\n",
            "Mini batch: 131/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0537 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 3.125%.\n",
            "Mini batch: 132/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0390 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 1.493%.\n",
            "Mini batch: 133/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0956 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.471%.\n",
            "Mini batch: 134/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0960 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "Mini batch: 135/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0421 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 136/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0332 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 137/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0569 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 138/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0377 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 139/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0304 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 140/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0102 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0495 | Train accuracy: 98.67\n",
            "pgd linf: attack effectiveness 3.980%.\n",
            "\tVal accuracy 95.88% with accuracy 96.02% under attack.\n",
            "\tModel select at epoch 10 with validation accuracy 96.63% and accuracy 97.01% under attack.\n",
            "pgd linf: attack effectiveness 4.615%.\n",
            "Mini batch: 141/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0474 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 1.562%.\n",
            "Mini batch: 142/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0190 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.493%.\n",
            "Mini batch: 143/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0183 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.471%.\n",
            "Mini batch: 144/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0318 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 3.226%.\n",
            "Mini batch: 145/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0991 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 3.636%.\n",
            "Mini batch: 146/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.1315 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 147/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0224 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 148/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0191 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 2.941%.\n",
            "Mini batch: 149/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0498 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 150/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0198 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0458 | Train accuracy: 98.98\n",
            "pgd linf: attack effectiveness 3.483%.\n",
            "\tVal accuracy 96.26% with accuracy 96.52% under attack.\n",
            "\tModel select at epoch 10 with validation accuracy 96.63% and accuracy 97.01% under attack.\n",
            "pgd linf: attack effectiveness 1.538%.\n",
            "Mini batch: 151/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.1045 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 1.562%.\n",
            "Mini batch: 152/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0997 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.493%.\n",
            "Mini batch: 153/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.1076 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 1.471%.\n",
            "Mini batch: 154/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0974 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 3.226%.\n",
            "Mini batch: 155/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.1211 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 3.636%.\n",
            "Mini batch: 156/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.1112 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 157/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0253 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 158/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0548 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 2.941%.\n",
            "Mini batch: 159/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0623 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 160/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0238 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0808 | Train accuracy: 98.83\n",
            "pgd linf: attack effectiveness 3.483%.\n",
            "\tVal accuracy 96.26% with accuracy 96.52% under attack.\n",
            "\tModel select at epoch 10 with validation accuracy 96.63% and accuracy 97.01% under attack.\n",
            "pgd linf: attack effectiveness 3.077%.\n",
            "Mini batch: 161/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0394 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.562%.\n",
            "Mini batch: 162/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0489 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.493%.\n",
            "Mini batch: 163/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0366 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.471%.\n",
            "Mini batch: 164/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0129 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 3.226%.\n",
            "Mini batch: 165/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0381 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 166/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0105 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 167/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0117 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 168/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0216 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 169/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0085 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 170/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0050 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0233 | Train accuracy: 99.45\n",
            "pgd linf: attack effectiveness 1.990%.\n",
            "\tVal accuracy 97.25% with accuracy 98.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 171/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0099 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 172/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 173/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0023 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 174/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0019 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "Mini batch: 175/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0105 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 176/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0056 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 177/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 178/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0091 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 179/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0083 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 180/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0063 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0062 | Train accuracy: 99.84\n",
            "pgd linf: attack effectiveness 3.483%.\n",
            "\tVal accuracy 96.26% with accuracy 96.52% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 1.538%.\n",
            "Mini batch: 181/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0577 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.562%.\n",
            "Mini batch: 182/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0513 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 183/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 184/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 185/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0106 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 186/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 187/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0044 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 188/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0294 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 189/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0112 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 190/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0013 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0179 | Train accuracy: 99.61\n",
            "pgd linf: attack effectiveness 2.985%.\n",
            "\tVal accuracy 96.63% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 1.538%.\n",
            "Mini batch: 191/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0979 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 1.562%.\n",
            "Mini batch: 192/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0694 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 193/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0071 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.471%.\n",
            "Mini batch: 194/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0589 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 195/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0196 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 196/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0132 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 197/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0099 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 198/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0164 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 199/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0160 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 200/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0078 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0316 | Train accuracy: 99.53\n",
            "pgd linf: attack effectiveness 1.990%.\n",
            "\tVal accuracy 97.0% with accuracy 98.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 201/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0272 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 202/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0192 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 203/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0057 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 204/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0069 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 3.226%.\n",
            "Mini batch: 205/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0459 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 3.636%.\n",
            "Mini batch: 206/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0392 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 207/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0093 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 208/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0437 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 209/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0062 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 210/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0096 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0213 | Train accuracy: 99.22\n",
            "pgd linf: attack effectiveness 1.990%.\n",
            "\tVal accuracy 96.13% with accuracy 98.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 211/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0348 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 212/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0307 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 213/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0046 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 214/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0030 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 215/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0054 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 216/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 217/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 218/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0319 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 4.412%.\n",
            "Mini batch: 219/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0503 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 220/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0124 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0181 | Train accuracy: 99.45\n",
            "pgd linf: attack effectiveness 2.488%.\n",
            "\tVal accuracy 96.51% with accuracy 97.51% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 221/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0080 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 222/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0260 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 223/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0207 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 224/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0096 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 225/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 226/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 227/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0075 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 228/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0059 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 229/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 230/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0107 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0101 | Train accuracy: 99.69\n",
            "pgd linf: attack effectiveness 3.980%.\n",
            "\tVal accuracy 95.88% with accuracy 96.02% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 3.077%.\n",
            "Mini batch: 231/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0660 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 232/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0017 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 233/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0036 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 234/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0011 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 235/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 236/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0051 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 237/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0089 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 238/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0047 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 239/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0151 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 240/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0031 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0113 | Train accuracy: 99.77\n",
            "pgd linf: attack effectiveness 2.985%.\n",
            "\tVal accuracy 96.38% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 1.538%.\n",
            "Mini batch: 241/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0252 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 242/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0009 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 243/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0014 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 244/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0028 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "Mini batch: 245/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0125 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 7.273%.\n",
            "Mini batch: 246/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0811 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 247/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0023 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 248/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0103 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 249/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0147 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 250/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0033 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0155 | Train accuracy: 99.53\n",
            "pgd linf: attack effectiveness 1.990%.\n",
            "\tVal accuracy 96.25% with accuracy 98.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 251/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0741 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 252/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0025 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 253/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0039 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 254/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0015 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "Mini batch: 255/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0152 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.818%.\n",
            "Mini batch: 256/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0320 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 257/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 258/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0161 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 259/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0079 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 260/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0024 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0161 | Train accuracy: 99.45\n",
            "pgd linf: attack effectiveness 2.985%.\n",
            "\tVal accuracy 96.13% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 1.538%.\n",
            "Mini batch: 261/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0414 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 262/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0035 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 263/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0022 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 264/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0017 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 265/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0086 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 266/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 267/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0157 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 268/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0108 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 269/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0017 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 270/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0004 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0092 | Train accuracy: 99.77\n",
            "pgd linf: attack effectiveness 2.985%.\n",
            "\tVal accuracy 96.26% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 1.538%.\n",
            "Mini batch: 271/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0462 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 272/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0010 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 273/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0013 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 274/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0015 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.613%.\n",
            "Mini batch: 275/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0167 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 5.455%.\n",
            "Mini batch: 276/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.1100 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 277/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0026 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 278/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0102 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 279/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0045 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 280/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0027 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0197 | Train accuracy: 99.45\n",
            "pgd linf: attack effectiveness 1.990%.\n",
            "\tVal accuracy 96.0% with accuracy 98.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 281/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0287 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 282/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0069 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 283/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0043 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 284/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0060 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 285/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0101 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 5.455%.\n",
            "Mini batch: 286/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.1385 | Train accuracy: 97.66%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 287/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0078 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 288/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0285 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 289/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0063 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 290/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0123 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0249 | Train accuracy: 99.61\n",
            "pgd linf: attack effectiveness 3.483%.\n",
            "\tVal accuracy 95.63% with accuracy 96.52% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 291/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0506 | Train accuracy: 98.44%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 292/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0061 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 293/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0006 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 294/300 | training time in 1 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0005 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 295/300 | training time in 1 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0020 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 1.818%.\n",
            "Mini batch: 296/300 | training time in 1 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0474 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.515%.\n",
            "Mini batch: 297/300 | training time in 1 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0294 | Train accuracy: 99.22%.\n",
            "pgd linf: attack effectiveness 1.639%.\n",
            "Mini batch: 298/300 | training time in 1 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0020 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 299/300 | training time in 1 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0097 | Train accuracy: 100.00%.\n",
            "pgd linf: attack effectiveness 0.000%.\n",
            "Mini batch: 300/300 | training time in 1 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0048 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0153 | Train accuracy: 99.69\n",
            "pgd linf: attack effectiveness 2.985%.\n",
            "\tVal accuracy 95.88% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 17 with validation accuracy 97.25% and accuracy 98.01% under attack.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test: accuracy\n",
        "max_adv_training_model.load()\n",
        "max_adv_training_model.model.predict(test_Loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--3mtpw65v2c",
        "outputId": "35e59611-74f5-479a-b160-2a55bbd6a25a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on the test dataset is 96.75000%\n",
            "The balanced accuracy on the test dataset is 96.73717%\n",
            "Other evaluation metrics we may need:\n",
            "False Negative Rate (FNR) is 1.98020%, False Positive Rate (FPR) is 4.54545%, F1 score is 96.82152%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_adv_training_model.adv_predict(test_Loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcH9mQEEC1Hb",
        "outputId": "c3f99eb8-2e33-4299-b244-c8123598da9b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd linf: attack effectiveness 1.980%.\n",
            "\tadversarial accuracy  98.02% under attack.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# norm = ''l2''\n",
        "\n",
        "model = MalwareDetectionDNN(3216,2)\n",
        "model = model.double().to(device)\n",
        "attack = PGD(norm='l2', use_random=False,rounding_threshold=0.5,is_attacker=False,manipulation_x=manipulation_x, omega=omega,api_flag=api_flag,device=device)\n",
        "attack_param = {}\n",
        "max_adv_training_model = PGDAdvTraining(model, attack, attack_param)\n",
        "max_adv_training_model.fit(train_Loader,validation_Loader,adv_epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928092ed-8138-42ce-c61c-1115959d36ad",
        "id": "GzGJdauV5wD1"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd l2: attack effectiveness 100.000%.\n",
            "Mini batch: 1/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.7264 | Train accuracy: 16.41%.\n",
            "pgd l2: attack effectiveness 54.688%.\n",
            "Mini batch: 2/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.6965 | Train accuracy: 53.12%.\n",
            "pgd l2: attack effectiveness 100.000%.\n",
            "Mini batch: 3/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.7009 | Train accuracy: 46.09%.\n",
            "pgd l2: attack effectiveness 48.529%.\n",
            "Mini batch: 4/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.6928 | Train accuracy: 42.97%.\n",
            "pgd l2: attack effectiveness 66.129%.\n",
            "Mini batch: 5/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.7273 | Train accuracy: 48.44%.\n",
            "pgd l2: attack effectiveness 54.545%.\n",
            "Mini batch: 6/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.7012 | Train accuracy: 45.31%.\n",
            "pgd l2: attack effectiveness 37.879%.\n",
            "Mini batch: 7/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.6211 | Train accuracy: 53.91%.\n",
            "pgd l2: attack effectiveness 29.508%.\n",
            "Mini batch: 8/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.6246 | Train accuracy: 58.59%.\n",
            "pgd l2: attack effectiveness 55.882%.\n",
            "Mini batch: 9/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.6330 | Train accuracy: 57.81%.\n",
            "pgd l2: attack effectiveness 47.619%.\n",
            "Mini batch: 10/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.5940 | Train accuracy: 61.70%.\n",
            "Training loss (epoch level): 0.6718 | Train accuracy: 48.44\n",
            "pgd l2: attack effectiveness 54.726%.\n",
            "\tVal accuracy 59.39% with accuracy 45.27% under attack.\n",
            "\tModel select at epoch 1 with validation accuracy 59.39% and accuracy 45.27% under attack.\n",
            "pgd l2: attack effectiveness 53.846%.\n",
            "Mini batch: 11/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.5543 | Train accuracy: 68.75%.\n",
            "pgd l2: attack effectiveness 54.688%.\n",
            "Mini batch: 12/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.5568 | Train accuracy: 71.88%.\n",
            "pgd l2: attack effectiveness 52.239%.\n",
            "Mini batch: 13/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.5868 | Train accuracy: 69.53%.\n",
            "pgd l2: attack effectiveness 50.000%.\n",
            "Mini batch: 14/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.5939 | Train accuracy: 70.31%.\n",
            "pgd l2: attack effectiveness 51.613%.\n",
            "Mini batch: 15/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.5979 | Train accuracy: 67.97%.\n",
            "pgd l2: attack effectiveness 41.818%.\n",
            "Mini batch: 16/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.6200 | Train accuracy: 68.75%.\n",
            "pgd l2: attack effectiveness 42.424%.\n",
            "Mini batch: 17/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.5213 | Train accuracy: 75.00%.\n",
            "pgd l2: attack effectiveness 36.066%.\n",
            "Mini batch: 18/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.4997 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 45.588%.\n",
            "Mini batch: 19/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.5404 | Train accuracy: 71.09%.\n",
            "pgd l2: attack effectiveness 42.857%.\n",
            "Mini batch: 20/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.5242 | Train accuracy: 74.47%.\n",
            "Training loss (epoch level): 0.5595 | Train accuracy: 71.59\n",
            "pgd l2: attack effectiveness 46.766%.\n",
            "\tVal accuracy 70.99% with accuracy 53.23% under attack.\n",
            "\tModel select at epoch 2 with validation accuracy 70.99% and accuracy 53.23% under attack.\n",
            "pgd l2: attack effectiveness 41.538%.\n",
            "Mini batch: 21/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.5235 | Train accuracy: 77.34%.\n",
            "pgd l2: attack effectiveness 39.062%.\n",
            "Mini batch: 22/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.4829 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 43.284%.\n",
            "Mini batch: 23/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.5073 | Train accuracy: 74.22%.\n",
            "pgd l2: attack effectiveness 44.118%.\n",
            "Mini batch: 24/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.4861 | Train accuracy: 75.78%.\n",
            "pgd l2: attack effectiveness 53.226%.\n",
            "Mini batch: 25/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.4654 | Train accuracy: 78.91%.\n",
            "pgd l2: attack effectiveness 41.818%.\n",
            "Mini batch: 26/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.5446 | Train accuracy: 75.00%.\n",
            "pgd l2: attack effectiveness 42.424%.\n",
            "Mini batch: 27/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.4740 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 36.066%.\n",
            "Mini batch: 28/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.4014 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 41.176%.\n",
            "Mini batch: 29/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.4975 | Train accuracy: 75.00%.\n",
            "pgd l2: attack effectiveness 38.095%.\n",
            "Mini batch: 30/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.4289 | Train accuracy: 80.85%.\n",
            "Training loss (epoch level): 0.4812 | Train accuracy: 77.54\n",
            "pgd l2: attack effectiveness 43.781%.\n",
            "\tVal accuracy 73.23% with accuracy 56.22% under attack.\n",
            "\tModel select at epoch 3 with validation accuracy 73.23% and accuracy 56.22% under attack.\n",
            "pgd l2: attack effectiveness 38.462%.\n",
            "Mini batch: 31/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.4769 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 34.375%.\n",
            "Mini batch: 32/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.3993 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 38.806%.\n",
            "Mini batch: 33/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.4636 | Train accuracy: 78.91%.\n",
            "pgd l2: attack effectiveness 41.176%.\n",
            "Mini batch: 34/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.4370 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 50.000%.\n",
            "Mini batch: 35/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.4920 | Train accuracy: 74.22%.\n",
            "pgd l2: attack effectiveness 41.818%.\n",
            "Mini batch: 36/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.5056 | Train accuracy: 78.91%.\n",
            "pgd l2: attack effectiveness 40.909%.\n",
            "Mini batch: 37/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.4666 | Train accuracy: 77.34%.\n",
            "pgd l2: attack effectiveness 40.984%.\n",
            "Mini batch: 38/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.4486 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 42.647%.\n",
            "Mini batch: 39/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.4940 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 47.619%.\n",
            "Mini batch: 40/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.4494 | Train accuracy: 78.72%.\n",
            "Training loss (epoch level): 0.4633 | Train accuracy: 78.03\n",
            "pgd l2: attack effectiveness 46.269%.\n",
            "\tVal accuracy 72.12% with accuracy 53.73% under attack.\n",
            "\tModel select at epoch 3 with validation accuracy 73.23% and accuracy 56.22% under attack.\n",
            "pgd l2: attack effectiveness 36.923%.\n",
            "Mini batch: 41/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.4273 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 35.938%.\n",
            "Mini batch: 42/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.4073 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 40.299%.\n",
            "Mini batch: 43/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.4583 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 42.647%.\n",
            "Mini batch: 44/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.4650 | Train accuracy: 75.00%.\n",
            "pgd l2: attack effectiveness 50.000%.\n",
            "Mini batch: 45/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.4545 | Train accuracy: 75.00%.\n",
            "pgd l2: attack effectiveness 40.000%.\n",
            "Mini batch: 46/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.5065 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 40.909%.\n",
            "Mini batch: 47/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.4840 | Train accuracy: 75.78%.\n",
            "pgd l2: attack effectiveness 39.344%.\n",
            "Mini batch: 48/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.4279 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 44.118%.\n",
            "Mini batch: 49/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.5123 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 47.619%.\n",
            "Mini batch: 50/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.4327 | Train accuracy: 80.85%.\n",
            "Training loss (epoch level): 0.4576 | Train accuracy: 78.16\n",
            "pgd l2: attack effectiveness 45.274%.\n",
            "\tVal accuracy 72.74% with accuracy 54.73% under attack.\n",
            "\tModel select at epoch 3 with validation accuracy 73.23% and accuracy 56.22% under attack.\n",
            "pgd l2: attack effectiveness 38.462%.\n",
            "Mini batch: 51/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.4109 | Train accuracy: 78.91%.\n",
            "pgd l2: attack effectiveness 35.938%.\n",
            "Mini batch: 52/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.4209 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 38.806%.\n",
            "Mini batch: 53/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.4204 | Train accuracy: 77.34%.\n",
            "pgd l2: attack effectiveness 42.647%.\n",
            "Mini batch: 54/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.4298 | Train accuracy: 75.78%.\n",
            "pgd l2: attack effectiveness 54.839%.\n",
            "Mini batch: 55/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.4426 | Train accuracy: 73.44%.\n",
            "pgd l2: attack effectiveness 47.273%.\n",
            "Mini batch: 56/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.4651 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 43.939%.\n",
            "Mini batch: 57/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.5158 | Train accuracy: 75.00%.\n",
            "pgd l2: attack effectiveness 37.705%.\n",
            "Mini batch: 58/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.4074 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 42.647%.\n",
            "Mini batch: 59/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.4542 | Train accuracy: 74.22%.\n",
            "pgd l2: attack effectiveness 33.333%.\n",
            "Mini batch: 60/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.3355 | Train accuracy: 89.36%.\n",
            "Training loss (epoch level): 0.4303 | Train accuracy: 78.47\n",
            "pgd l2: attack effectiveness 18.408%.\n",
            "\tVal accuracy 86.05% with accuracy 81.59% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 9.231%.\n",
            "Mini batch: 61/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.3578 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 12.500%.\n",
            "Mini batch: 62/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.3516 | Train accuracy: 87.50%.\n",
            "pgd l2: attack effectiveness 7.463%.\n",
            "Mini batch: 63/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.2988 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 36.765%.\n",
            "Mini batch: 64/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.4578 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 35.484%.\n",
            "Mini batch: 65/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.4317 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 41.818%.\n",
            "Mini batch: 66/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.5679 | Train accuracy: 73.44%.\n",
            "pgd l2: attack effectiveness 37.879%.\n",
            "Mini batch: 67/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.4224 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 36.066%.\n",
            "Mini batch: 68/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.4245 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 42.647%.\n",
            "Mini batch: 69/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.4736 | Train accuracy: 75.78%.\n",
            "pgd l2: attack effectiveness 38.095%.\n",
            "Mini batch: 70/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.3519 | Train accuracy: 80.85%.\n",
            "Training loss (epoch level): 0.4138 | Train accuracy: 80.90\n",
            "pgd l2: attack effectiveness 44.776%.\n",
            "\tVal accuracy 74.11% with accuracy 55.22% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 38.462%.\n",
            "Mini batch: 71/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.3806 | Train accuracy: 77.34%.\n",
            "pgd l2: attack effectiveness 32.812%.\n",
            "Mini batch: 72/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.3759 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 38.806%.\n",
            "Mini batch: 73/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.3845 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 39.706%.\n",
            "Mini batch: 74/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.3691 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 46.774%.\n",
            "Mini batch: 75/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.4030 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 38.182%.\n",
            "Mini batch: 76/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.4611 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 36.364%.\n",
            "Mini batch: 77/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.3839 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 34.426%.\n",
            "Mini batch: 78/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.3968 | Train accuracy: 82.03%.\n",
            "pgd l2: attack effectiveness 41.176%.\n",
            "Mini batch: 79/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.5220 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 42.857%.\n",
            "Mini batch: 80/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.3386 | Train accuracy: 80.85%.\n",
            "Training loss (epoch level): 0.4015 | Train accuracy: 79.49\n",
            "pgd l2: attack effectiveness 45.771%.\n",
            "\tVal accuracy 73.49% with accuracy 54.23% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 38.462%.\n",
            "Mini batch: 81/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.3552 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 32.812%.\n",
            "Mini batch: 82/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.3490 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 7.463%.\n",
            "Mini batch: 83/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.3588 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 10.294%.\n",
            "Mini batch: 84/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.3530 | Train accuracy: 89.06%.\n",
            "pgd l2: attack effectiveness 16.129%.\n",
            "Mini batch: 85/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.3924 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 18.182%.\n",
            "Mini batch: 86/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.4169 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 31.818%.\n",
            "Mini batch: 87/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.4469 | Train accuracy: 78.91%.\n",
            "pgd l2: attack effectiveness 37.705%.\n",
            "Mini batch: 88/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.4026 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 42.647%.\n",
            "Mini batch: 89/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.4723 | Train accuracy: 75.00%.\n",
            "pgd l2: attack effectiveness 38.095%.\n",
            "Mini batch: 90/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.3465 | Train accuracy: 85.11%.\n",
            "Training loss (epoch level): 0.3894 | Train accuracy: 81.56\n",
            "pgd l2: attack effectiveness 41.294%.\n",
            "\tVal accuracy 73.48% with accuracy 58.71% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 32.308%.\n",
            "Mini batch: 91/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.3750 | Train accuracy: 74.22%.\n",
            "pgd l2: attack effectiveness 29.688%.\n",
            "Mini batch: 92/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.3542 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 23.881%.\n",
            "Mini batch: 93/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.3532 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 10.294%.\n",
            "Mini batch: 94/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.3067 | Train accuracy: 89.06%.\n",
            "pgd l2: attack effectiveness 14.516%.\n",
            "Mini batch: 95/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.3137 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 16.364%.\n",
            "Mini batch: 96/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.3861 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 30.303%.\n",
            "Mini batch: 97/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.3849 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 34.426%.\n",
            "Mini batch: 98/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.3800 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 42.647%.\n",
            "Mini batch: 99/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.4648 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 42.857%.\n",
            "Mini batch: 100/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.3165 | Train accuracy: 80.85%.\n",
            "Training loss (epoch level): 0.3635 | Train accuracy: 80.98\n",
            "pgd l2: attack effectiveness 46.766%.\n",
            "\tVal accuracy 72.74% with accuracy 53.23% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 38.462%.\n",
            "Mini batch: 101/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.3388 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 23.438%.\n",
            "Mini batch: 102/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.2923 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 5.970%.\n",
            "Mini batch: 103/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.2612 | Train accuracy: 92.97%.\n",
            "pgd l2: attack effectiveness 5.882%.\n",
            "Mini batch: 104/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.2860 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 14.516%.\n",
            "Mini batch: 105/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.2830 | Train accuracy: 89.06%.\n",
            "pgd l2: attack effectiveness 10.909%.\n",
            "Mini batch: 106/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.3995 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 12.121%.\n",
            "Mini batch: 107/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.2823 | Train accuracy: 87.50%.\n",
            "pgd l2: attack effectiveness 29.508%.\n",
            "Mini batch: 108/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.3693 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 41.176%.\n",
            "Mini batch: 109/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.5746 | Train accuracy: 75.78%.\n",
            "pgd l2: attack effectiveness 33.333%.\n",
            "Mini batch: 110/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.3203 | Train accuracy: 85.11%.\n",
            "Training loss (epoch level): 0.3407 | Train accuracy: 85.39\n",
            "pgd l2: attack effectiveness 38.308%.\n",
            "\tVal accuracy 77.1% with accuracy 61.69% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 29.231%.\n",
            "Mini batch: 111/300 | training time in 0 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.3208 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 32.812%.\n",
            "Mini batch: 112/300 | training time in 0 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.3097 | Train accuracy: 82.03%.\n",
            "pgd l2: attack effectiveness 29.851%.\n",
            "Mini batch: 113/300 | training time in 0 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.3422 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 27.941%.\n",
            "Mini batch: 114/300 | training time in 0 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.3033 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 29.032%.\n",
            "Mini batch: 115/300 | training time in 0 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.3208 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 10.909%.\n",
            "Mini batch: 116/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.3845 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 30.303%.\n",
            "Mini batch: 117/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.3710 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 26.230%.\n",
            "Mini batch: 118/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.3043 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 39.706%.\n",
            "Mini batch: 119/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.4709 | Train accuracy: 75.78%.\n",
            "pgd l2: attack effectiveness 38.095%.\n",
            "Mini batch: 120/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.3205 | Train accuracy: 80.85%.\n",
            "Training loss (epoch level): 0.3448 | Train accuracy: 80.98\n",
            "pgd l2: attack effectiveness 42.289%.\n",
            "\tVal accuracy 75.11% with accuracy 57.71% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 30.769%.\n",
            "Mini batch: 121/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.3099 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 23.438%.\n",
            "Mini batch: 122/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.3309 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 29.851%.\n",
            "Mini batch: 123/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.3280 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 26.471%.\n",
            "Mini batch: 124/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.2932 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 12.903%.\n",
            "Mini batch: 125/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.3362 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 7.273%.\n",
            "Mini batch: 126/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.3405 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 13.636%.\n",
            "Mini batch: 127/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.2706 | Train accuracy: 90.62%.\n",
            "pgd l2: attack effectiveness 21.311%.\n",
            "Mini batch: 128/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.2617 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 32.353%.\n",
            "Mini batch: 129/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.4720 | Train accuracy: 78.91%.\n",
            "pgd l2: attack effectiveness 33.333%.\n",
            "Mini batch: 130/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.2723 | Train accuracy: 82.98%.\n",
            "Training loss (epoch level): 0.3215 | Train accuracy: 83.45\n",
            "pgd l2: attack effectiveness 35.323%.\n",
            "\tVal accuracy 76.96% with accuracy 64.68% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 23.077%.\n",
            "Mini batch: 131/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.2844 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 18.750%.\n",
            "Mini batch: 132/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.3706 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 25.373%.\n",
            "Mini batch: 133/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.3793 | Train accuracy: 82.03%.\n",
            "pgd l2: attack effectiveness 33.824%.\n",
            "Mini batch: 134/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.3283 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 19.355%.\n",
            "Mini batch: 135/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.3842 | Train accuracy: 90.62%.\n",
            "pgd l2: attack effectiveness 18.182%.\n",
            "Mini batch: 136/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.3888 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 25.758%.\n",
            "Mini batch: 137/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.3647 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 26.230%.\n",
            "Mini batch: 138/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.2996 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 38.235%.\n",
            "Mini batch: 139/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.3851 | Train accuracy: 76.56%.\n",
            "pgd l2: attack effectiveness 38.095%.\n",
            "Mini batch: 140/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.3250 | Train accuracy: 82.98%.\n",
            "Training loss (epoch level): 0.3510 | Train accuracy: 84.16\n",
            "pgd l2: attack effectiveness 43.284%.\n",
            "\tVal accuracy 73.23% with accuracy 56.72% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 33.846%.\n",
            "Mini batch: 141/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.3293 | Train accuracy: 78.91%.\n",
            "pgd l2: attack effectiveness 35.938%.\n",
            "Mini batch: 142/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.4037 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 38.806%.\n",
            "Mini batch: 143/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.4292 | Train accuracy: 75.78%.\n",
            "pgd l2: attack effectiveness 42.647%.\n",
            "Mini batch: 144/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.4475 | Train accuracy: 73.44%.\n",
            "pgd l2: attack effectiveness 46.774%.\n",
            "Mini batch: 145/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.4066 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 14.545%.\n",
            "Mini batch: 146/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.4538 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 13.636%.\n",
            "Mini batch: 147/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.3593 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 22.951%.\n",
            "Mini batch: 148/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.2909 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 32.353%.\n",
            "Mini batch: 149/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.3623 | Train accuracy: 77.34%.\n",
            "pgd l2: attack effectiveness 38.095%.\n",
            "Mini batch: 150/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.3275 | Train accuracy: 78.72%.\n",
            "Training loss (epoch level): 0.3810 | Train accuracy: 78.81\n",
            "pgd l2: attack effectiveness 44.279%.\n",
            "\tVal accuracy 74.61% with accuracy 55.72% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 86.05% and accuracy 81.59% under attack.\n",
            "pgd l2: attack effectiveness 36.923%.\n",
            "Mini batch: 151/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.3530 | Train accuracy: 78.91%.\n",
            "pgd l2: attack effectiveness 34.375%.\n",
            "Mini batch: 152/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.3279 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 40.299%.\n",
            "Mini batch: 153/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.3449 | Train accuracy: 75.00%.\n",
            "pgd l2: attack effectiveness 36.765%.\n",
            "Mini batch: 154/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.3240 | Train accuracy: 78.91%.\n",
            "pgd l2: attack effectiveness 43.548%.\n",
            "Mini batch: 155/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.3787 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 34.545%.\n",
            "Mini batch: 156/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.3766 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 12.121%.\n",
            "Mini batch: 157/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.3153 | Train accuracy: 85.94%.\n",
            "pgd l2: attack effectiveness 19.672%.\n",
            "Mini batch: 158/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.2395 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 25.000%.\n",
            "Mini batch: 159/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.3577 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 33.333%.\n",
            "Mini batch: 160/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.2496 | Train accuracy: 85.11%.\n",
            "Training loss (epoch level): 0.3267 | Train accuracy: 81.71\n",
            "pgd l2: attack effectiveness 12.935%.\n",
            "\tVal accuracy 87.66% with accuracy 87.06% under attack.\n",
            "\tModel select at epoch 16 with validation accuracy 87.66% and accuracy 87.06% under attack.\n",
            "pgd l2: attack effectiveness 6.154%.\n",
            "Mini batch: 161/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.2582 | Train accuracy: 89.06%.\n",
            "pgd l2: attack effectiveness 18.750%.\n",
            "Mini batch: 162/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.3168 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 25.373%.\n",
            "Mini batch: 163/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.3749 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 29.412%.\n",
            "Mini batch: 164/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.3344 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 32.258%.\n",
            "Mini batch: 165/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.3040 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 30.909%.\n",
            "Mini batch: 166/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.4173 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 10.606%.\n",
            "Mini batch: 167/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.2267 | Train accuracy: 92.19%.\n",
            "pgd l2: attack effectiveness 18.033%.\n",
            "Mini batch: 168/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.2268 | Train accuracy: 85.94%.\n",
            "pgd l2: attack effectiveness 29.412%.\n",
            "Mini batch: 169/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.3431 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 33.333%.\n",
            "Mini batch: 170/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.2919 | Train accuracy: 87.23%.\n",
            "Training loss (epoch level): 0.3094 | Train accuracy: 84.90\n",
            "pgd l2: attack effectiveness 34.328%.\n",
            "\tVal accuracy 77.71% with accuracy 65.67% under attack.\n",
            "\tModel select at epoch 16 with validation accuracy 87.66% and accuracy 87.06% under attack.\n",
            "pgd l2: attack effectiveness 21.538%.\n",
            "Mini batch: 171/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.2836 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 23.438%.\n",
            "Mini batch: 172/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.3307 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 23.881%.\n",
            "Mini batch: 173/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.3441 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 33.824%.\n",
            "Mini batch: 174/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.3279 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 17.742%.\n",
            "Mini batch: 175/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.3362 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 16.364%.\n",
            "Mini batch: 176/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.3236 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 12.121%.\n",
            "Mini batch: 177/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.3058 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 19.672%.\n",
            "Mini batch: 178/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.2326 | Train accuracy: 87.50%.\n",
            "pgd l2: attack effectiveness 17.647%.\n",
            "Mini batch: 179/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.3233 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 14.286%.\n",
            "Mini batch: 180/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.1931 | Train accuracy: 91.49%.\n",
            "Training loss (epoch level): 0.3001 | Train accuracy: 86.81\n",
            "pgd l2: attack effectiveness 17.910%.\n",
            "\tVal accuracy 86.67% with accuracy 82.09% under attack.\n",
            "\tModel select at epoch 16 with validation accuracy 87.66% and accuracy 87.06% under attack.\n",
            "pgd l2: attack effectiveness 7.692%.\n",
            "Mini batch: 181/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.1889 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 12.500%.\n",
            "Mini batch: 182/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.2179 | Train accuracy: 91.41%.\n",
            "pgd l2: attack effectiveness 5.970%.\n",
            "Mini batch: 183/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.1665 | Train accuracy: 94.53%.\n",
            "pgd l2: attack effectiveness 7.353%.\n",
            "Mini batch: 184/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.2478 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 30.645%.\n",
            "Mini batch: 185/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.3182 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 10.909%.\n",
            "Mini batch: 186/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.3516 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 24.242%.\n",
            "Mini batch: 187/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.3488 | Train accuracy: 82.03%.\n",
            "pgd l2: attack effectiveness 9.836%.\n",
            "Mini batch: 188/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.2216 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 33.824%.\n",
            "Mini batch: 189/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.4592 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 28.571%.\n",
            "Mini batch: 190/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.2544 | Train accuracy: 85.11%.\n",
            "Training loss (epoch level): 0.2775 | Train accuracy: 87.65\n",
            "pgd l2: attack effectiveness 15.423%.\n",
            "\tVal accuracy 87.79% with accuracy 84.58% under attack.\n",
            "\tModel select at epoch 19 with validation accuracy 87.79% and accuracy 84.58% under attack.\n",
            "pgd l2: attack effectiveness 7.692%.\n",
            "Mini batch: 191/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.1959 | Train accuracy: 95.31%.\n",
            "pgd l2: attack effectiveness 9.375%.\n",
            "Mini batch: 192/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.2195 | Train accuracy: 94.53%.\n",
            "pgd l2: attack effectiveness 22.388%.\n",
            "Mini batch: 193/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.2812 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 5.882%.\n",
            "Mini batch: 194/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.2240 | Train accuracy: 94.53%.\n",
            "pgd l2: attack effectiveness 11.290%.\n",
            "Mini batch: 195/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.2304 | Train accuracy: 92.97%.\n",
            "pgd l2: attack effectiveness 16.364%.\n",
            "Mini batch: 196/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.3911 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 10.606%.\n",
            "Mini batch: 197/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.2575 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 19.672%.\n",
            "Mini batch: 198/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.3270 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 32.353%.\n",
            "Mini batch: 199/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.4016 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 33.333%.\n",
            "Mini batch: 200/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.2795 | Train accuracy: 87.23%.\n",
            "Training loss (epoch level): 0.2808 | Train accuracy: 88.72\n",
            "pgd l2: attack effectiveness 34.826%.\n",
            "\tVal accuracy 78.34% with accuracy 65.17% under attack.\n",
            "\tModel select at epoch 19 with validation accuracy 87.79% and accuracy 84.58% under attack.\n",
            "pgd l2: attack effectiveness 23.077%.\n",
            "Mini batch: 201/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.2410 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 32.812%.\n",
            "Mini batch: 202/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.3002 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 38.806%.\n",
            "Mini batch: 203/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.3258 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 33.824%.\n",
            "Mini batch: 204/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.3342 | Train accuracy: 82.03%.\n",
            "pgd l2: attack effectiveness 41.935%.\n",
            "Mini batch: 205/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.3388 | Train accuracy: 79.69%.\n",
            "pgd l2: attack effectiveness 30.909%.\n",
            "Mini batch: 206/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.3141 | Train accuracy: 85.94%.\n",
            "pgd l2: attack effectiveness 9.091%.\n",
            "Mini batch: 207/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.3019 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 18.033%.\n",
            "Mini batch: 208/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.2395 | Train accuracy: 91.41%.\n",
            "pgd l2: attack effectiveness 22.059%.\n",
            "Mini batch: 209/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.3420 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 38.095%.\n",
            "Mini batch: 210/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.2686 | Train accuracy: 82.98%.\n",
            "Training loss (epoch level): 0.3006 | Train accuracy: 84.24\n",
            "pgd l2: attack effectiveness 13.930%.\n",
            "\tVal accuracy 87.41% with accuracy 86.07% under attack.\n",
            "\tModel select at epoch 19 with validation accuracy 87.79% and accuracy 84.58% under attack.\n",
            "pgd l2: attack effectiveness 6.154%.\n",
            "Mini batch: 211/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.1973 | Train accuracy: 94.53%.\n",
            "pgd l2: attack effectiveness 20.312%.\n",
            "Mini batch: 212/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.3058 | Train accuracy: 87.50%.\n",
            "pgd l2: attack effectiveness 22.388%.\n",
            "Mini batch: 213/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.2792 | Train accuracy: 85.94%.\n",
            "pgd l2: attack effectiveness 27.941%.\n",
            "Mini batch: 214/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.3165 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 32.258%.\n",
            "Mini batch: 215/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.3761 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 32.727%.\n",
            "Mini batch: 216/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.3533 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 21.212%.\n",
            "Mini batch: 217/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.2993 | Train accuracy: 89.06%.\n",
            "pgd l2: attack effectiveness 19.672%.\n",
            "Mini batch: 218/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.2297 | Train accuracy: 89.06%.\n",
            "pgd l2: attack effectiveness 26.471%.\n",
            "Mini batch: 219/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.3005 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 23.810%.\n",
            "Mini batch: 220/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.1854 | Train accuracy: 91.49%.\n",
            "Training loss (epoch level): 0.2843 | Train accuracy: 87.66\n",
            "pgd l2: attack effectiveness 29.353%.\n",
            "\tVal accuracy 80.57% with accuracy 70.65% under attack.\n",
            "\tModel select at epoch 19 with validation accuracy 87.79% and accuracy 84.58% under attack.\n",
            "pgd l2: attack effectiveness 21.538%.\n",
            "Mini batch: 221/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.2506 | Train accuracy: 87.50%.\n",
            "pgd l2: attack effectiveness 7.812%.\n",
            "Mini batch: 222/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.2440 | Train accuracy: 92.19%.\n",
            "pgd l2: attack effectiveness 7.463%.\n",
            "Mini batch: 223/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.2523 | Train accuracy: 92.19%.\n",
            "pgd l2: attack effectiveness 8.824%.\n",
            "Mini batch: 224/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.2099 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 11.290%.\n",
            "Mini batch: 225/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.3161 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 10.909%.\n",
            "Mini batch: 226/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.2827 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 12.121%.\n",
            "Mini batch: 227/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.3142 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 6.557%.\n",
            "Mini batch: 228/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.2402 | Train accuracy: 90.62%.\n",
            "pgd l2: attack effectiveness 16.176%.\n",
            "Mini batch: 229/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.2786 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 14.286%.\n",
            "Mini batch: 230/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.2010 | Train accuracy: 91.49%.\n",
            "Training loss (epoch level): 0.2589 | Train accuracy: 90.79\n",
            "pgd l2: attack effectiveness 13.433%.\n",
            "\tVal accuracy 88.03% with accuracy 86.57% under attack.\n",
            "\tModel select at epoch 23 with validation accuracy 88.03% and accuracy 86.57% under attack.\n",
            "pgd l2: attack effectiveness 6.154%.\n",
            "Mini batch: 231/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.1970 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 15.625%.\n",
            "Mini batch: 232/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.2631 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 20.896%.\n",
            "Mini batch: 233/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.3006 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 10.294%.\n",
            "Mini batch: 234/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.2048 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 11.290%.\n",
            "Mini batch: 235/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.2205 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 30.909%.\n",
            "Mini batch: 236/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.3286 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 25.758%.\n",
            "Mini batch: 237/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.3388 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 16.393%.\n",
            "Mini batch: 238/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.2301 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 26.471%.\n",
            "Mini batch: 239/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.3101 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 9.524%.\n",
            "Mini batch: 240/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.2081 | Train accuracy: 93.62%.\n",
            "Training loss (epoch level): 0.2602 | Train accuracy: 88.89\n",
            "pgd l2: attack effectiveness 38.308%.\n",
            "\tVal accuracy 75.97% with accuracy 61.69% under attack.\n",
            "\tModel select at epoch 23 with validation accuracy 88.03% and accuracy 86.57% under attack.\n",
            "pgd l2: attack effectiveness 32.308%.\n",
            "Mini batch: 241/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.2872 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 28.125%.\n",
            "Mini batch: 242/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.2764 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 31.343%.\n",
            "Mini batch: 243/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.3053 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 39.706%.\n",
            "Mini batch: 244/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.3127 | Train accuracy: 82.03%.\n",
            "pgd l2: attack effectiveness 41.935%.\n",
            "Mini batch: 245/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.3601 | Train accuracy: 77.34%.\n",
            "pgd l2: attack effectiveness 20.000%.\n",
            "Mini batch: 246/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.3717 | Train accuracy: 85.94%.\n",
            "pgd l2: attack effectiveness 22.727%.\n",
            "Mini batch: 247/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.3274 | Train accuracy: 81.25%.\n",
            "pgd l2: attack effectiveness 19.672%.\n",
            "Mini batch: 248/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.3850 | Train accuracy: 85.94%.\n",
            "pgd l2: attack effectiveness 19.118%.\n",
            "Mini batch: 249/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.3092 | Train accuracy: 85.94%.\n",
            "pgd l2: attack effectiveness 28.571%.\n",
            "Mini batch: 250/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.2875 | Train accuracy: 87.23%.\n",
            "Training loss (epoch level): 0.3222 | Train accuracy: 83.33\n",
            "pgd l2: attack effectiveness 32.836%.\n",
            "\tVal accuracy 78.33% with accuracy 67.16% under attack.\n",
            "\tModel select at epoch 23 with validation accuracy 88.03% and accuracy 86.57% under attack.\n",
            "pgd l2: attack effectiveness 23.077%.\n",
            "Mini batch: 251/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.2911 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 7.812%.\n",
            "Mini batch: 252/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.2200 | Train accuracy: 95.31%.\n",
            "pgd l2: attack effectiveness 32.836%.\n",
            "Mini batch: 253/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.3610 | Train accuracy: 78.12%.\n",
            "pgd l2: attack effectiveness 39.706%.\n",
            "Mini batch: 254/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.3696 | Train accuracy: 77.34%.\n",
            "pgd l2: attack effectiveness 33.871%.\n",
            "Mini batch: 255/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.3752 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 16.364%.\n",
            "Mini batch: 256/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.3265 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 16.667%.\n",
            "Mini batch: 257/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.3014 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 18.033%.\n",
            "Mini batch: 258/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.3305 | Train accuracy: 85.94%.\n",
            "pgd l2: attack effectiveness 25.000%.\n",
            "Mini batch: 259/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.3376 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 23.810%.\n",
            "Mini batch: 260/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.2232 | Train accuracy: 87.23%.\n",
            "Training loss (epoch level): 0.3136 | Train accuracy: 84.50\n",
            "pgd l2: attack effectiveness 27.861%.\n",
            "\tVal accuracy 80.69% with accuracy 72.14% under attack.\n",
            "\tModel select at epoch 23 with validation accuracy 88.03% and accuracy 86.57% under attack.\n",
            "pgd l2: attack effectiveness 18.462%.\n",
            "Mini batch: 261/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.2694 | Train accuracy: 85.94%.\n",
            "pgd l2: attack effectiveness 17.188%.\n",
            "Mini batch: 262/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.2357 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 22.388%.\n",
            "Mini batch: 263/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.2823 | Train accuracy: 83.59%.\n",
            "pgd l2: attack effectiveness 35.294%.\n",
            "Mini batch: 264/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.2983 | Train accuracy: 80.47%.\n",
            "pgd l2: attack effectiveness 33.871%.\n",
            "Mini batch: 265/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.2905 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 23.636%.\n",
            "Mini batch: 266/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.2848 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 25.758%.\n",
            "Mini batch: 267/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.2638 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 24.590%.\n",
            "Mini batch: 268/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.2318 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 26.471%.\n",
            "Mini batch: 269/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.2367 | Train accuracy: 89.84%.\n",
            "pgd l2: attack effectiveness 28.571%.\n",
            "Mini batch: 270/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.1832 | Train accuracy: 89.36%.\n",
            "Training loss (epoch level): 0.2577 | Train accuracy: 86.28\n",
            "pgd l2: attack effectiveness 32.836%.\n",
            "\tVal accuracy 79.21% with accuracy 67.16% under attack.\n",
            "\tModel select at epoch 23 with validation accuracy 88.03% and accuracy 86.57% under attack.\n",
            "pgd l2: attack effectiveness 24.615%.\n",
            "Mini batch: 271/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.2369 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 20.312%.\n",
            "Mini batch: 272/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.3004 | Train accuracy: 89.06%.\n",
            "pgd l2: attack effectiveness 31.343%.\n",
            "Mini batch: 273/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.3424 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 25.000%.\n",
            "Mini batch: 274/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.2392 | Train accuracy: 85.16%.\n",
            "pgd l2: attack effectiveness 8.065%.\n",
            "Mini batch: 275/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.2411 | Train accuracy: 90.62%.\n",
            "pgd l2: attack effectiveness 7.273%.\n",
            "Mini batch: 276/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.3644 | Train accuracy: 82.81%.\n",
            "pgd l2: attack effectiveness 4.545%.\n",
            "Mini batch: 277/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.2170 | Train accuracy: 90.62%.\n",
            "pgd l2: attack effectiveness 6.557%.\n",
            "Mini batch: 278/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.1812 | Train accuracy: 90.62%.\n",
            "pgd l2: attack effectiveness 11.765%.\n",
            "Mini batch: 279/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.2220 | Train accuracy: 88.28%.\n",
            "pgd l2: attack effectiveness 9.524%.\n",
            "Mini batch: 280/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.1078 | Train accuracy: 93.62%.\n",
            "Training loss (epoch level): 0.2453 | Train accuracy: 88.19\n",
            "pgd l2: attack effectiveness 12.438%.\n",
            "\tVal accuracy 89.41% with accuracy 87.56% under attack.\n",
            "\tModel select at epoch 28 with validation accuracy 89.41% and accuracy 87.56% under attack.\n",
            "pgd l2: attack effectiveness 4.615%.\n",
            "Mini batch: 281/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.1048 | Train accuracy: 95.31%.\n",
            "pgd l2: attack effectiveness 12.500%.\n",
            "Mini batch: 282/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.2100 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 7.463%.\n",
            "Mini batch: 283/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.1816 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 7.353%.\n",
            "Mini batch: 284/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.1835 | Train accuracy: 96.09%.\n",
            "pgd l2: attack effectiveness 33.871%.\n",
            "Mini batch: 285/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.3391 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 9.091%.\n",
            "Mini batch: 286/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.2756 | Train accuracy: 94.53%.\n",
            "pgd l2: attack effectiveness 12.121%.\n",
            "Mini batch: 287/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.2684 | Train accuracy: 92.97%.\n",
            "pgd l2: attack effectiveness 13.115%.\n",
            "Mini batch: 288/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.1905 | Train accuracy: 91.41%.\n",
            "pgd l2: attack effectiveness 8.824%.\n",
            "Mini batch: 289/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.2253 | Train accuracy: 92.19%.\n",
            "pgd l2: attack effectiveness 9.524%.\n",
            "Mini batch: 290/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.1446 | Train accuracy: 91.49%.\n",
            "Training loss (epoch level): 0.2123 | Train accuracy: 92.59\n",
            "pgd l2: attack effectiveness 13.433%.\n",
            "\tVal accuracy 88.66% with accuracy 86.57% under attack.\n",
            "\tModel select at epoch 28 with validation accuracy 89.41% and accuracy 87.56% under attack.\n",
            "pgd l2: attack effectiveness 6.154%.\n",
            "Mini batch: 291/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.1713 | Train accuracy: 96.09%.\n",
            "pgd l2: attack effectiveness 14.062%.\n",
            "Mini batch: 292/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.2101 | Train accuracy: 92.97%.\n",
            "pgd l2: attack effectiveness 23.881%.\n",
            "Mini batch: 293/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.3086 | Train accuracy: 87.50%.\n",
            "pgd l2: attack effectiveness 27.941%.\n",
            "Mini batch: 294/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.3208 | Train accuracy: 84.38%.\n",
            "pgd l2: attack effectiveness 9.677%.\n",
            "Mini batch: 295/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.2043 | Train accuracy: 95.31%.\n",
            "pgd l2: attack effectiveness 10.909%.\n",
            "Mini batch: 296/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.2424 | Train accuracy: 92.19%.\n",
            "pgd l2: attack effectiveness 9.091%.\n",
            "Mini batch: 297/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.2048 | Train accuracy: 93.75%.\n",
            "pgd l2: attack effectiveness 14.754%.\n",
            "Mini batch: 298/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.1817 | Train accuracy: 91.41%.\n",
            "pgd l2: attack effectiveness 23.529%.\n",
            "Mini batch: 299/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.2494 | Train accuracy: 86.72%.\n",
            "pgd l2: attack effectiveness 23.810%.\n",
            "Mini batch: 300/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.1926 | Train accuracy: 89.36%.\n",
            "Training loss (epoch level): 0.2286 | Train accuracy: 90.97\n",
            "pgd l2: attack effectiveness 13.930%.\n",
            "\tVal accuracy 88.53% with accuracy 86.07% under attack.\n",
            "\tModel select at epoch 28 with validation accuracy 89.41% and accuracy 87.56% under attack.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test: accuracy\n",
        "max_adv_training_model.load()\n",
        "max_adv_training_model.model.predict(test_Loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_cupu1j6Wp7",
        "outputId": "56a4492a-059d-454b-cbcb-7e059c9cda84"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on the test dataset is 93.75000%\n",
            "The balanced accuracy on the test dataset is 93.71187%\n",
            "Other evaluation metrics we may need:\n",
            "False Negative Rate (FNR) is 2.47525%, False Positive Rate (FPR) is 10.10101%, F1 score is 94.03341%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_adv_training_model.adv_predict(test_Loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVpzTPspCiwR",
        "outputId": "259444b8-2200-4a29-a251-ef12300d5261"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd l2: attack effectiveness 8.911%.\n",
            "\tadversarial accuracy  91.09% under attack.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Norm : L1"
      ],
      "metadata": {
        "id": "hvyxydcghyGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PGDl1():\n",
        "    \"\"\"\n",
        "    Projected gradient descent (ascent) with gradients 'normalized' using l1 norm.\n",
        "    By comparing BCA, the api removal is leveraged\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    @param is_attacker, Boolean, play the role of attacker (note: the defender conducts adversarial training)\n",
        "    @param oblivion, Boolean, whether know the adversary indicator or not\n",
        "    @param kappa, attack confidence\n",
        "    @param manipulation_x, manipulations\n",
        "    @param omega, the indices of interdependent apis corresponding to each api\n",
        "    @param device, 'cpu' or 'cuda'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_attacker=True, oblivion=False, kappa=1., manipulation_x=None, omega=None, api_flag=None, device=None):\n",
        "        super(PGDl1, self).__init__()\n",
        "        self.is_attacker = is_attacker\n",
        "        self.lambda_ = 1.\n",
        "        self.omeag = omega\n",
        "        self.manipulation_x = manipulation_x\n",
        "        self.api_flag = api_flag\n",
        "        self.device = device\n",
        "\n",
        "    def _perturb(self, model, x,  label=None,\n",
        "                 steps=10,\n",
        "                 lambda_=1.):\n",
        "        \"\"\"\n",
        "        perturb node feature vectors\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        @param model, a victim model\n",
        "        @param x: torch.FloatTensor, node feature vectors (each represents the occurrences of apis in a graph) with shape [batch_size, number_of_graphs, vocab_dim]\n",
        "        @param label: torch.LongTensor, ground truth labels\n",
        "        @param steps: Integer, maximum number of perturbations\n",
        "        @param lambda_, float, penalty factor\n",
        "        \"\"\"\n",
        "        adv_x = x\n",
        "        worst_x = x.detach().clone()\n",
        "        self.lambda_ = lambda_\n",
        "        self.padding_mask = torch.sum(adv_x, dim=-1, keepdim=True) > 1  # we set a graph contains two apis at least\n",
        "        model.eval()\n",
        "        for t in range(steps):\n",
        "            var_adv_x = torch.autograd.Variable(adv_x, requires_grad=True)\n",
        "            loss, done = self.get_loss(model, var_adv_x, label, self.lambda_)\n",
        "            worst_x[done] = adv_x[done]\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            grad = torch.autograd.grad(torch.mean(loss), var_adv_x)[0]\n",
        "            perturbation, direction = self.get_perturbation(grad, x, adv_x)\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            perturbation[done] = 0.\n",
        "            adv_x = torch.clamp(adv_x + perturbation * direction, min=0., max=1.)\n",
        "        done = self.get_scores(model, adv_x, label)\n",
        "        worst_x[done] = adv_x[done]\n",
        "        return worst_x\n",
        "\n",
        "    def perturb(self, model, x, label=None,\n",
        "                steps=10,\n",
        "                min_lambda_=1e-5,\n",
        "                max_lambda_=1e5,\n",
        "                base=10.,\n",
        "                verbose=True):\n",
        "        \"\"\"\n",
        "        enhance attack\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            self.lambda_ = min_lambda_\n",
        "        else:\n",
        "            self.lambda_ = max_lambda_\n",
        "        adv_x = x.detach().clone().to(torch.double)\n",
        "        while self.lambda_ <= max_lambda_:\n",
        "            _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            pert_x = self._perturb(model, adv_x[~done], label[~done],\n",
        "                                   steps,\n",
        "                                   lambda_=self.lambda_\n",
        "                                   )\n",
        "            adv_x[~done] = pert_x\n",
        "            self.lambda_ *= base\n",
        "            #if not self.check_lambda(model):\n",
        "                #break\n",
        "        with torch.no_grad():\n",
        "            _, done = self.get_loss(model, adv_x, label, self.lambda_)\n",
        "            if verbose:\n",
        "                print(f\"pgd l1: attack effectiveness {done.sum().item() / x.size()[0]}.\")\n",
        "        return adv_x\n",
        "\n",
        "    def get_perturbation(self, gradients, features, adv_features):\n",
        "        # 1. mask paddings\n",
        "        gradients = gradients * self.padding_mask\n",
        "\n",
        "        # 2. look for allowable position, because only '1--> -' and '0 --> +' are permitted\n",
        "        #    2.1 api insertion\n",
        "        pos_insertion = (adv_features <= 0.5) * 1\n",
        "        grad4insertion = (gradients > 0) * pos_insertion * gradients\n",
        "        #    2.2 api removal\n",
        "        pos_removal = (adv_features > 0.5) * 1\n",
        "        grad4removal = (gradients <= 0) * (pos_removal & self.manipulation_x) * gradients\n",
        "        if self.is_attacker:\n",
        "            #     2.2.1 cope with the interdependent apis\n",
        "            checking_nonexist_api = (pos_removal ^ self.omega) & self.omega\n",
        "            grad4removal[:, self.api_flag] += torch.sum(gradients * checking_nonexist_api, dim=-1, keepdim=True)\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # 3. remove duplications\n",
        "        un_mod = torch.abs(features - adv_features) <= 1e-6\n",
        "        gradients = gradients * un_mod\n",
        "\n",
        "        # 4. look for important position\n",
        "        absolute_grad = torch.abs(gradients).reshape(features.shape[0], -1)\n",
        "        _, position = torch.max(absolute_grad, dim=-1)\n",
        "        perturbations = F.one_hot(position, num_classes=absolute_grad.shape[-1]).double()\n",
        "        perturbations = perturbations.reshape(features.shape)\n",
        "        directions = torch.sign(gradients) * (perturbations > 1e-6)\n",
        "\n",
        "        # 5. tailor the interdependent apis\n",
        "        if self.is_attacker:\n",
        "            perturbations += (torch.any(directions[:, self.api_flag] < 0, dim=-1, keepdim=True)) * checking_nonexist_api\n",
        "            directions += perturbations * self.omega\n",
        "        return perturbations, directions\n",
        "\n",
        "\n",
        "    def get_loss(self, model, adv_x, label, lambda_=None):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(adv_x)\n",
        "        else:\n",
        "            logits_f = model.forward(adv_x)\n",
        "\n",
        "        ce = F.cross_entropy(logits_f, label, reduction='none')\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            assert lambda_ is not None\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            if self.is_attacker:\n",
        "                loss_no_reduction = ce + lambda_ * (torch.clamp(tau - prob_g,\n",
        "                                                                max=self.kappa)\n",
        "                                                    )\n",
        "            else:\n",
        "                loss_no_reduction = ce + lambda_ * (tau - prob_g)\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            loss_no_reduction = ce\n",
        "            done = y_pred != label\n",
        "        return loss_no_reduction, done\n",
        "\n",
        "    def get_scores(self, model, pertb_x, label, lmda=1.):\n",
        "        if hasattr(model, 'is_detector_enabled'):\n",
        "            logits_f, prob_g = model.forward(pertb_x)\n",
        "        else:\n",
        "            logits_f = model.forward(pertb_x)\n",
        "        y_pred = logits_f.argmax(1)\n",
        "        if hasattr(model, 'is_detector_enabled') and (not self.oblivion):\n",
        "            tau = model.get_tau_sample_wise(y_pred)\n",
        "            done = (y_pred != label) & (prob_g <= tau)\n",
        "        else:\n",
        "            done = y_pred != label\n",
        "        return done"
      ],
      "metadata": {
        "id": "q-FgY4O2hy2g"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def __init__(self, is_attacker=True, oblivion=False, kappa=1., manipulation_x=None, omega=None, device=None):\n",
        "        super(PGDl1, self).__init__(is_attacker, oblivion, kappa, manipulation_x, omega, device)"
      ],
      "metadata": {
        "id": "_9CCrkXhjtgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MalwareDetectionDNN(3216,2)\n",
        "model = model.double().to(device)\n",
        "attack = PGDl1(is_attacker=False,manipulation_x=manipulation_x, omega=omega,api_flag=api_flag,device=device)\n",
        "attack_param = {}\n",
        "max_adv_training_model = PGDAdvTraining(model, attack, attack_param)\n",
        "max_adv_training_model.fit(train_Loader,validation_Loader,adv_epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuBfuwG2jkEA",
        "outputId": "0d2d44b3-241c-4d0c-883a-ce2df4551db8"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 1/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.6906 | Train accuracy: 51.56%.\n",
            "pgd l1: attack effectiveness 0.65625.\n",
            "Mini batch: 2/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.5953 | Train accuracy: 71.09%.\n",
            "pgd l1: attack effectiveness 0.4925373134328358.\n",
            "Mini batch: 3/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.4279 | Train accuracy: 75.00%.\n",
            "pgd l1: attack effectiveness 0.39705882352941174.\n",
            "Mini batch: 4/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.3318 | Train accuracy: 82.81%.\n",
            "pgd l1: attack effectiveness 0.14516129032258066.\n",
            "Mini batch: 5/300 | training time in 0 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.3323 | Train accuracy: 84.38%.\n",
            "pgd l1: attack effectiveness 0.2.\n",
            "Mini batch: 6/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.4006 | Train accuracy: 81.25%.\n",
            "pgd l1: attack effectiveness 0.3181818181818182.\n",
            "Mini batch: 7/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.3173 | Train accuracy: 82.81%.\n",
            "pgd l1: attack effectiveness 0.11475409836065574.\n",
            "Mini batch: 8/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.2305 | Train accuracy: 89.06%.\n",
            "pgd l1: attack effectiveness 0.1323529411764706.\n",
            "Mini batch: 9/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.3457 | Train accuracy: 82.03%.\n",
            "pgd l1: attack effectiveness 0.23809523809523808.\n",
            "Mini batch: 10/300 | training time in 0 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.3498 | Train accuracy: 80.85%.\n",
            "Training loss (epoch level): 0.4022 | Train accuracy: 78.09\n",
            "pgd l1: attack effectiveness 0.38308457711442784.\n",
            "\tVal accuracy 75.97% with accuracy 61.69% under attack.\n",
            "\tModel select at epoch 1 with validation accuracy 75.97% and accuracy 61.69% under attack.\n",
            "pgd l1: attack effectiveness 0.36923076923076925.\n",
            "Mini batch: 11/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.3096 | Train accuracy: 82.81%.\n",
            "pgd l1: attack effectiveness 0.109375.\n",
            "Mini batch: 12/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.2492 | Train accuracy: 86.72%.\n",
            "pgd l1: attack effectiveness 0.2835820895522388.\n",
            "Mini batch: 13/300 | training time in 0 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.2365 | Train accuracy: 88.28%.\n",
            "pgd l1: attack effectiveness 0.45588235294117646.\n",
            "Mini batch: 14/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.2623 | Train accuracy: 81.25%.\n",
            "pgd l1: attack effectiveness 0.20967741935483872.\n",
            "Mini batch: 15/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.1707 | Train accuracy: 89.06%.\n",
            "pgd l1: attack effectiveness 0.2.\n",
            "Mini batch: 16/300 | training time in 0 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.1906 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.16666666666666666.\n",
            "Mini batch: 17/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.1400 | Train accuracy: 92.19%.\n",
            "pgd l1: attack effectiveness 0.18032786885245902.\n",
            "Mini batch: 18/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.1643 | Train accuracy: 89.84%.\n",
            "pgd l1: attack effectiveness 0.22058823529411764.\n",
            "Mini batch: 19/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.2774 | Train accuracy: 88.28%.\n",
            "pgd l1: attack effectiveness 0.23809523809523808.\n",
            "Mini batch: 20/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.2669 | Train accuracy: 85.11%.\n",
            "Training loss (epoch level): 0.2268 | Train accuracy: 87.50\n",
            "pgd l1: attack effectiveness 0.13432835820895522.\n",
            "\tVal accuracy 89.16% with accuracy 86.57% under attack.\n",
            "\tModel select at epoch 2 with validation accuracy 89.16% and accuracy 86.57% under attack.\n",
            "pgd l1: attack effectiveness 0.1076923076923077.\n",
            "Mini batch: 21/300 | training time in 0 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.2199 | Train accuracy: 89.84%.\n",
            "pgd l1: attack effectiveness 0.0625.\n",
            "Mini batch: 22/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.1822 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.05970149253731343.\n",
            "Mini batch: 23/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.2438 | Train accuracy: 85.94%.\n",
            "pgd l1: attack effectiveness 0.07352941176470588.\n",
            "Mini batch: 24/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.1134 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.24193548387096775.\n",
            "Mini batch: 25/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.1833 | Train accuracy: 86.72%.\n",
            "pgd l1: attack effectiveness 0.36363636363636365.\n",
            "Mini batch: 26/300 | training time in 0 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.1956 | Train accuracy: 85.16%.\n",
            "pgd l1: attack effectiveness 0.3181818181818182.\n",
            "Mini batch: 27/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.1634 | Train accuracy: 88.28%.\n",
            "pgd l1: attack effectiveness 0.26229508196721313.\n",
            "Mini batch: 28/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.1687 | Train accuracy: 89.84%.\n",
            "pgd l1: attack effectiveness 0.16176470588235295.\n",
            "Mini batch: 29/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.2012 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.19047619047619047.\n",
            "Mini batch: 30/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.2015 | Train accuracy: 87.23%.\n",
            "Training loss (epoch level): 0.1873 | Train accuracy: 89.04\n",
            "pgd l1: attack effectiveness 0.11442786069651742.\n",
            "\tVal accuracy 90.4% with accuracy 88.56% under attack.\n",
            "\tModel select at epoch 3 with validation accuracy 90.4% and accuracy 88.56% under attack.\n",
            "pgd l1: attack effectiveness 0.09230769230769231.\n",
            "Mini batch: 31/300 | training time in 0 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.1598 | Train accuracy: 92.19%.\n",
            "pgd l1: attack effectiveness 0.046875.\n",
            "Mini batch: 32/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.1069 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.05970149253731343.\n",
            "Mini batch: 33/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.1695 | Train accuracy: 89.84%.\n",
            "pgd l1: attack effectiveness 0.04411764705882353.\n",
            "Mini batch: 34/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.1595 | Train accuracy: 90.62%.\n",
            "pgd l1: attack effectiveness 0.11290322580645161.\n",
            "Mini batch: 35/300 | training time in 0 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.1758 | Train accuracy: 88.28%.\n",
            "pgd l1: attack effectiveness 0.10909090909090909.\n",
            "Mini batch: 36/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.1796 | Train accuracy: 89.84%.\n",
            "pgd l1: attack effectiveness 0.09090909090909091.\n",
            "Mini batch: 37/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.1021 | Train accuracy: 92.19%.\n",
            "pgd l1: attack effectiveness 0.11475409836065574.\n",
            "Mini batch: 38/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.1131 | Train accuracy: 93.75%.\n",
            "pgd l1: attack effectiveness 0.4117647058823529.\n",
            "Mini batch: 39/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.2535 | Train accuracy: 82.81%.\n",
            "pgd l1: attack effectiveness 0.19047619047619047.\n",
            "Mini batch: 40/300 | training time in 0 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.1601 | Train accuracy: 91.49%.\n",
            "Training loss (epoch level): 0.1580 | Train accuracy: 90.63\n",
            "pgd l1: attack effectiveness 0.24378109452736318.\n",
            "\tVal accuracy 85.19% with accuracy 75.62% under attack.\n",
            "\tModel select at epoch 3 with validation accuracy 90.4% and accuracy 88.56% under attack.\n",
            "pgd l1: attack effectiveness 0.16923076923076924.\n",
            "Mini batch: 41/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.1631 | Train accuracy: 90.62%.\n",
            "pgd l1: attack effectiveness 0.109375.\n",
            "Mini batch: 42/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0764 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.07462686567164178.\n",
            "Mini batch: 43/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0723 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.058823529411764705.\n",
            "Mini batch: 44/300 | training time in 0 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0815 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.08064516129032258.\n",
            "Mini batch: 45/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.1163 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.05454545454545454.\n",
            "Mini batch: 46/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.1650 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.06060606060606061.\n",
            "Mini batch: 47/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.1174 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.06557377049180328.\n",
            "Mini batch: 48/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.1009 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.11764705882352941.\n",
            "Mini batch: 49/300 | training time in 0 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.1363 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.09523809523809523.\n",
            "Mini batch: 50/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0833 | Train accuracy: 95.74%.\n",
            "Training loss (epoch level): 0.1112 | Train accuracy: 93.32\n",
            "pgd l1: attack effectiveness 0.23880597014925373.\n",
            "\tVal accuracy 85.43% with accuracy 76.12% under attack.\n",
            "\tModel select at epoch 3 with validation accuracy 90.4% and accuracy 88.56% under attack.\n",
            "pgd l1: attack effectiveness 0.16923076923076924.\n",
            "Mini batch: 51/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.1447 | Train accuracy: 90.62%.\n",
            "pgd l1: attack effectiveness 0.234375.\n",
            "Mini batch: 52/300 | training time in 0 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.1127 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.08955223880597014.\n",
            "Mini batch: 53/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0480 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.058823529411764705.\n",
            "Mini batch: 54/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0464 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.11290322580645161.\n",
            "Mini batch: 55/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0812 | Train accuracy: 93.75%.\n",
            "pgd l1: attack effectiveness 0.07272727272727272.\n",
            "Mini batch: 56/300 | training time in 0 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0822 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.06060606060606061.\n",
            "Mini batch: 57/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0687 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.04918032786885246.\n",
            "Mini batch: 58/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0686 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.07352941176470588.\n",
            "Mini batch: 59/300 | training time in 0 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.1214 | Train accuracy: 93.75%.\n",
            "pgd l1: attack effectiveness 0.09523809523809523.\n",
            "Mini batch: 60/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0704 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0844 | Train accuracy: 94.94\n",
            "pgd l1: attack effectiveness 0.0945273631840796.\n",
            "\tVal accuracy 92.52% with accuracy 90.55% under attack.\n",
            "\tModel select at epoch 6 with validation accuracy 92.52% and accuracy 90.55% under attack.\n",
            "pgd l1: attack effectiveness 0.06153846153846154.\n",
            "Mini batch: 61/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0847 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.09375.\n",
            "Mini batch: 62/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0640 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.14925373134328357.\n",
            "Mini batch: 63/300 | training time in 0 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0905 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.014705882352941176.\n",
            "Mini batch: 64/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0300 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.08064516129032258.\n",
            "Mini batch: 65/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0607 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.07272727272727272.\n",
            "Mini batch: 66/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0603 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.06060606060606061.\n",
            "Mini batch: 67/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0597 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.06557377049180328.\n",
            "Mini batch: 68/300 | training time in 0 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0889 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.07352941176470588.\n",
            "Mini batch: 69/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0982 | Train accuracy: 93.75%.\n",
            "pgd l1: attack effectiveness 0.09523809523809523.\n",
            "Mini batch: 70/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0829 | Train accuracy: 93.62%.\n",
            "Training loss (epoch level): 0.0720 | Train accuracy: 95.22\n",
            "pgd l1: attack effectiveness 0.0845771144278607.\n",
            "\tVal accuracy 92.77% with accuracy 91.54% under attack.\n",
            "\tModel select at epoch 7 with validation accuracy 92.77% and accuracy 91.54% under attack.\n",
            "pgd l1: attack effectiveness 0.046153846153846156.\n",
            "Mini batch: 71/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0929 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.03125.\n",
            "Mini batch: 72/300 | training time in 0 minutes, 16 seconds.\n",
            "Training loss (batch level): 0.0618 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.014925373134328358.\n",
            "Mini batch: 73/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0583 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.014705882352941176.\n",
            "Mini batch: 74/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0343 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.08064516129032258.\n",
            "Mini batch: 75/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0583 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.07272727272727272.\n",
            "Mini batch: 76/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0596 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.12121212121212122.\n",
            "Mini batch: 77/300 | training time in 0 minutes, 17 seconds.\n",
            "Training loss (batch level): 0.0804 | Train accuracy: 93.75%.\n",
            "pgd l1: attack effectiveness 0.06557377049180328.\n",
            "Mini batch: 78/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0622 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.07352941176470588.\n",
            "Mini batch: 79/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0948 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 80/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0379 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0641 | Train accuracy: 96.25\n",
            "pgd l1: attack effectiveness 0.07960199004975124.\n",
            "\tVal accuracy 93.27% with accuracy 92.04% under attack.\n",
            "\tModel select at epoch 8 with validation accuracy 93.27% and accuracy 92.04% under attack.\n",
            "pgd l1: attack effectiveness 0.046153846153846156.\n",
            "Mini batch: 81/300 | training time in 0 minutes, 18 seconds.\n",
            "Training loss (batch level): 0.0675 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.03125.\n",
            "Mini batch: 82/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0451 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.014925373134328358.\n",
            "Mini batch: 83/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0418 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 84/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0217 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.08064516129032258.\n",
            "Mini batch: 85/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0503 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.05454545454545454.\n",
            "Mini batch: 86/300 | training time in 0 minutes, 19 seconds.\n",
            "Training loss (batch level): 0.0509 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.06060606060606061.\n",
            "Mini batch: 87/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0608 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 88/300 | training time in 0 minutes, 20 seconds.\n",
            "Training loss (batch level): 0.0333 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.029411764705882353.\n",
            "Mini batch: 89/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0740 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 90/300 | training time in 0 minutes, 21 seconds.\n",
            "Training loss (batch level): 0.0392 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0485 | Train accuracy: 97.11\n",
            "pgd l1: attack effectiveness 0.12437810945273632.\n",
            "\tVal accuracy 91.53% with accuracy 87.56% under attack.\n",
            "\tModel select at epoch 8 with validation accuracy 93.27% and accuracy 92.04% under attack.\n",
            "pgd l1: attack effectiveness 0.046153846153846156.\n",
            "Mini batch: 91/300 | training time in 0 minutes, 22 seconds.\n",
            "Training loss (batch level): 0.0729 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.0625.\n",
            "Mini batch: 92/300 | training time in 0 minutes, 23 seconds.\n",
            "Training loss (batch level): 0.0411 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.14925373134328357.\n",
            "Mini batch: 93/300 | training time in 0 minutes, 24 seconds.\n",
            "Training loss (batch level): 0.0745 | Train accuracy: 93.75%.\n",
            "pgd l1: attack effectiveness 0.029411764705882353.\n",
            "Mini batch: 94/300 | training time in 0 minutes, 25 seconds.\n",
            "Training loss (batch level): 0.0202 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.0967741935483871.\n",
            "Mini batch: 95/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0525 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.01818181818181818.\n",
            "Mini batch: 96/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0341 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.06060606060606061.\n",
            "Mini batch: 97/300 | training time in 0 minutes, 26 seconds.\n",
            "Training loss (batch level): 0.0633 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 98/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0298 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.014705882352941176.\n",
            "Mini batch: 99/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0804 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 100/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0495 | Train accuracy: 95.74%.\n",
            "Training loss (epoch level): 0.0518 | Train accuracy: 96.68\n",
            "pgd l1: attack effectiveness 0.0845771144278607.\n",
            "\tVal accuracy 92.77% with accuracy 91.54% under attack.\n",
            "\tModel select at epoch 8 with validation accuracy 93.27% and accuracy 92.04% under attack.\n",
            "pgd l1: attack effectiveness 0.046153846153846156.\n",
            "Mini batch: 101/300 | training time in 0 minutes, 27 seconds.\n",
            "Training loss (batch level): 0.0635 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.015625.\n",
            "Mini batch: 102/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0314 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.1044776119402985.\n",
            "Mini batch: 103/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0546 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.058823529411764705.\n",
            "Mini batch: 104/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0252 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.04838709677419355.\n",
            "Mini batch: 105/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0372 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.07272727272727272.\n",
            "Mini batch: 106/300 | training time in 0 minutes, 28 seconds.\n",
            "Training loss (batch level): 0.0555 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.06060606060606061.\n",
            "Mini batch: 107/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0579 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 108/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0307 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.058823529411764705.\n",
            "Mini batch: 109/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0765 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 110/300 | training time in 0 minutes, 29 seconds.\n",
            "Training loss (batch level): 0.0537 | Train accuracy: 95.74%.\n",
            "Training loss (epoch level): 0.0486 | Train accuracy: 96.76\n",
            "pgd l1: attack effectiveness 0.05970149253731343.\n",
            "\tVal accuracy 93.64% with accuracy 94.03% under attack.\n",
            "\tModel select at epoch 11 with validation accuracy 93.64% and accuracy 94.03% under attack.\n",
            "pgd l1: attack effectiveness 0.03076923076923077.\n",
            "Mini batch: 111/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0567 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.015625.\n",
            "Mini batch: 112/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0384 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 113/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0217 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 114/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0114 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.16129032258064516.\n",
            "Mini batch: 115/300 | training time in 0 minutes, 30 seconds.\n",
            "Training loss (batch level): 0.0811 | Train accuracy: 92.97%.\n",
            "pgd l1: attack effectiveness 0.03636363636363636.\n",
            "Mini batch: 116/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0363 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.12121212121212122.\n",
            "Mini batch: 117/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0741 | Train accuracy: 92.19%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 118/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0269 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.08823529411764706.\n",
            "Mini batch: 119/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0973 | Train accuracy: 93.75%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 120/300 | training time in 0 minutes, 31 seconds.\n",
            "Training loss (batch level): 0.0383 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0482 | Train accuracy: 96.43\n",
            "pgd l1: attack effectiveness 0.04975124378109453.\n",
            "\tVal accuracy 93.89% with accuracy 95.02% under attack.\n",
            "\tModel select at epoch 12 with validation accuracy 93.89% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.03076923076923077.\n",
            "Mini batch: 121/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0780 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 122/300 | training time in 0 minutes, 32 seconds.\n",
            "Training loss (batch level): 0.0411 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 123/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0694 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 124/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0213 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.03225806451612903.\n",
            "Mini batch: 125/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0261 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.09090909090909091.\n",
            "Mini batch: 126/300 | training time in 0 minutes, 33 seconds.\n",
            "Training loss (batch level): 0.0408 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.13636363636363635.\n",
            "Mini batch: 127/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0771 | Train accuracy: 93.75%.\n",
            "pgd l1: attack effectiveness 0.26229508196721313.\n",
            "Mini batch: 128/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.1216 | Train accuracy: 92.19%.\n",
            "pgd l1: attack effectiveness 0.11764705882352941.\n",
            "Mini batch: 129/300 | training time in 0 minutes, 34 seconds.\n",
            "Training loss (batch level): 0.0946 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.09523809523809523.\n",
            "Mini batch: 130/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0435 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0613 | Train accuracy: 95.65\n",
            "pgd l1: attack effectiveness 0.0845771144278607.\n",
            "\tVal accuracy 92.77% with accuracy 91.54% under attack.\n",
            "\tModel select at epoch 12 with validation accuracy 93.89% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.046153846153846156.\n",
            "Mini batch: 131/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0677 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 132/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0356 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 133/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0706 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 134/300 | training time in 0 minutes, 35 seconds.\n",
            "Training loss (batch level): 0.0414 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.016129032258064516.\n",
            "Mini batch: 135/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0415 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.01818181818181818.\n",
            "Mini batch: 136/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0550 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.030303030303030304.\n",
            "Mini batch: 137/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0461 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.09836065573770492.\n",
            "Mini batch: 138/300 | training time in 0 minutes, 36 seconds.\n",
            "Training loss (batch level): 0.0491 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.16176470588235295.\n",
            "Mini batch: 139/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.1132 | Train accuracy: 91.41%.\n",
            "pgd l1: attack effectiveness 0.38095238095238093.\n",
            "Mini batch: 140/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.1550 | Train accuracy: 89.36%.\n",
            "Training loss (epoch level): 0.0675 | Train accuracy: 95.73\n",
            "pgd l1: attack effectiveness 0.0845771144278607.\n",
            "\tVal accuracy 93.65% with accuracy 91.54% under attack.\n",
            "\tModel select at epoch 12 with validation accuracy 93.89% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.06153846153846154.\n",
            "Mini batch: 141/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0557 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 142/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0267 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 143/300 | training time in 0 minutes, 37 seconds.\n",
            "Training loss (batch level): 0.0734 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 144/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0833 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.016129032258064516.\n",
            "Mini batch: 145/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0742 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.01818181818181818.\n",
            "Mini batch: 146/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0870 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.030303030303030304.\n",
            "Mini batch: 147/300 | training time in 0 minutes, 38 seconds.\n",
            "Training loss (batch level): 0.0450 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.19672131147540983.\n",
            "Mini batch: 148/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.1016 | Train accuracy: 90.62%.\n",
            "pgd l1: attack effectiveness 0.20588235294117646.\n",
            "Mini batch: 149/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.1071 | Train accuracy: 90.62%.\n",
            "pgd l1: attack effectiveness 0.19047619047619047.\n",
            "Mini batch: 150/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0889 | Train accuracy: 95.74%.\n",
            "Training loss (epoch level): 0.0743 | Train accuracy: 95.43\n",
            "pgd l1: attack effectiveness 0.13432835820895522.\n",
            "\tVal accuracy 90.66% with accuracy 86.57% under attack.\n",
            "\tModel select at epoch 12 with validation accuracy 93.89% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.07692307692307693.\n",
            "Mini batch: 151/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0556 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.0625.\n",
            "Mini batch: 152/300 | training time in 0 minutes, 39 seconds.\n",
            "Training loss (batch level): 0.0437 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.014925373134328358.\n",
            "Mini batch: 153/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0266 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.014705882352941176.\n",
            "Mini batch: 154/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0217 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.04838709677419355.\n",
            "Mini batch: 155/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0672 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 156/300 | training time in 0 minutes, 40 seconds.\n",
            "Training loss (batch level): 0.0974 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.015151515151515152.\n",
            "Mini batch: 157/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0687 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 158/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0577 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.07352941176470588.\n",
            "Mini batch: 159/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0784 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.09523809523809523.\n",
            "Mini batch: 160/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0535 | Train accuracy: 95.74%.\n",
            "Training loss (epoch level): 0.0571 | Train accuracy: 96.61\n",
            "pgd l1: attack effectiveness 0.11940298507462686.\n",
            "\tVal accuracy 92.15% with accuracy 88.06% under attack.\n",
            "\tModel select at epoch 12 with validation accuracy 93.89% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.046153846153846156.\n",
            "Mini batch: 161/300 | training time in 0 minutes, 41 seconds.\n",
            "Training loss (batch level): 0.0477 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.140625.\n",
            "Mini batch: 162/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0699 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.14925373134328357.\n",
            "Mini batch: 163/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0670 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.04411764705882353.\n",
            "Mini batch: 164/300 | training time in 0 minutes, 42 seconds.\n",
            "Training loss (batch level): 0.0287 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.06451612903225806.\n",
            "Mini batch: 165/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0356 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.03636363636363636.\n",
            "Mini batch: 166/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0448 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.030303030303030304.\n",
            "Mini batch: 167/300 | training time in 0 minutes, 43 seconds.\n",
            "Training loss (batch level): 0.0297 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 168/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0320 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.029411764705882353.\n",
            "Mini batch: 169/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0615 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 170/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0228 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0440 | Train accuracy: 96.66\n",
            "pgd l1: attack effectiveness 0.07960199004975124.\n",
            "\tVal accuracy 93.77% with accuracy 92.04% under attack.\n",
            "\tModel select at epoch 12 with validation accuracy 93.89% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.046153846153846156.\n",
            "Mini batch: 171/300 | training time in 0 minutes, 44 seconds.\n",
            "Training loss (batch level): 0.0692 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.015625.\n",
            "Mini batch: 172/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0216 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.014925373134328358.\n",
            "Mini batch: 173/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0226 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 174/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0092 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.03225806451612903.\n",
            "Mini batch: 175/300 | training time in 0 minutes, 45 seconds.\n",
            "Training loss (batch level): 0.0224 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.01818181818181818.\n",
            "Mini batch: 176/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0256 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.06060606060606061.\n",
            "Mini batch: 177/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0471 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 178/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0245 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.04411764705882353.\n",
            "Mini batch: 179/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0690 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 180/300 | training time in 0 minutes, 46 seconds.\n",
            "Training loss (batch level): 0.0297 | Train accuracy: 100.00%.\n",
            "Training loss (epoch level): 0.0341 | Train accuracy: 97.66\n",
            "pgd l1: attack effectiveness 0.1691542288557214.\n",
            "\tVal accuracy 88.79% with accuracy 83.08% under attack.\n",
            "\tModel select at epoch 12 with validation accuracy 93.89% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.13846153846153847.\n",
            "Mini batch: 181/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0843 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.03125.\n",
            "Mini batch: 182/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0356 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 183/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0176 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 184/300 | training time in 0 minutes, 47 seconds.\n",
            "Training loss (batch level): 0.0111 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.03225806451612903.\n",
            "Mini batch: 185/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0253 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.01818181818181818.\n",
            "Mini batch: 186/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0402 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.015151515151515152.\n",
            "Mini batch: 187/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0396 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.01639344262295082.\n",
            "Mini batch: 188/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0310 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.17647058823529413.\n",
            "Mini batch: 189/300 | training time in 0 minutes, 48 seconds.\n",
            "Training loss (batch level): 0.0985 | Train accuracy: 93.75%.\n",
            "pgd l1: attack effectiveness 0.23809523809523808.\n",
            "Mini batch: 190/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0862 | Train accuracy: 95.74%.\n",
            "Training loss (epoch level): 0.0469 | Train accuracy: 96.92\n",
            "pgd l1: attack effectiveness 0.15422885572139303.\n",
            "\tVal accuracy 90.29% with accuracy 84.58% under attack.\n",
            "\tModel select at epoch 12 with validation accuracy 93.89% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.046153846153846156.\n",
            "Mini batch: 191/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0455 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.046875.\n",
            "Mini batch: 192/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0359 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.13432835820895522.\n",
            "Mini batch: 193/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0498 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 194/300 | training time in 0 minutes, 49 seconds.\n",
            "Training loss (batch level): 0.0016 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.06451612903225806.\n",
            "Mini batch: 195/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0375 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.05454545454545454.\n",
            "Mini batch: 196/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0254 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.030303030303030304.\n",
            "Mini batch: 197/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0306 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 198/300 | training time in 0 minutes, 50 seconds.\n",
            "Training loss (batch level): 0.0257 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.014705882352941176.\n",
            "Mini batch: 199/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0510 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 200/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0358 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0339 | Train accuracy: 97.44\n",
            "pgd l1: attack effectiveness 0.04975124378109453.\n",
            "\tVal accuracy 94.39% with accuracy 95.02% under attack.\n",
            "\tModel select at epoch 20 with validation accuracy 94.39% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.03076923076923077.\n",
            "Mini batch: 201/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0487 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 202/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0230 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 203/300 | training time in 0 minutes, 51 seconds.\n",
            "Training loss (batch level): 0.0141 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 204/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0140 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.016129032258064516.\n",
            "Mini batch: 205/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0141 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 206/300 | training time in 0 minutes, 52 seconds.\n",
            "Training loss (batch level): 0.0212 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.015151515151515152.\n",
            "Mini batch: 207/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0354 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.01639344262295082.\n",
            "Mini batch: 208/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0182 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.058823529411764705.\n",
            "Mini batch: 209/300 | training time in 0 minutes, 53 seconds.\n",
            "Training loss (batch level): 0.0602 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.09523809523809523.\n",
            "Mini batch: 210/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0400 | Train accuracy: 95.74%.\n",
            "Training loss (epoch level): 0.0289 | Train accuracy: 97.78\n",
            "pgd l1: attack effectiveness 0.18407960199004975.\n",
            "\tVal accuracy 88.92% with accuracy 81.59% under attack.\n",
            "\tModel select at epoch 20 with validation accuracy 94.39% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.13846153846153847.\n",
            "Mini batch: 211/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0831 | Train accuracy: 92.97%.\n",
            "pgd l1: attack effectiveness 0.15625.\n",
            "Mini batch: 212/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0794 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.014925373134328358.\n",
            "Mini batch: 213/300 | training time in 0 minutes, 54 seconds.\n",
            "Training loss (batch level): 0.0113 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 214/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0015 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.06451612903225806.\n",
            "Mini batch: 215/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0351 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.05454545454545454.\n",
            "Mini batch: 216/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0375 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.045454545454545456.\n",
            "Mini batch: 217/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0331 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.01639344262295082.\n",
            "Mini batch: 218/300 | training time in 0 minutes, 55 seconds.\n",
            "Training loss (batch level): 0.0185 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.014705882352941176.\n",
            "Mini batch: 219/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0368 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 220/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0429 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0379 | Train accuracy: 96.90\n",
            "pgd l1: attack effectiveness 0.05472636815920398.\n",
            "\tVal accuracy 94.14% with accuracy 94.53% under attack.\n",
            "\tModel select at epoch 20 with validation accuracy 94.39% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.03076923076923077.\n",
            "Mini batch: 221/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0389 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 222/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0195 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 223/300 | training time in 0 minutes, 56 seconds.\n",
            "Training loss (batch level): 0.0148 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 224/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0053 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 225/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0179 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.01818181818181818.\n",
            "Mini batch: 226/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0365 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.015151515151515152.\n",
            "Mini batch: 227/300 | training time in 0 minutes, 57 seconds.\n",
            "Training loss (batch level): 0.0295 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.01639344262295082.\n",
            "Mini batch: 228/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0231 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.029411764705882353.\n",
            "Mini batch: 229/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0500 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 230/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0242 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0260 | Train accuracy: 98.22\n",
            "pgd l1: attack effectiveness 0.11940298507462686.\n",
            "\tVal accuracy 92.28% with accuracy 88.06% under attack.\n",
            "\tModel select at epoch 20 with validation accuracy 94.39% and accuracy 95.02% under attack.\n",
            "pgd l1: attack effectiveness 0.03076923076923077.\n",
            "Mini batch: 231/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0311 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.03125.\n",
            "Mini batch: 232/300 | training time in 0 minutes, 58 seconds.\n",
            "Training loss (batch level): 0.0324 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.04477611940298507.\n",
            "Mini batch: 233/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0176 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.04411764705882353.\n",
            "Mini batch: 234/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0210 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.11290322580645161.\n",
            "Mini batch: 235/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0639 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.05454545454545454.\n",
            "Mini batch: 236/300 | training time in 0 minutes, 59 seconds.\n",
            "Training loss (batch level): 0.0306 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.06060606060606061.\n",
            "Mini batch: 237/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0527 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 238/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0245 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.014705882352941176.\n",
            "Mini batch: 239/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0360 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 240/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0389 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0349 | Train accuracy: 97.37\n",
            "pgd l1: attack effectiveness 0.029850746268656716.\n",
            "\tVal accuracy 94.76% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 24 with validation accuracy 94.76% and accuracy 97.01% under attack.\n",
            "pgd l1: attack effectiveness 0.015384615384615385.\n",
            "Mini batch: 241/300 | training time in 1 minutes, 0 seconds.\n",
            "Training loss (batch level): 0.0491 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 242/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0194 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 243/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0177 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 244/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0080 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.016129032258064516.\n",
            "Mini batch: 245/300 | training time in 1 minutes, 1 seconds.\n",
            "Training loss (batch level): 0.0216 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 246/300 | training time in 1 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0351 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.015151515151515152.\n",
            "Mini batch: 247/300 | training time in 1 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0391 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.08196721311475409.\n",
            "Mini batch: 248/300 | training time in 1 minutes, 2 seconds.\n",
            "Training loss (batch level): 0.0417 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.029411764705882353.\n",
            "Mini batch: 249/300 | training time in 1 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0352 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.09523809523809523.\n",
            "Mini batch: 250/300 | training time in 1 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0441 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0311 | Train accuracy: 98.22\n",
            "pgd l1: attack effectiveness 0.07462686567164178.\n",
            "\tVal accuracy 94.27% with accuracy 92.54% under attack.\n",
            "\tModel select at epoch 24 with validation accuracy 94.76% and accuracy 97.01% under attack.\n",
            "pgd l1: attack effectiveness 0.03076923076923077.\n",
            "Mini batch: 251/300 | training time in 1 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0309 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 252/300 | training time in 1 minutes, 3 seconds.\n",
            "Training loss (batch level): 0.0101 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 253/300 | training time in 1 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0025 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 254/300 | training time in 1 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0004 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.016129032258064516.\n",
            "Mini batch: 255/300 | training time in 1 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0134 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.05454545454545454.\n",
            "Mini batch: 256/300 | training time in 1 minutes, 4 seconds.\n",
            "Training loss (batch level): 0.0233 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.15151515151515152.\n",
            "Mini batch: 257/300 | training time in 1 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0709 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.06557377049180328.\n",
            "Mini batch: 258/300 | training time in 1 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0371 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.058823529411764705.\n",
            "Mini batch: 259/300 | training time in 1 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0600 | Train accuracy: 95.31%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 260/300 | training time in 1 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0232 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0272 | Train accuracy: 97.91\n",
            "pgd l1: attack effectiveness 0.06965174129353234.\n",
            "\tVal accuracy 94.02% with accuracy 93.03% under attack.\n",
            "\tModel select at epoch 24 with validation accuracy 94.76% and accuracy 97.01% under attack.\n",
            "pgd l1: attack effectiveness 0.03076923076923077.\n",
            "Mini batch: 261/300 | training time in 1 minutes, 5 seconds.\n",
            "Training loss (batch level): 0.0371 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 262/300 | training time in 1 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0180 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 263/300 | training time in 1 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0227 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 264/300 | training time in 1 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0168 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 265/300 | training time in 1 minutes, 6 seconds.\n",
            "Training loss (batch level): 0.0236 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 266/300 | training time in 1 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0280 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 267/300 | training time in 1 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0212 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.01639344262295082.\n",
            "Mini batch: 268/300 | training time in 1 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0183 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.029411764705882353.\n",
            "Mini batch: 269/300 | training time in 1 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0365 | Train accuracy: 96.09%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 270/300 | training time in 1 minutes, 7 seconds.\n",
            "Training loss (batch level): 0.0265 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0249 | Train accuracy: 97.91\n",
            "pgd l1: attack effectiveness 0.10945273631840796.\n",
            "\tVal accuracy 92.53% with accuracy 89.05% under attack.\n",
            "\tModel select at epoch 24 with validation accuracy 94.76% and accuracy 97.01% under attack.\n",
            "pgd l1: attack effectiveness 0.06153846153846154.\n",
            "Mini batch: 271/300 | training time in 1 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0366 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 272/300 | training time in 1 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0091 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 273/300 | training time in 1 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0041 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.014705882352941176.\n",
            "Mini batch: 274/300 | training time in 1 minutes, 8 seconds.\n",
            "Training loss (batch level): 0.0142 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.04838709677419355.\n",
            "Mini batch: 275/300 | training time in 1 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0222 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.01818181818181818.\n",
            "Mini batch: 276/300 | training time in 1 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0130 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.06060606060606061.\n",
            "Mini batch: 277/300 | training time in 1 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0311 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.03278688524590164.\n",
            "Mini batch: 278/300 | training time in 1 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0234 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.08823529411764706.\n",
            "Mini batch: 279/300 | training time in 1 minutes, 9 seconds.\n",
            "Training loss (batch level): 0.0572 | Train accuracy: 94.53%.\n",
            "pgd l1: attack effectiveness 0.09523809523809523.\n",
            "Mini batch: 280/300 | training time in 1 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0303 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0241 | Train accuracy: 98.22\n",
            "pgd l1: attack effectiveness 0.0945273631840796.\n",
            "\tVal accuracy 92.65% with accuracy 90.55% under attack.\n",
            "\tModel select at epoch 24 with validation accuracy 94.76% and accuracy 97.01% under attack.\n",
            "pgd l1: attack effectiveness 0.015384615384615385.\n",
            "Mini batch: 281/300 | training time in 1 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0351 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 282/300 | training time in 1 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0146 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 283/300 | training time in 1 minutes, 10 seconds.\n",
            "Training loss (batch level): 0.0125 | Train accuracy: 100.00%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 284/300 | training time in 1 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0097 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.016129032258064516.\n",
            "Mini batch: 285/300 | training time in 1 minutes, 11 seconds.\n",
            "Training loss (batch level): 0.0138 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 286/300 | training time in 1 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0234 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.015151515151515152.\n",
            "Mini batch: 287/300 | training time in 1 minutes, 12 seconds.\n",
            "Training loss (batch level): 0.0565 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.18032786885245902.\n",
            "Mini batch: 288/300 | training time in 1 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0954 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.029411764705882353.\n",
            "Mini batch: 289/300 | training time in 1 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0370 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.047619047619047616.\n",
            "Mini batch: 290/300 | training time in 1 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0160 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0314 | Train accuracy: 98.62\n",
            "pgd l1: attack effectiveness 0.12437810945273632.\n",
            "\tVal accuracy 92.03% with accuracy 87.56% under attack.\n",
            "\tModel select at epoch 24 with validation accuracy 94.76% and accuracy 97.01% under attack.\n",
            "pgd l1: attack effectiveness 0.046153846153846156.\n",
            "Mini batch: 291/300 | training time in 1 minutes, 13 seconds.\n",
            "Training loss (batch level): 0.0449 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.03125.\n",
            "Mini batch: 292/300 | training time in 1 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0165 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.04477611940298507.\n",
            "Mini batch: 293/300 | training time in 1 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0172 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.029411764705882353.\n",
            "Mini batch: 294/300 | training time in 1 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0154 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.08064516129032258.\n",
            "Mini batch: 295/300 | training time in 1 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0536 | Train accuracy: 96.88%.\n",
            "pgd l1: attack effectiveness 0.05454545454545454.\n",
            "Mini batch: 296/300 | training time in 1 minutes, 14 seconds.\n",
            "Training loss (batch level): 0.0336 | Train accuracy: 98.44%.\n",
            "pgd l1: attack effectiveness 0.015151515151515152.\n",
            "Mini batch: 297/300 | training time in 1 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0235 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.01639344262295082.\n",
            "Mini batch: 298/300 | training time in 1 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0149 | Train accuracy: 99.22%.\n",
            "pgd l1: attack effectiveness 0.014705882352941176.\n",
            "Mini batch: 299/300 | training time in 1 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0273 | Train accuracy: 97.66%.\n",
            "pgd l1: attack effectiveness 0.0.\n",
            "Mini batch: 300/300 | training time in 1 minutes, 15 seconds.\n",
            "Training loss (batch level): 0.0296 | Train accuracy: 97.87%.\n",
            "Training loss (epoch level): 0.0276 | Train accuracy: 98.22\n",
            "pgd l1: attack effectiveness 0.029850746268656716.\n",
            "\tVal accuracy 94.63% with accuracy 97.01% under attack.\n",
            "\tModel select at epoch 24 with validation accuracy 94.76% and accuracy 97.01% under attack.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test: accuracy\n",
        "max_adv_training_model.load()\n",
        "max_adv_training_model.model.predict(test_Loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l85ZkQlmlQER",
        "outputId": "5e4c926c-ab40-4429-b96a-cfc3ba4e7e71"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on the test dataset is 93.00000%\n",
            "The balanced accuracy on the test dataset is 92.94929%\n",
            "Other evaluation metrics we may need:\n",
            "False Negative Rate (FNR) is 1.98020%, False Positive Rate (FPR) is 12.12121%, F1 score is 93.39623%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_adv_training_model.adv_predict(test_Loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyzdZ9yWlQYz",
        "outputId": "fe689719-3193-49b8-a8b5-eb7fb5c30baf"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pgd l1: attack effectiveness 0.039603960396039604.\n",
            "\tadversarial accuracy  96.04% under attack.\n"
          ]
        }
      ]
    }
  ]
}